# 问题的提出

## 更新的并发性

多线程的引入，为应用程序带来性能上的卓越提升，同时也带来了一个最大的副作用，那就是并发。《深入理解计算机系统》一书对并发进行了如下定义：如果逻辑控制流在时间上重叠，那么它们就是并发的。这里提到的逻辑控制流，通俗地讲，就是一次程序操作，比如读取或更新内存中变量的值。

## 分布式一致性问题

分布式系统对于数据的复制需求一般都来自于一下两个原因。

- 为了增加系统的可用性，以防止单点故障引起的系统不可用。
- 提高系统的整体性能，通过负载均衡技术，能够让分布在不同地方的数据副本都能够为用户提供服务。

数据复制在可用性和性能方面给分布式系统带来的巨大好处是不言而喻的，然而数据复制所带来的一致性挑战，也是每一个系统研发人员不得不面对的。

所谓的分布式一致性问题，是指分布式环境引入数据复制机制后，不同数据节点间可能出现的，并无法依靠计算机应用程序自身解决的数据不一致情况。简单地讲，数据一致性就是指在对一个副本数据进行更新的同时，必须确保也能够更新其他的副本，否则不同副本之间的数据将不再一致。

总的来讲，我们无法找到一种能够满足分布式系统所有系统属性的分布式一致性解决方案。因此，如何既保证数据的一致性，同时又不影响系统运行的性能，是每一个分布式系统都需要重点考虑和权衡的。于是，一致性级别由此诞生。

**强一致性**

这种一致性级别是最符合用户直觉的，它要求系统写入什么，读出来的也会是什么，用户体验好，但实现起来往往对系统的性能影响比较大。

**弱一致性**

这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不具体承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态。弱一致性还可以再进行细分：

- **会话一致性**：该一致性级别只保证对于写入的值，在同一个客户端会话中可以读到一致的值，但其他的会话不能保证。
- **用户一致性**：该一致性级别只保证对于写入的值，在同一个用户中可以读到一致的值，但其他用户不能保证。

**最终一致性**

最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态。这里之所以将最终一致性单独提出来，是因为它是弱一致性中非常重要的一种一致性模型，也是业界在大型分布式系统的数据一致性上比较推崇的模型。

# 第1章 分布式架构

随着计算机系统规模变得越来越大，将所有的业务单元集中部署在一个或若干个大型机上体系结构，已经越来越不能满足当今计算机系统，尤其是大型互联网系统的快速发展，各种灵活多变的系统架构模型层出不穷。

## 1.1 从集中式到分布式

从20世纪80年代以来，计算机系统向网络化和微型化的发展日趋明显，传统的集中式处理模式越来越不能适应人们的需求。

首先，大型主机的人才培养成本非常之高。

其次，大型主机也是非常昂贵的。

另外，集中式系统具有明显的单点问题。

而另一方面，随着PC机性能的不断提升和网络技术的快速普及，大型主机的市场份额变得越来越小，很多企业开始放弃原来的大型主机，而改用小型机和普通PC服务器来搭建分布式的计算机系统。

### 1.1.1 集中式的特点

所谓的集中式系统就是指由一台或多台主计算机组成中心节点，数据集中存储于这个中心节点中，并且整个系统的所有业务单元都集中部署在这个中心节点上，系统的所有功能均由其集中处理。也就是说，在集中式系统中，每个终端或客户端机器仅仅负责数据的录入和输出，而数据的存储与控制处理完全交由主机来完成。

集中式系统最大的特点就是部署结构简单。

### 1.1.2 分布式的特点

在《分布式系统概念与设计》一书中，对分布式系统做了如下定义：

> 分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统。

**分布性**

分布式系统中的多台计算机都会在空间上随意分布，同时，机器的分布情况也会随时变动。

**对等性**

分布式系统中的计算机没有主/从之分，既没有控制整个系统的主机，也没有被控制的从机，组成分布式系统的所有计算机节点都是对等的。副本（Replica）是分布式系统最常见的概念之一，指的是分布式系统对数据和服务提供的一种冗余方式。在常见的分布式系统中，为了对外提高高可用的服务，我们往往会对数据和服务进行副本处理。数据副本是指在不同的节点上持久化同一份数据，当某一个节点上存储的数据丢失时，可以从副本上读取到该数据，这是解决分布式系统数据丢失问题最为有效的手段。另一类副本是服务副本，指多个节点提供同样的服务，每个节点都有能力接收来自外部的请求并进行相应的处理。

**并发性**

在“问题的提出”部分，我们已经提到过与“更新的并发性”相关的内容。在一个计算机网络中，程序运行过程中的并发性操作是非常常见的行为，例如同一个分布式系统中的多个节点，可能会并发地操作一些共享的资源，诸如数据库或分布式存储等，如何准确并高效地协调分布式并发操作也成为了分布式系统架构与设计中最大的挑战之一。

**缺乏全局时钟**

在上面的讲解中，我们已经了解到，一个典型的分布式系统是由一系列在空间上随意分布的多个进程组成的，具有明显的分布性，这些进程之间通过交换消息来进行相互通信。因此，在分布式系统中，很难定义两个事件究竟谁先谁后，原因就是因为分布式系统缺乏一个全局的时钟序列控制。

**故障总是会发生**

组成分布式系统的所有计算机，都有可能发生任何形式的故障。一个被大量工程实践所检验过的黄金定理是：任何在设计阶段考虑到的异常情况，一定会在系统实际运行中发生，并且，在系统实际运行过程中还会遇到很多在设计时未能考虑到的异常故障。所以，除非需求指标允许，在系统设计时不能放过任何异常情况。

### 1.1.3 分布式环境的各种问题

分布式系统体系结构从其出现之初就伴随着诸多的难题和挑战，本节将向读者简要的介绍分布式环境中一些典型的问题。

**通信异常**

从集中式向分布式演变的过程中，必然引入了网络因素，而由于网络本身的不可靠性，因此也引入了额外的问题。分布式系统需要在各个节点之间进行网络通信，因此每次网络通信都会伴随着网络不可用的风险。另外，即使分布式系统各节点之间的网络通信能够正常进行，其延时也会远大于单机操作。

**网络分区**

当网络由于发生异常情况，导致分布式系统中部分节点的网络延时不断增大，最终导致组成分布式系统的所有节点中，只有部分节点之间能够进行正常通信，而另一些节点而不能——我们将这个现象称为网络分区，就是俗称的“脑裂”。当网络分区出现时，分布式系统会出现局部小集群，在极端情况下，这些局部小集群会独立完成原本需要整个分布式系统才能完成的功能，包括对数据的事务处理，这就对分布式一致性提出了非常大的挑战。

**三态**

从上面的介绍中，我们已经了解到了在分布式环境下，网络可能会出现各式各样的问题，因此分布式系统的每一次请求与响应，存在特有的“三态”概念，即成功、失败与超时。在传统的单机系统中，应用程序在调用一个函数之后，能够得到一个非常明确的响应：成功或失败。而在分布式系统中，由于网络是不可靠的，虽然在绝大部分情况下，网络通信也能接收到成功或失败的响应，但是当网络出现异常的情况下，就可能会出现超时现象，通常有以下两种情况：

- 由于网络原因，该请求（消息）并没有被成功地发送到接收方，而是在发送过程就发生了消息丢失现象。
- 该请求（消息）成功的被接收方接收后，并进行了处理，但是在将响应反馈给发送方的过程中，发生了消息丢失现象。

当出现这样的超时现象时，网络通信的发起方是无法确定当前请求是否被成功处理的。

**节点故障**

节点故障则是分布式环境下另一个比较常见的问题，指的是组成分布式系统的服务器节点出现的宕机或“僵死”现象。通常根据经验来说，每个节点都有可能会出现故障，并且每天都在发生。

## 1.2 ACID到CAP/BASE

### 1.2.1 ACID

事务（Transaction）是由一系列对系统中数据进行访问与更新的操作所组成的一个程序执行逻辑单元（Unit），狭义上的事务特指数据库事务。一方面，当多个应用程序并发访问数据库时，事务可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。另一方面，事务为数据库操作序列提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持数据一致性的方法。

事务具有四个特征，分别是原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability），简称为事务的ACID特性。

**原子性**

事务的原子性是指事物必须是一个原子的操作序列单元。事物中包含的各项操作在一次执行过程中，只允许出现以下两种状态之一。

- 全部成功执行。
- 全部不执行。

任何一项操作失败都将导致整个事务失败，同时其他已经被执行的操作都将被撤销并回滚，只有所有的操作全部成功，整个事物才算是成功完成。

**一致性**

事务的一致性是指事务的执行不能破坏数据库数据的完整性和一致性，一个事务在执行之前和执行之后，数据库都必须处于一致性状态。也就是说，事务执行的结果必须是使数据从一个一致性状态转变到另一个一致性状态，因此当数据库只包含成功事务提交的结果时，就能说数据库处于一致性状态。而如果数据库系统在运行过程中发生故障，有些事务尚未完成就被迫中断，这些未完成的事务对数据库所做的修改有一部分已写入物理数据库，这时数据库就处于一种不正确的状态，或者说是不一致的状态。

**隔离性**

事务的隔离性是指在并发环境中，并发的事务是相互隔离的，一个事务的执行不能被其他事务干扰。也就是说，不同的事务并发操纵相同的数据时，每个事务都有各自完整的数据空间，即一个事务内部的操作及使用的数据对其他并发事务是隔离的，并发执行的各个事务之间不能互相干扰。

标准SQL规范中，定义了4个事务隔离级别，不同的隔离级别对事务的处理不同，如未授权读取、授权读取、可重复读取和串行化。

*未授权读取*

未授权读取也被称为读未提交（Read Uncommited），该隔离级别允许脏读取，其隔离级别最低。换句话说，如果一个事务正在处理某一数据，并对其进行了更新，但同时尚未完成事务，因此还没有进行事务提交；而与此同时，允许另一个事务也能够访问该数据。

*授权读取*

授权读取也被称为读已提交（Read Committed），它和未授权读取非常相近，唯一的区别就是授权读取只允许获取已经被提交的数据。授权读取允许不可重复读取。

*可重复读取*

可重复读取（Repeatable Read），简单地说，就是保证在事务处理过程中，多次读取同一个数据时，其值都和事务开始时刻是一致的。因此该事务级别禁止了不可重复读取和脏读取，但是有可能出现幻影数据。所谓幻影数据，就是指同样的事务操作，在前后两个时间段内执行对同一个数据项的读取，可能出现不一致的结果。

*串行化*

串行化（Serializable）是最严格的事务隔离级别。它要求所有事务都被串行执行，即事务只能一个接一个地进行处理，不能并发执行。

图1-1展示了不同隔离级别下事务访问数据的差异

~~~
1…2……9…10………………………………………………11…12……19…20
--事务A-------------------------事务C-->
事务A将数据项从1更新到10，存在多个中间状态值
事务C将数据项从10更新到20，存在多个中间状态值

未授权读取：可能读取到1~20中任意的值
授权读取：只可能读取到1、10和20
可重复读取：只能读取到1
串行化：不可访问
~~~

以上4个隔离级别的隔离性依此增强，分别解决不同的问题，表1-1对这4个隔离级别进行了一个简单的对比。

| 隔离级别   | 脏读   | 可重复读 | 幻读   |
| ---------- | ------ | -------- | ------ |
| 未授权读取 | 存在   | 不可以   | 存在   |
| 授权读取   | 不存在 | 不可以   | 存在   |
| 可重复读   | 不存在 | 可以     | 存在   |
| 串行化     | 不存在 | 可以     | 不存在 |

事务隔离级别越高，就越能保证数据的完整性和一致性，但同时对并发性能的影响也越大。通常，对于绝大多数的应用程序来说，可以优先考虑将数据库系统的隔离级别设置为授权读取，这能够在避免脏读取的同时保证较好的并发性能。尽管这种事务隔离级别会导致不可重复读、虚读和第二类丢失更新等并发问题，但较为科学的做法是在可能出现这类问题的个别场合中，由应用程序主动采用悲观锁或乐观锁来进行事务控制。

**持久性**

事务的持久性也被称为永久性，是指一个事务一旦提交，它对数据库中对应数据的状态变更就应该是永久性的。换句话说，一旦某个事务成功结束，那么它对数据库所做的更新就必须被永久保存下来——即使发生系统崩溃或机器宕机等故障，只要数据库能够重新启动，那么一定能够将其恢复到事物成功结束时的状态。

### 1.2.2 分布式事务

分布式事务是指事物的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于分布式系统的不同节点之上。通常一个分布式事务中会涉及多个数据源或业务系统的操作。

我们可以看到，一个分布式事务可以看作是由多个分布式的操作序列组成的，通常可以把这一系列分布式的操作序列称为子事务。因此，分布式事务也可以被定义为一种嵌套型的事务，同时也就具有了ACID事务特性。但由于在分布式事务中，各个子事务的执行是分布式的，因此要实现一种能够保证ACID特性的分布式事务处理系统就显得格外复杂。

### 1.2.3 CAP和BASE理论

如何构建一个兼顾可用性和一致性的分布式系统成为了无数工程师探讨的难题，出现了诸如CAP和BASE这样的分布式系统经典理论。

#### CAP定理

2000年7月，来自加州大学伯克利分校的Eric Brewer教授在ACM PODC（Principles of Distributed Computing）会议上，首次提出了著名的CAP猜想。2年后，来自麻省理工学院的Seth Gilbert和Nancy Lynch从理论上证明了Brewer教授CAP猜想的可行性，从此，CAP理论正式在学术上成为了分布式计算领域的公认定理，并深深地影响了分布式计算的发展。

CAP理论告诉我们，一个分布式系统不可能同时满足一致性（C: Consistency）、可用性（A: Availability）和分区容错性（P: Partition tolerance）这三个基本需求，最多只能同时满足其中的两项。

**一致性**

在分布式环境中，一致性是指数据在多个副本之间是否能够保持一致的特性。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一致的状态。

对于一个将数据副本分布在不同分布式节点上的系统来说，如果对第一个节点的数据进行了更新操作并且更新成功后，却没有使得第二个节点上的数据得到相应的更新，于是在对第二个节点的数据进行读取操作时，获取的依然是老数据（或称为脏数据），这就是典型的分布式数据不一致情况。在分布式系统中，如果能够做到针对一个数据项的更新操作执行成功后，所有的用户都可以读取到其最新的值，那么这样的系统就被认为具有强一致性（或严格的一致性）。

**可用性**

可用性是指系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。这里我们重点看一下“有限的时间内”和“返回结果”。

“有限的时间内”是指，对于用户的一个操作请求，系统必须能够在指定的时间（即响应时间）内返回对应的处理结果，如果超过了这个时间范围，那么系统就被认为是不可用的。另外，“有限的时间内”是一个在系统涉及之初就设定好的系统运行指标，通常不同的系统之间会有很大的不同。

“返回结果”是可用性的另一个非常重要的指标，它要求系统在完成对用户请求的处理后，返回一个正常的响应结果。正常的响应结果通常能够明确地反映出对请求的处理结果，即成功或失败，而不是一个让用户感到困惑的返回结果。

**分区容错性**

分区容错性约束了一个分布式系统需要具有如下特性：分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。

网络分区是指在分布式系统中，不同的节点分布在不同的子网络（机房或异地网络等）中，由于一些特殊的原因导致这些子网络之间出现网络不连通的状况，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个孤立的区域。

以上就是对CAP定理的一致性、可用性和分区容错性的讲解。

既然在上文中我们提到，一个分布式系统无法同时满足上述三个需求，而只能满足其中的两项，因此在进行对CAP定理的应用时，我们就需要抛弃其中的一项，表1-2所示是抛弃CAP定理中任意一项特性的场景说明。

| 放弃CAP定理 | 说明                                                         |
| ----------- | ------------------------------------------------------------ |
| 放弃P       | 如果希望能够避免系统出现分区容错性问题，一种较为简单的做法是将所有的数据（或者仅仅是那些与事务相关的数据）都放在一个分布式节点上。这样的做法虽然无法100%保证系统不会出错，但至少不会碰到由于网络分区带来的负面影响。但同时需要注意的是，放弃P的同时也就意味着放弃了系统的可扩展性 |
| 放弃A       | 相对于放弃“分区容错性”来说，放弃可用性则正好相反，其做法是一旦系统遇到网络分区或其他故障时，那么受到影响的服务需要等待一定的时间，因此在等待期间系统无法对外提供正常的服务，即不可用 |
| 放弃C       | 这里所说的放弃一致性，并不是完全不需要数据一致性，如果真是这样的话，那么系统的数据都是没有意义的，整个系统也是没有价值的。<br />事实上，放弃一致性指的是放弃数据的强一致性，而保留数据的最终一致性。这样的系统无法保证数据保持实时的一致性，但是能够承诺的是，数据最终会达到一个一致的状态。这就引入了一个时间窗口的概念，具体多久能够达到数据一致取决于系统的设计，主要包括数据副本在不同节点之间的复制时间长短 |

从CAP定理中我们可以看出，一个分布式系统不可能同时满足一致性、可用性和分区容错性这三个需求。另一方面，需要明确的一点是，对于一个分布式系统而言，分区容错性可以说是一个最基本的要求。为什么这样说，其实很简单，因为既然是一个分布式系统，那么分布式系统中的组件必然需要被部署到不同的节点，否则也就无所谓分布式系统了，因此必然出现子网络。而对于分布式系统而言，网络问题又是一个必定会出现的异常情况，因此分区容错性也就成为了一个分布式系统必然需要面对和解决的问题。因此系统架构设计师往往需要把精力花在如何根据业务特点在C（一致性）和A（可用性）之间寻求平衡。

#### BASE理论

BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的简写，是由来自eBay的架构师Dan Pritchett在其文章BASE: An Acid Alternative中第一次明确提出的。BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于CAP定理逐步演化而来的，其核心思想是即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventually consistency）。

**基本可用**

基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性——但请注意，这绝不等价于系统不可用。以下两个就是“基本可用”的典型例子。

- **响应时间上的损失**：由于出现故障，查询结果的响应时间增加到了1~2秒。
- **功能上的损失**：消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。

**弱状态**

弱状态也称为软状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。

**最终一致性**

最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

亚马逊首席技术官Werner Vogels在于2008年发表的一篇经典文章Eventually Consistent Revisited 中，对最终一致性进行了非常详细的介绍。他认为最终一致性是一种特殊的弱一致性：系统能够保证在没有其他新的更新操作的情况下，数据最终一定能够达到一致的状态，因此所有客户端对系统的数据访问都能够获取到最新的值。同时，在没有发生故障的前提下，数据达到一致状态的时间延迟，取决于网络延迟，系统负载和数据复制方案设计等因素。

在实际工程实践中，最终一致性存在以下五类主要变种。

*因果一致性*（Casual consistency）

因果一致性是指，如果进程A在更新完某个数据项后通知了进程B，那么进程B之后对该数据项的访问都应该能够获取到进程A更新后的最新值，并且如果进程B要对该数据项进行更新操作的话，务必基于进程A更新后的最新值，即不能发生丢失更新情况。与此同时，与进程A无因果关系的进程C的数据访问则没有这样的限制。

*读己之所写*（Read your writes）

读己之所写是指，进程A更新一个数据项之后，它自己总是能够访问到更新过的最新值，而不会看到旧值。也就是说，对于单个数据获取者来说，其读取到的数据，一定不会比自己上次写入的值旧。因此，读己之所写也可以看作是一种特殊的因果一致性。

*会话一致性*（Session consistency）

会话一致性将对系统数据的访问过程框定在了一个会话当中：系统能保证在同一个有效的会话中实现“读己之所写”的一致性，也就是说，执行更能操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。

*单调读一致性*（Monotonic read consistency）

单调读一致性是指如果一个进程从系统中读取出一个数据项的某个值后，那么系统对于该进程后续的任何数据访问都不应该返回更旧的值。

*单调写一致性*（Monotonic write consistency）

单调写一致性是指，一个系统需要能够保证来自同一个进程的写操作被顺序的执行。

以上就是最终一致性的五类常见的变种，在实际系统实践中，可以将其中的若干个变种互相结合起来，以构建一个具有最终一致性特性的分布式系统。事实上，最终一致性并不是只有那些大型分布式系统才涉及的特性，许多现代的关系型数据库都采用了最终一致性模型。在现代关系型数据库中，大多都会采用同步和异步方式来实现主备数据复制技术。

总的来说，BASE理论面向的是大型高可用可扩展的分布式系统，和传统事务的ACID特性是相反的，它完全不同于ACID的强一致性模型，而是提出通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。但同时，在实际的分布式场景中，不同业务单元和组件对数据一致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID特性与BASE理论往往又会结合在一起使用。

# 第2章 一致性协议

为了解决分布式一致性问题，在长期的探索研究过程中，涌现出了一大批经典的一致性协议和算法，其中最著名的就是二阶段提交协议、三阶段提交协议和Paxos算法了。

## 2.1 2PC与3PC

在分布式系统中，每一个机器节点虽然都能够明确地知道自己在进行事务操作过程中的结果是成功或失败，但却无法直接获取到其它分布式节点的操作结果。因此，当一个事务操作需要跨越多个分布式节点的时候，为了保持事务处理的ACID特性，就需要引入一个称为“协调者（Coordinator）”的组件来统一调度所有分布式节点的执行逻辑，这些被调度的分布式节点则被称为“参与者”（Participant）。协调者负责调度参与者的行为，并最终决定这些参与者是否要把事务真正进行提交。基于这个思想，衍生出了二阶段提交和三阶段提交两种协议，在本节中，我们将重点对这两种分布式事务中涉及的一致性协议进行讲解。

### 2.1.1 2PC

2PC，是Two-Phase Commit的缩写，即二阶段提交，是计算机网络尤其是在数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务处理过程中能够保持原子性和一致性而设计的一种算法。通常，二阶段提交协议也被认为是一种一致性协议，用来保证分布式系统数据的一致性。目前，绝大部分的关系型数据库都是采用二阶段提交协议来完成分布式事务处理的，利用该协议能够非常方便地完成所有分布式事务参与者的协调，统一决定事务的提交或回滚，从而能够有效地保证分布式数据一致性，因此二阶段提交协议被广泛地应用在许多分布式系统中。

#### 协议说明

顾名思义，二阶段提交协议是将事务的过程分成了两个阶段来进行处理，其执行流程如下。

**阶段一：提交事务请求**

1. 事务询问。
   协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者的响应。
2. 执行事务。
   各参与者节点执行事务操作，并将Undo和Redo信息记入事务日志中。
3. 各参与者向协调者反馈事务询问的响应。
   如果参与者成功执行了事务操作，那么就反馈给协调者Yes响应，表示事务可以执行；如果参与者没有成功执行事务，那么就反馈给协调者No响应，表示事务不可以执行。

由于上面讲述的内容在形式上近似是协调者组织各参与者对一次事务操作的投票表态过程，因此二阶段提交协议的阶段一也被称为“投票阶段”，即各参与者投票表明是否要继续执行接下去的事务提交操作。

**阶段二：执行事务提交**

在阶段二中，协调者会根据各参与者的反馈情况来决定最终是否可以进行事务提交操作，正常情况下，包含以下两种可能。

*执行事务提交*

假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务提交。

1. 发送提交请求。
   协调者向所有参与者节点发出Commit请求。
2. 事务提交。
   参与者接收到Commit请求后，会正式执行事务提交操作，并在完成提交之后释放在整个事务执行期间占用的事务资源。
3. 反馈事务提交结果。
   参与者在完成事务提交之后，向协调者发送Ack消息。
4. 完成事务。
   协调者接收到所有参与者反馈的Ack消息后，完成事务。

*中断事务*

假如任何一个参与者向协调者反馈了No响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响应，那么就会中断事务。

1. 发送回滚请求。
   协调者向所有参与者节点发出Rollback请求。
2. 事务回滚。
   参与者接收到Rollback请求后，会利用其在阶段一中记录的Undo信息来执行事务回滚操作，并在完成回滚之后释放在整个事务执行期间占用的资源。
3. 反馈事务回滚结果。
   参与者在完成事务回滚之后，向协调者发送Ack消息。
4. 中断事务。
   协调者接收到所有参与者反馈的Ack消息后，完成事务中断。

以上就是二阶段提交过程中，前后两个阶段分别进行的处理逻辑。简单地讲，二阶段提交将一个事务的处理过程分为了投票和执行两个阶段，其核心是对每个事务都采用先尝试后提交的处理方式，因此也可以将二阶段提交看作一个强一致性的算法。

#### 优缺点

二阶段提交协议的优点：原理简单，实现方便。

二阶段提交协议的缺点：同步阻塞、单点问题、脑裂、太过保守。

*同步阻塞*

二阶段提交协议存在的最明显也是最大的一个问题就是同步阻塞，这会极大地限制分布式系统的性能。在二阶段提交的执行过程中，所有参与该事务操作的逻辑都处于阻塞状态，也就是说，各个参与者在等待其他参与者响应的过程中，将无法进行其他任何操作。

*单点问题*

在上面的讲解过程中，相信读者可以看出，协调者的角色在整个二阶段提交协议中起到了非常重要的作用。一旦协调者出现问题，那么整个二阶段提交流程将无法运转，更为严重的是，如果协调者是在阶段二中出现问题的话，那么其他参与者将会一直处于锁定事务资源的状态中，而无法继续完成事务操作。

*数据不一致*

在二阶段提交协议的阶段二，即执行事务提交的时候，当协调者向所有的参与者发送Commit请求之后，发生了局部网络异常或者是协调者在尚未发送完Commit请求之前自身发生了崩溃，导致最终只有部分参与者收到了Commit请求。于是，这部分收到了Commit请求的参与者就会进行事务的提交，而其他没有收到Commit请求的参与者则无法进行事务提交，于是整个分布式系统便出现了数据不一致现象。

*太过保守*

如果在协调者指示参与者进行事务提交询问的过程中，参与者出现故障而导致协调者无法获取到所有参与者的响应信息的话，这时协调者只能依靠其自身的超时机制来判断是否需要中断事务，这样的策略显得比较保守。换句话说，二阶段提交协议没有设计较为完善的容错机制，任意一个节点的失败都会导致整个事务的失败。

### 2.1.2 3PC

在上文中，我们讲解了二阶段提交协议的设计和实现原理，并明确指出了其在实际运行过程中可能存在的诸如同步阻塞、协调者的单点问题、脑裂和太过保守的容错机制等缺陷，因此研究者在二阶段提交协议的基础上进行了改进，提出了三阶段提交协议。

#### 协议说明

3PC，是Three-Phase Commit的缩写，即三阶段提交，是2PC的改进版，其将二阶段提交协议的“提交事务请求”过程一分为二，形成了由CanCommit、PreCommit和do Commit三个阶段组成的事务处理协议。

**阶段一：CanCommit**

1. 事务询问
   协调者向所有的参与者发送一个包含事务内容的canCommit请求，询问是否可以执行事务提交操作，并开始等待各参与者的响应。
2. 各参与者向协调者反馈事务询问的响应。
   参与者在接收到来自协调者的canCommit请求后，正常情况下，如果其自身认为可以顺利执行事务，那么会反馈Yes响应，并进入预备状态，否则反馈No响应。

**阶段二：PreCommit**

在阶段二中，协调者会根据各参与者的反馈情况来决定是否可以进行事务的PreCommit操作，正常情况下，包含两种可能。

*执行事务预提交*

假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务预提交。

1. 发送预提交请求。
   协调者向所有参与者节点发出preCommit的请求，并进入Prepared阶段。
2. 事务预提交。
   参与者接收到preCommit请求后，会执行事务操作，并将Undo和Redo信息记录到事务日志中。
3. 各参与者向协调者反馈事务执行的响应。
   如果参与者成功执行了事务操作，那么就会反馈给协调者Ack响应，同时等待最终的指令：提交（commit）或中止（abort）。

*中断事务*

假如任何一个参与者向协调者反馈了No响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响应，那么就会中断事务。

1. 发送中断请求。
   协调者向所有参与者节点发出abort请求。
2. 中断事务。
   无论是收到来自协调者的abort请求，或者是在等待协调者请求过程中出现超时，参与者都会中断事务。

**阶段三：doCommit**

该阶段将进行真正的事务提交，会存在以下两种可能的情况。

*执行提交*

1. 发送提交请求。
   进入这一阶段，假设协调者处于正常工作状态，并且它接收到了来自所有参与者的Ack响应，那么它将从“预提交”状态转换到“提交”状态，并向所有的参与者发送doCommit请求。
2. 事务提交。
   参与者接收到doCommit请求后，会正式执行事务提交操作，并在完成提交之后释放在整个事务执行期间占用的事务资源。
3. 反馈事务提交结果。
   参与者在完成事务提交之后，向协调者发送Ack消息。
4. 完成事务。
   协调者接收到所有参与者反馈的Ack消息后，完成事务。

*中断事务*

进入这一阶段，假设协调者处于正常工作状态，并且有任意一个参与者向协调者反馈了No响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响应，那么就会中断事务。

1. 发送中断请求。
   协调者向所有的参与者节点发送abort请求。
2. 事务回滚。
   参与者接收到abort请求后，会利用其在阶段二中记录的Undo信息来执行事务回滚操作，并在完成回滚之后释放在整个事务执行期间占用的资源。
3. 反馈事务回滚结果。
   参与者在完成事务回滚之后，向协调者发送Ack消息。
4. 中断事务。
   协调者接收到所有参与者反馈的Ack消息后，中断事务。

需要注意的是，一旦进入阶段三，可能会存在以下两种故障。

- 协调者出现问题。
- 协调者和参与者之间的网络出现故障。

无论出现哪种情况，最终都会导致参与者无法及时接收到来自协调者的doCommit或是abort请求，针对这样的异常情况，参与者都会在等待超时之后，继续进行事务提交。

#### 优缺点

三阶段提交协议的优点：相较于二阶段提交协议，三阶段提交协议最大的优点就是降低了参与者的阻塞范围，并且能够在出现单点故障后继续达成一致。

三阶段提交协议的缺点：三阶段提交协议在去除阻塞的同时也引入了新的问题，那就是在参与者接收到preCommit消息后，如果网络出现分区，此时协调者所在的节点和参与者无法进行正常的网络通信，在这种情况下，该参与者依然会进行事务的提交，这必然出现数据的不一致性。

## 2.2 Paxos算法

Paxos算法是莱斯利·兰伯特（Leslie Lamport）于1990年提出的一种基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一。

在第1章中我们已经提到，在常见的分布式系统中，总会发生诸如机器宕机或网络异常等情况。Paxos算法需要解决的问题就是如何在一个可能发生上述异常的分布式系统中，快速且正确地在集群内部对某个数据的值达成一致，并且保证不论发生以上任何异常，都不会破坏整个系统的一致性。

### 2.2.1 追本溯源

1982年，Lamport与另两人共同发表了论文The Byzantine Generals Problem，提出了一种计算机容错理论。在理论描述过程中，为了将所要描述的问题形象的表达出来，Lamport设想出了下面这样的一个场景：

> 拜占庭帝国有许多支军队，不同军队的将军之间必须制订一个统一的行动计划，从而做出进攻或者撤退的决定，同时，各个将军在地理上都是被分隔开来的，只能依靠军队的通讯员来进行通讯。然而，在所有的通讯员中可能会存在叛徒，这些叛徒可以任意篡改消息，从而达到欺骗将军的目的。

这就是著名的“拜占庭将军问题”。从理论上来说，在分布式计算领域，试图在异步系统和不可靠的通道上来达到一致性状态是不可能的，因此在对一致性的研究过程中，都往往假设信道是可靠的。而事实上，大多数系统都是部署在同一个局域网中的，因此消息被篡改的情况非常罕见；另一方面，由于硬件和网络原因而造成的消息不完整问题，只需一套简单的校验算法即可避免——因此，在实际工程实践中，可以假设不存在拜占庭问题，也即假设所有消息都是完整的，没有被篡改的。那么，在这种情况下需要什么样的算法来保证一致性呢？

Lamport在1990年提出了一个理论上的一致性解决方案，同时给出了严格的数学证明。鉴于之前采用故事类比的方式成功的阐述了“拜占庭将军问题”，因此这次Lamport同样用心良苦地设想出了一个场景来描述这种，及其具体的解决过程：

> 在古希腊有一个交租Paxos的小岛，岛上采用议会的形式来通过法令，议会中的议员通过信使进行消息的传递。值得注意的是，议员和信使都是兼职的，他们随时有可能会离开议会厅，并且信使可能会重复的传递消息，也可能一去不复返。因此，议会协议要保证在这种情况下法令仍然能够正确的产生，并且不会出现冲突。

这就是论文The Part-Time Parliament中提到的兼职议会，而Paxos算法名称的由来也是取自论文中提到的Paxos小岛。

### 2.2.2 Paxos理论的诞生

### 2.2.3 Paxos算法详解

Paxos算法的核心是一个一致性算法，也就是论文The Part-Time Parliament中提到的“synod”算法，我们将从对一致性问题的描述开始讲解该算法需要解决的实际需求。

#### 问题描述

假设有一组可以提出提案的进程集合，那么对于一个一致性算法来说需要保证以下几点：

- 在这些被提供的提案中，只有一个会被选定。
- 如果没有提案被提供，那么就不会有被选定的提案。
- 当一个提案被选定后，进程应该可以获取被选定的提案信息。

对于一致性来说，安全性（Safety）需求如下：

- 只有被提出的提案才能被选定（Chosen）。
- 只能有一个值被选定。
- 如果某个进程认为某个提案被选定了，那么这个提案必须是真的被选定的那个。

在对Paxos算法的讲解过程中，我们不去精确地定义其活性（Liveness）需求

在该一致性算法中，有三种参与角色，我们用Proposer、Acceptor和Learner来表示。在具体的实现中，一个进程可能充当不止一种角色，在这里我们并不关心进程如何映射到各种角色。

#### 提案的选定

可以使用多个Acceptor来避免Acceptor的单点问题。

**推导过程**

**数学归纳法证明**

**Proposer生成提案**

现在我们来看看，在P2c的基础上如何进行提案的生成。对于一个Proposer来说，获取那些已经被通过的提案远比预测未来可能会被通过的提案来得简单。因此，Proposer在产生一个编号为$M_n$的提案时，必须要知道当前某一个将要或已经被半数以上Acceptor批准的编号小于$M_n$的提案——这就引出了如下的提案生成算法。

1. Proposer选择一个新的提案编号$M_n$，然后向某个Acceptor集合的成员发送请求，要求该集合中的Acceptor做出如下回应。

   - 向Proposer承诺，保证不再批准任何编号小于$M_n$的提案。
   - 如果Acceptor已经批准过任何提案，那么其就向Proposer反馈当前该Acceptor已经批准的编号小于$M_n$但为最大编号的那个提案的值。

   我们将该请求称为编号为$M_n$的提案的Prepare请求。

2. 如果Proposer收到了来自半数以上的Acceptor的响应结果，那么它就可以产生编号为$M_n$，Value值为$V_n$的提案，这里的$V_n$是所有响应中编号最大的提案的Value值。当然还存在另一种情况，就是半数以上的Acceptor都没有批准过任何提案，即响应中不包含任何提案，那么此时$V_n$值就可以由Proposer任意选择。

在确定提案之后，Proposer就会将该提案再次发送给某个Acceptor集合，并期望获得它们的批准，我们称此请求为Accept请求。需要注意的一点是，此时接受Accept请求的Acceptor集合不一定是之前响应Prepare请求的Acceptor集合——这点相信读者也能够明白，任意两个半数以上的Acceptor集合，必定包含至少一个公共Acceptor。

**Acceptor批准提案**

在上文中，我们已经讲解了Paxos算法中Proposer的处理逻辑，下面我们来看看Acceptor是如何批准提案的。

根据上面的内容，一个Acceptor可能会收到来自Proposer的两种请求，分别是Prepare请求和Accept请求，对这两类请求做出响应的条件分别如下。

- **Prepare请求**：Acceptor可以在任何时候响应一个Prepare请求。
- **Accept请求**：在不违背Accept现有承诺的前提下，可以任意响应Accept请求。

因此，对Acceptor逻辑处理的约束条件，大体可以定义如下。

> P1a：一个Acceptor只要尚未响应过任何编号大于$M_n$的Prepare请求，那么它就可以接受这个编号为$M_n$的提案。

从上面这个约束条件中，我们可以看出，P1a包含了P1。同时，值得一提的是，Paxos算法允许Acceptor忽略任何请求而不用担心破坏其算法的安全性。

**算法优化**

接下来我们再对这个初步算法做一个小优化。尽可能地忽略Prepare请求：

>假设一个Acceptor收到了一个编号为$M_n$的Prepare请求，但此时该Acceptor已经对编号大于$M_n$的Prepare请求做出了响应，因此它肯定不会再批准任何新的编号为$M_n$的提案，那么很显然，Acceptor就没有必要对这个Prepare请求做出响应，于是Acceptor可以选择忽略这样的Prepare请求。同时，Acceptor也可以忽略掉那些它已经批准过的提案的Prepare请求。

通过这个优化，每个Acceptor只需要记住它已经批准的提案的最大编号以及它已经做出Prepare请求响应的提案的最大编号，以便在出现故障或节点重启的情况下，也能保证P2c的不变性。而对于Proposer来说，只要它可以保证不会产生具有相同编号的提案，那么就可以丢弃任意的提案以及它所有的运行时状态信息。

**算法陈述**

综合前面讲解的内容，我们来对Paxos算法的提案选定过程进行一个陈述。结合Proposer和Acceptor对提案的处理逻辑，就可以得到如下类似于两阶段的算法执行过程。

*阶段一*

1. Proposer选择一个提案编号$M_n$，然后向Acceptor的某个超过半数的子集成员发送编号为$M_n$的Prepare请求。
2. 如果一个Acceptor收到一个编号为$M_n$的Prepare请求，且编号$M_n$大于该Acceptor已经响应的所有Prepare请求的编号，那么它就会将它已经批准过的最大编号的提案作为响应反馈给Proposer，同时该Acceptor会承诺不会再批准任何编号小于$M_n$的提案。

举个例子来说，假定一个Acceptor已经响应过所有Prepare请求对应的提案编号分别为1、2、…、5和7，那么该Acceptor在接收一个编号为8的Prepare请求后，就会将编号为7的提案作为响应反馈给Proposer。

*阶段二*

1. 如果Proposer收到来自半数以上的Acceptor对于其发出的编号为M_n的Prepare请求的响应，那么它就会发送一个针对$[M_n, V_n]$提案的Accept请求给Acceptor。注意，$V_n$的值就是收到的响应中编号最大的提案的值，如果响应中不包含任何提案，那么它就是任意值。
2. 如果Acceptor收到这个针对$[M_n,V_n]$提案的Accept请求，只要该Accept尚未对编号大于$M_n$的Prepare请求做出响应，它就可以通过这个提案。

当然，在实际允许过程中，每一个Proposer都有可能会产生多个提案，但只要每个Proposer都遵循如上所述的算法运行，就一定能够保证算法执行的正确性。值得一提的是，每个Proposer都可以在任意时刻丢弃一个提案，哪怕针对该提案的请求和响应在提案被丢弃后会到达，但根据Paxos算法的一系列规约，依然可以保证其在提案选定上的正确性。事实上，如果某个Proposer已经在视图生成编号更大的提案，那么丢弃一些旧的提案未尝不是一个好的选择。因此，如果一个Acceptor因为收到过更大编号的Prepare请求而忽略某个编号更小的Prepare或者Accept请求，那么它也应当通知其对应的Proposer，以便该Proposer也能够将该提案进行丢弃——这和上面“算法优化”部分中提到的提案丢弃是一致的。

#### 提案的获取

在上文中，我们已经介绍了如何来选定一个提案，下面我们再来看看如何让Learner获取提案，大体可以有以下几种方案。

*方案一*

Learner获取一个已经被选定的提案的前提是，该提案已经被半数以上的Acceptor批准。因此，最简单的做法就是一旦Acceptor批准了一个提案，就将该提案发送给所有的Learner。

很明显，这种做法虽然可以让Learner尽快地获取被选定的提案，但是却需要让每个Acceptor与所有的Learner逐个进行一次通信，通信的次数至少为二者个数的乘积。

*方案二*

另一种可行的方案是，我们可以让所有的Acceptor将它们对提案的批准情况，统一发送给一个特定的Learner（下文中我们将这样的Learner称为“主Learner”），在不考虑拜占庭将军问题的前提下，我们假定Learner之间可以通过消息通信来互相感知提案的选定情况。基于这样的前提，当主Learner被通知一个提案已经被选定时，它会负责通知其他的Learner。

在这种方案中，Acceptor首先会将得到批准的提案发送给主Learner，再由其同步给其他Learner，因此较方案一而言，方案二虽然需要多一个步骤才能将提案通知到所有的Learner，但其通信次数却大大减少了，通常只是Acceptor和Learner的个数总和。但同时，该方案引入了一个新的不稳定因素：主Learner随时可能出现故障。

*方案三*

在讲解方案二的时候，我们提到，方案二最大的问题在于主Learner存在单点问题，即主Learner随时可能出现故障。因此，对方案二进行改进，可以将主Learner的范围扩大，即Acceptor可以将批准的提案发送给一个特定的Learner集合，该集合中的每个Learner都可以在一个提案被选定后通知所有其他的Learner。这个Learner集合中的Learner个数越多，可靠性就越好，但同时网络通信的复杂度也就越高。

#### 通过选取主Proposer保证算法的活性

为了保证Paxos算法流程的可持续性，以避免陷入“死循环”，就必须选择一个主Proposer，并规定只有主Proposer才能提出议案。这样一来，只要主Proposer和过半的Acceptor能够正常进行网络通信，那么但凡主Proposer提出一个编号更高的提案，该提案终将会被批准。当然，如果Proposer发现当前算法流程中已经有一个编号更大的提案被提出或正在接收批准，那么它会丢弃当前这个编号较小的提案，并最终能够选出一个编号足够大的提案。因此，如果系统中有足够多的组件（包括Proposer、Acceptor和其他网络通信组件）能够正常工作，那么通过选择一个主Proposer，整套Paxos算法流程就能够保持活性。

# 第3章 Paxos的工程实践

## 3.1 Chubby

Google Chubby是一个大名鼎鼎的分布式锁服务，GFS和Big Table等大型系统都用它来解决分布式协作、元数据存储和Master选举等一系列与分布式锁服务相关的问题。Chubby的底层一致性实现就是以Paxos算法为基础的，这给Paxos算法的学习者提供了一个理论联系的范例，从而可以了解到Paxos算法是如何在实际工程中得到应用的。

### 3.1.1 概述

Chubby是一个面向松耦合分布式系统的锁服务，通常用于为一个由大量小型计算机构成的松耦合分布式系统提供高可用的分布式锁服务。一个分布式锁服务的目的是允许它的客户端进程同步彼此的操作，并对当前所处环境的基本状态信息达成一致。

Chubby的客户端接口设计非常类似于UNIX文件系统结构，应用程序通过Chubby的客户端接口，不仅能够对Chubby服务器上的整个文件进行读写操作，还能够添加对文件节点的所控制，并且能够订阅Chubby服务端发出的一系列文件变动的事件通知。

### 3.1.2 应用场景

在Chubby的众多场景中，最为典型的就是集群中服务器的Master选举。

### 3.1.3 设计目标

Chubby之所以设计成这样一个完整的分布式锁服务，是因为锁服务具有以下4个传统算法库所不具有的优点。

**对上层应用程序的侵入性更小**

在这种情况下，尽管这些措施都可以通过一个封装了分布式一致性协议的客户端库来完成，但相比之下，使用一个分布式锁服务的接口方式对上层应用程序的侵入性会更小，并且更易于保持系统已有的程序结构和网络通信模式。

**便于提供数据的发布与订阅**

**开发人员对基于锁的接口更为熟悉**

**更便捷地构建更可靠的服务**

通常一个分布式一致性算法都需要使用Quorum机制来进行数据项值的选定。Quorum机制是分布式系统中实现数据一致性的一个比较特殊的策略，它指的是在一个由若干个机器组成的集群中，在一个数据项值的选定过程中，要求集群中存在过半的机器达成一致，因此Quorum机制也被称作“过半机制”。在Chubby中通常使用5台服务器来组成一个集群单元（cell），根据Quorum机制，只要整个集群中有3台服务器是正常运行的，那么整个集群就可以对外提供正常的服务。相反的，如果仅提供一个分布式一致性协议的客户端库，那么这些高可用性的系统部署都将交给开发人员自己来处理，这无疑提高了成本。



因此，Chubby被设计成一个需要访问中心化节点的分布式锁服务。同时，在Chubby的设计过程中，提出了以下几个设计目标。

**提供一个完整的、独立的分布式锁服务，而非仅仅是一个一致性协议的客户端库**

在上面的内容中我们已经讲到，提供一个独立的锁服务的最大好处在于，Chubby对于使用它的应用程序的侵入性非常低，应用程序不需要修改已有程序的结构即可使用分布式一致性特性。例如，对于“Master选举同时将Master信息登记并广播”的场景，应用程序只需要向Chubby请求一个锁，并且在获得锁之后向相应的锁文件写入Master信息即可，其余的客户端就可以通过读取这个锁文件来获取Master信息。

**提供粗粒度的锁服务**

Chubby锁服务针对的应用场景是客户端获得锁之后会进行长时间持有（数小时或数天），而非用于短暂获取锁的场景。针对这种应用场景，当锁服务短暂失效时（例如服务器宕机），Chubby需要保持所有锁的持有状态，以避免持有锁的客户端出现问题。这和细粒度锁的设计方式有很大的区别，细粒度锁通常设计为锁服务一旦失效就释放所有锁，因为细粒度锁的持有时间很短，相比而言放弃锁带来的代价较小。

**在提供锁服务的同时提供对小文件的读写功能**

Chubby提供对小文件的读写服务，以使得被选举出来的Master可以在不依赖额外服务的情况下，非常方便地向所有客户端发布自己的状态信息。具体的，当一个客户端成功获取到一个Chubby文件锁而成为Master之后，就可以继续向这个文件里写入Master信息，其他客户端就可以通过读取这个文件得知当前的Master信息。

**高可用、高可靠**

基于Paxos算法的实现，对于一个由5台机器组成的Chubby集群来说，只要保证存在3台正常运行的机器，整个集群对外服务就能保持可用。

另外，由于Chubby支持通过小文件读写服务的方式来进行Master选举结果的发布与订阅，因此在Chubby的实际应用过程中，必须能够支撑成百上千个Chubby客户端对同一个文件进行监视和读取。

**提供事件通知机制**

Chubby需要有能力将服务端的数据变化情况（例如文件内容变更）以事件的形式通知到所有订阅的客户端。

### 3.1.4 Chubby技术架构

#### 系统结构

Chubby的整个系统结构主要由服务端和客户端两部分组成，客户端通过RPC调用与服务端进行通信。

一旦某台服务器成为了Master，Chubby就会保证在一段时期内不会再有其他服务器成为Master——这段时期被称为Master租期（Master lease）。

集群中的每个服务器都维护着一份服务端数据库的副本，但在实际运行过程中，只有Master服务器才能对数据库进行写操作，而其他服务器都是使用Paxos协议从Master服务器上同步数据库数据的更新。

Chubby的客户端时如何定位到Master服务器的。

如果集群中的一个服务器发生崩溃并在几小时后仍无法恢复正常，那么就需要加入新的机器，并同时更新DNS列表。

#### 目录与文件

Chubby对外提供了一套与Unix文件系统非常相近但是更简单的访问接口。Chubby的数据结构可以看作是一个由文件和目录组成的树，其中每一个节点都可以表示为一个使用斜杠分割的字符串，典型的节点路径表示如下：

~~~
/ls/foo/wombat/pouch
~~~

其中，ls是所有Chubby节点所共有的前缀，代表着锁服务，是Lock Service的缩写；foo则指定了Chubby集群的名字，从DNS可以查询到由一个或多个服务器组成该Chubby集群；剩余部分的路径`/wombat/pouch`则是一个真正包含业务含义的节点名字，由Chubby服务器内部解析并定义到数据节点。

Chubby的命名空间，包括文件和目录，我们称之为节点（nodes，在本书后面的内容中，我们以数据节点来泛指Chubby的文件或目录）。在同一个Chubby集群数据库中，每一个节点都是全局唯一的。和Unix系统一样，每个目录都可以包含一系列的子文件和子目录列表，而每个文件中则会包含文件内容。当然，Chubby并非模拟一个完整的文件系统，因此没有符号链接和硬连接的概念。

Chubby上的每个数据节点都分为持久节点和临时节点两大类，其中持久节点需要显式地调用接口API来进行删除，而临时节点则会在其对应的客户端会话失效后被自动删除。

另外，Chubby上的每个数据节点都包含了少量的元数据信息，其中包括用于权限控制的访问控制列表（ACL）信息。同时，每个节点的元数据中还包括4个单调递增的64位编号，分别如下。

- **实例编号**：实例编号用于标识Chubby创建该数据节点的顺序，节点的创建顺序不同，其实例编号也不同，因此，通过实例编号，即使针对两个名字相同的数据节点，客户端也能够非常方便地识别出是否是同一个数据节点——因为创建时间晚的数据节点，其实例编号必定大于任意先创建的同名节点。
- **文件内容编号**（只针对文件）：文件内容编号用于标识文件内容的变化情况，该编号会在文件内容被写入时增加。
- **锁编号**：锁编号用于标识节点锁状态变更情况，该编号会在节点锁从自由（free）状态转换到被持有（held）状态时增加。
- **ACL编号**：ACL编号用于标识节点的ACL信息变更情况，该编号会在节点的ACL配置信息被写入时增加。

同时，Chubby还会标识一个64位的文件内容校验码，以便客户端能够识别出文件是否变更。

#### 锁与锁序列器

在Chubby中，任意一个数据节点都可以充当一个读写锁来使用：一种是单个客户端以排他（写）模式持有这个锁，另一种则是任意数目的客户端以共享（读）模式持有这个锁。同时，在Chubby的锁机制中需要注意的一点是，Chubby舍弃了严格的强制锁，客户端可以在没有获取任何锁的情况下访问Chubby的文件，也就是说，持有锁F既不是访问文件F的必要条件，也不会阻止其他客户端访问文件F。

在Chubby中，主要采用锁延迟和锁序列器两种策略来解决上面我们提到的由于消息延迟和重排序引起的分布式锁问题。其中锁延迟是一种比较简单的策略，使用Chubby的应用几乎不需要进行任何代码修改。具体的，如果一个客户端以正常的方式主动释放了一个锁，那么Chubby服务端将会允许其他客户端能够立即获取到该锁。而如果一个锁是因为客户端的异常情况（如客户端无响应）而被释放的话，那么Chubby服务器会为该锁保留一定的时间，我们称之为“锁延迟”（lock-delay），在这段时间内，其他客户端无法获取这个锁。锁延迟措施能够很好地防止一些客户端由于网络闪断等原因而与服务器暂时断开的场景出现。总的来说，该方案尽管不完美，但是锁延时能够有效地保护在出现消息延时情况下发生的数据不一致现象。

Chubby提供的另一种方式是使用锁序列器，当然该策略需要Chubby的上层应用配合在代码中加入相应的修改逻辑。任何时候，锁的持有者都可以向Chubby请求一个锁序列器，其包括锁的名字、锁模式（排他或共享模式），以及锁序号。当客户端应用程序在进行一些需要锁机制保护的操作时，可以将该锁序列器一并发送给服务端。Chubby服务端接收到这样的请求后，会首先检测该序列器是否有效，以及检查客户端是否处于恰当的锁模式；如果没有通过检查，那么服务端就会拒绝该客户端请求。

#### Chubby中的事件通知机制

为了避免大量客户端轮询Chubby服务器状态所带来的压力，Chubby提供了事件通知机制。Chubby客户端可以向服务端注册事件通知，当触发这些事件的时候，服务端就会向客户端发送对应的事件通知。在Chubby的事件通知机制中，消息通知都是通过异步的方式发送给客户端的，常见的Chubby事件如下。

**文件内容变更**

例如，BigTable集群使用Chubby锁来确定集群中哪台BigTable机器是Master；获得锁的BigTable Master会将自身信息写入Chubby上对应的文件中。BigTable集群中的其他客户端可以通过监视这个Chubby文件变化来确定新的BigTable Master机器。

**节点删除**

当Chubby上指定节点被删除的时候，会产生“节点删除”事件，这通常在临时节点中比较常见，可以利用该特性来间接判断该临时节点对应的客户端会话是否有效。

**子节点新增、删除**

当Chubby上指定节点的子节点新增或是减少时，会产生“子节点新增、删除”事件。

**Master服务器转移**

当Chubby服务器发送Master转移时，会以事件的形式通知客户端。

#### Chubby中的缓存

为了提高Chubby的性能，同时也是为了减少客户端和服务端之间频繁的读请求对服务端的压力，Chubby除了提供事件通知机制之外，还在客户端中实现了缓存，会在客户端对文件内容和元数据信息进行缓存。使用缓存机制在提高系统整体性能的同时，也为系统带来了一定的复杂性，其中最主要的问题就是如何保证缓存的一致性。在Chubby中，通过租期机制来保证缓存的一致性。

Chubby缓存的生命周期和Master租期机制紧密相关，Master会维护每个客户端的数据缓存情况，并通过向客户端发送过期信息的方式来保证客户端数据的一致性。在这种机制下，Chubby就能够保证客户端要么能够从缓存中访问到一致的数据，要么访问出错，而一定不会访问到不一致的数据。具体的，每个客户端的缓存都有一个租期，一旦该租期到期，客户端就需要向服务端续订租期以继续维持缓存的有效性。当文件数据或元数据信息被修改时，Chubby服务端首先会阻塞该修改操作，然后由Master向所有可能缓存了该数据的客户端发送缓存过期信号，以使其缓存失效，等到Master在接收到所有相关客户端针对该过期信号的应答（应答包括两类，一类是客户端明确要求更新缓存，另一类则是客户端允许缓存租期过期）后，再继续进行之前的修改操作。

通过上面这个缓存机制的介绍，相信读者都已经明白了，Chubby的缓存数据保证了强一致性。尽管要保证严格的数据一致性对于性能的开销和系统的吞吐影响很大，但由于弱一致性模型在实际使用过程中极容易出现问题，因此Chubby在设计之初就决定了选择强一致性模型。

#### 会话和会话激活（KeepAlive）

Chubby客户端和服务端之间通过创建一个TCP连接来进行所有的网络通信操作，我们将这一连接称为会话（Session）。会话是有生命周期的，存在一个超时时间，在超时时间内，Chubby客户端和服务端之间可以通过心跳检测来保持会话的活性，以使会话周期得到延续，我们将这个过程称为KeepAlive（会话激活）。如果能够成功地通过KeepAlive过程将Chubby会话一直延续下去，那么客户端创建的句柄、锁和缓存数据等依然有效。

#### KeepAlive请求

Master在接收到客户端的KeepAlive请求时，首先会将该请求阻塞住，并等到该客户端的当前会话租期即将过期时，才为其续租该客户端的会话租期，之后再向客户端响应这个KeepAlive请求，并同时将最新的会话租期超时时间反馈给客户端。Master对于会话续租时间的设置，默认是12秒，但这不是一个固定的值，Chubby会根据实际的允许情况，自行调节该周期的长短。举个例子来说，如果当前Master处于高负载运行状态的话，那么Master会适当地延长会话租期的长度，以减少客户端KeepAlive请求的发送频率。客户端在接收到来自Master的续租响应后，会立即发起一个新的KeepAlive请求，再由Master进行阻塞。因此我们可以看出，在正常运行过程中，每一个Chubby客户端总是会有一个KeepAlive请求阻塞在Master服务器上。

除了为客户端进行会话续租外，Master还将通过KeepAlive响应来传递Chubby事件通知和缓存过期通知给客户端。具体的，如果Master发现服务端已经触发了针对该客户端的事件通知或缓存过期通知，那么会提前将KeepAlive响应反馈给客户端。

#### 会话超时

谈到会话租期，Chubby的客户端也会维持一个和Master端近似相同的会话租期。为什么是近似相同呢？这是因为客户端必须考虑两方面的因素：一方面，KeepAlive响应在网络传输过程中会花费一定的时间；另一方面，Master服务端和Chubby客户端存在时钟不一致性现象。因此在Chubby会话中，存在Master端会话租期和客户端本地会话租期。

如果Chubby客户端在运行过程中，按照本地的会话租期超时时间，检测到其会话租期已经过期却尚未接收到Master的KeepAlive响应，那么这个时候，它将无法确定Master服务端是否已经中止了当前会话，我们称这个时候客户端处于“危险状态”。此时，Chubby客户端会清空其本地缓存，并将其标记为不可用。同时，客户端还会等待一个被称作“宽限期”的时间周期，这个宽限期默认是45秒。如果在宽限期到期前，客户端和服务端之间成功进行了KeepAlive，那么客户端就会再次开启本地缓存，否则，客户端就会认为当前会话已经过期了，从而中止本次会话。

我们再着重来看看上面提到的“危险状态”。当客户端进入上述提到的危险状态时，Chubby的客户端库会通过一个“jeopardy”事件来通知上层应用程序。如果恢复正常，客户端同样会以一个“safe”事件来通知应用程序可以继续正常运行了。但如果客户端最终没能从危险状态中恢复过来，那么客户端会以一个“expired”事件来通知应用程序当前Chubby会话已经超时。Chubby通过这些不同的事件类型通知，能够很好地辅助上层应用程序在不明确Chubby会话状态的情况下，根据不同的事件类型来做出不同的处理：等待或重启。有了这样的机制保证之后，对于那些在短时间内Chubby服务不可用的场景下，客户端应用程序可以选择等待，而不是重启，这对于那些重启整个应用程序需要花费较大代价的系统来说非常有帮助。

#### Chubby Master 故障恢复

总的来讲，一个新的Chubby Master服务器选举产生之后，会进行如下几个主要处理。

1. 新的Master选举产生后，首先需要确定Master周期。Master周期用来唯一标识一个Chubby集群的Master统治轮次，以便区分不同的Master。一旦新的Master周期确定下来之后，Master就会拒绝所有携带其他Master周期编号的客户端请求，同时告知其最新的Master周期编号。需要注意的一点是，只要发生Master重新选举，就一定会产生新的Master周期，即使是在选举前后Master都是同一台机器的情况下也是如此。
2. 选举产生的新的Master能够立即对客户端的Master寻址请求进行响应，但是不会立即开始处理客户端会话相关的请求操作。
3. Master根据本地数据库中存储的会话和锁信息，来构建服务器的内存状态。
4. 到现在为止，Master已经能够处理客户端的KeepAlive请求了，但依然无法处理其他会话相关的操作。
5. Master会发送一个“Master故障切换”事件给每一个会话，客户端接收到这个事件后，会清空它的本地缓存，并警告上层应用程序可能已经丢失了别的事件，之后再向Master反馈应答。
6. 此时，Master会一直等待客户端的应答，直到每一个会话都应答了这个切换事件。
7. 在Master接收到了所有客户端的应答之后，就能够开始处理所有的请求操作了。
8. 如果客户端使用了一个在故障切换之前创建的句柄，Master会重新为其创建这个句柄的内存对象，并执行调用。而如果该句柄在之前的Master周期中已经被关闭了，那么它就不能在这个Master周期内再次被重建了——这一机制就确保了即使由于网络原因使得Master接收到那些延迟或重发的网络数据包，也不会错误地重建一个已经关闭的句柄。

### 3.1.5 Paxos协议实现

Chubby服务端的基本架构大致分为三层：

- 最底层是容错日志系统（Fault-Tolerant Log），通过Paxos算法能够保证集群中所有机器上的日志完全一致，同时具备较好的容错性。
- 日志层之上是Key-Value类型的容错数据库（Fault-Tolerant DB），其通过下层的日志来保证一致性和容错性。
- 存储层之上就是Chubby对外提供的分布式锁服务和小文件存储服务。

Paxos算法的作用就在于保证集群内各个副本节点的日志能够保持一致。Chubby事务日志中的每一个Value对应Paxos算法中的一个Instance，由于Chubby需要对外提供不间断的服务，因此事务日志会无限增长，于是在整个Chubby运行过程中，会存在多个Paxos Instance。同时，Chubby会为每一个Paxos Instance都按序分配一个全局唯一的Instance编号，并将其顺序写入到事务日志中去。

## 3.2 Hypertable

Hypertable是一个使用C++语言开发的开源、高性能、可伸缩的数据库，其以Google的BigTable相关论文为基础指导，采用与HBase非常相似的分布式模型，其目的是要构建一个针对分布式海量数据的高并发数据库。

### 3.2.1 概述

目前Hypertable只支持最基本的添、删、改、查功能，对于事务处理和关联查询等关系型数据库的高级特性都尚未支持。同时，就少量数据记录的查询性能和吞吐量而言，Hypertable可能也不如传统的关系型数据库。和传统关系型数据库相比，Hypertable最大的优势在于以下几点。

- 支持对大量并发请求的处理。
- 支持对海量数据的管理。
- 扩展性良好，在保证可用性的前提下，能够通过随意添加集群中的机器来实现水平扩容。
- 可用性极高，具有非常好的容错性，任何节点的失效，既不会造成系统瘫痪也不会影响数据的完整性。

### 3.2.2 算法实现

# 第4章 ZooKeeper和Paxos

Apache ZooKeeper是由Apache Hadoop的子项目发展而来，于2010年11月正式成为了Apache的顶级项目。ZooKeeper为分布式应用提供了高效且可靠的分布式协调服务，提供了诸如统一命名服务、配置管理和分布式锁等分布式的基础服务。在解决分布式数据一致性方面，ZooKeeper并没有直接采用Paxos算法，而是采用了一种被称为ZAB（ZooKeeper Atomic Broadcast）的一致性协议。

## 4.1 初识ZooKeeper

### 4.1.1 ZooKeeper介绍

ZooKeeper是一个开发源代码的分布式协调服务，由知名互联网公司雅虎创建，是Google Chubby的开源实现。ZooKeeper的设计目标是将那些复杂且容易出错的分布式一致性服务封装，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。

#### ZooKeeper是什么

ZooKeeper是一个典型的分布式数据一致性的解决方案，分布式应用程序可以基于它实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁和分布式队列等功能。ZooKeeper可以保证如下分布式一致性特性。

**顺序一致性**

从同一个客户端发起的事务请求，最终将会严格地按照其发起顺序被应用到ZooKeeper中去。

**原子性**

所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群所有机器都成功应用了某一个事务，要么都没有应用，一定不会出现集群中部分机器应用了该事务，而另外一部分没有应用的情况。

**单一视图（Single System Image）**

无论客户端连接的是哪个ZooKeeper服务器，其看到的服务端数据模型都是一致的。

**可靠性**

一旦服务端成功地应用了一个事务，并完成对客户端的响应，那么该事务所引起的服务端状态变更将会被一直保留下来，除非有另一个事务又对其进行了变更。

**实时性**

通常人们看到实时性的第一反应是，一旦一个事务被成功应用，那么客户端能够立即从服务端上读取到这个事务变更后的最新数据状态。这里需要注意的是，ZooKeeper仅仅保证在一定的时间段内，客户端最终一定能够从服务端上读取到最新的数据状态。

#### ZooKeeper的设计目标

ZooKeeper致力于提供一个高性能、高可用，且具有严格的顺序访问控制能力（主要是写操作的严格顺序性）的分布式协调服务。高性能使得ZooKeeper能够应用于那些对系统吞吐有明确要求的大型分布式系统中，高可用使得分布式的单点问题得到了很好的解决，而严格的顺序访问控制使得客户端能够基于ZooKeeper实现一些复杂的同步原语。下面我们来具体看一下ZooKeeper的四个设计目标。

**目标一：简单的数据模型**

ZooKeeper使得分布式程序能够通过一个共享的、树型结构的名字空间来进行相互协调。

这里所说的树型结构的名字空间，是指ZooKeeper服务器内存中的一个数据模型，其由一系列被称为ZNode的数据节点组成，总的来说，其数据模型类似于一个文件系统，而ZNode之间的层级关系，就像文件系统的目录结构一样。不过和传统的磁盘文件系统不同的是，ZooKeeper将全量数据存储在内存中，以此来实现提高服务器吞吐，减少延迟的目的。关于ZooKeeper的数据模型，将会在7.1.1节中做详细阐述。

**目标二：可以构建集群**

一个ZooKeeper集群通常由一组机器组成，一般3~5台机器就可以组成一个可用的ZooKeeper集群了。

组成ZooKeeper集群的每台机器都会在内存中维护当前的服务器状态，并且每台机器之间都互相保持着通信。值得一提的是，只要集群中存在超过一半的机器能够正确工作，那么整个集群就能够正常对外服务。

ZooKeeper的客户端程序会选择和集群中任意一台机器共同来创建一个TCP连接，而一旦客户端和某台ZooKeeper服务器之间的连接断开后，客户端会自动连接到集群中的其他机器。关于ZooKeeper客户端的工作原理，将会在7.3节中做详细阐述。

**目标三：顺序访问**

对于来自客户端的每个更新请求，ZooKeeper都会分配一个全局唯一的递增编号，这个编号反映了所有事务操作的先后顺序，应用程序可以使用ZooKeeper的这个特性来实现更高层次的同步原语。关于ZooKeeper的事务请求处理和事务ID的生成，将会在7.8节中做详细阐述。

**目标四：高性能**

由于ZooKeeper将全量数据存储在内存中，并直接服务于客户端的所有非事务请求，因此它尤其适用于以读操作为主的应用场景。

### 4.1.2 ZooKeeper从何而来

ZooKeeper最早起源于雅虎研究院的一个研究小组。在当时，研究人员发现，在雅虎内部很多大型系统基本都需要依赖一个类似的系统来进行分布式协调，但是这些系统往往都存在分布式单点问题。所以雅虎的开发人员就视图开发一个通用的无单点问题的分布式协调框架，以便让开发人员将精力集中在处理业务逻辑上。

### 4.1.3 ZooKeeper的基本概念

#### 集群角色

通常在分布式系统中，构成一个集群的每一台机器都有自己的角色，最典型的集群模式就是Master/Slave模式（主备模式）。在这种模式中，我们把能够处理所有写操作的机器称为Master机器，把所有通过异步复制方式获取最新数据，并提供读服务的机器称为Slave机器。

而在ZooKeeper中，这些概念被颠覆了。它没有沿用传统的Master/Slave概念，而是引入了Leader、Follower和Observer三种角色。ZooKeeper集群中的所有机器通过一个Leader选举过程来选定一台被称为“Leader”的机器，Leader服务器为客户端提供读和写服务。除Leader外，其他机器包括Follower和Obsever。Follower和Observer都能够提供读服务，唯一的区别在于，Observer机器不参与Leader选举过程，也不参与写操作的“过半写成功”策略，因此Observer可以在不影响写性能的情况下提升集群的读性能。关于ZooKeeper的集群结构和各角色的工作原理，将会在7.7节中做详细阐述。

#### 会话（Session）

Session是指客户端会话，在讲解会话之前，我们首先来了解一个客户端连接。在ZooKeeper中，一个客户端连接是指客户端和服务端之间的一个TCP长连接。ZooKeeper对外的服务端口默认是2181，客户端启动的时候，首先会与服务器建立一个TCP连接，从第一次连接建立开始，客户端会话的生命周期也开始了，提供这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向ZooKeeper服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的Watch事件通知。Session的sessionTimeout值用来设置一个客户端会话的超时事件。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在sessionTimeout规定的事件内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。关于ZooKeeper客户端会话，将会在7.4节中做详细阐述。

#### 数据节点（Znode）

在谈到分布式的时候，我们通常说的“节点”是指组成集群的每一台机器。然而，在ZooKeeper中，“节点”分为两类，第一类同类是指构成集群的机器，我们称之为机器节点；第二类则是指数据模型中的数据单元，我们称之为数据节点——ZNode。ZooKeeper将所有数据存储在内存中，数据模型是一棵树（ZNode Tree），由斜杠（`/`）进行分割的路径，就是一个Znode，例如`/foo/path1`。每个ZNode上都会保存自己的数据内容，同时还会保存一系列属性信息。

在ZooKeeper中，ZNode可以分为持久节点和临时节点两类。所谓持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在ZooKeeper上。而临时节点就不一样了，它的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。另外，ZooKeeper还允许用户为每个节点添加一个特殊的属性：SEQUENTIAL。一旦节点被标记上这个属性，那么在这个节点被创建的时候，ZooKeeper会自动在其节点后面追加一个整型数字，这个整型数字是一个由父节点维护的自增数字。

关于ZooKeeper的节点特性以及完整的数据模型，将会在7.1节中做详细阐述。

#### 版本

在前面我们已经提到，ZooKeeper的每个ZNode上都会存储数据，对应于每个ZNode，ZooKeeper都会为其维护一个叫作Stat的数据结构，Stat中记录了这个ZNode的三个数据版本，分别是version（当前ZNode的版本）、cversion（当前ZNode子节点的版本）和aversion（当前ZNode的ACL版本）。关于ZooKeeper数据模型中的版本，将会在7.1.3节中做详细阐述。

#### Watcher

Watcher（事件监听器），是ZooKeeper中的一个很重要的特性。ZooKeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是ZooKeeper实现分布式协调服务的重要特性。关于ZooKeeper Watcher机制的特性和使用，将会在7.1.4节中做详细阐述。

#### ACL

ZooKeeper采用ACL（Access Control Lists）策略来进行权限控制，类似于UNIX文件系统的权限控制。ZooKeeper定义了如下5种权限。

- **CREATE**：创建子节点的权限。
- **READ**：获取节点数据和子节点列表的权限。
- **WRITE**：更新节点数据和权限。
- **DELETE**：删除子节点的权限。
- **ADMIN**：设置节点ACL的权限。

其中尤其需要注意的是，CREATE和DELETE这两种权限都是针对子节点的权限控制。关于ZooKeeper权限控制的原理和使用方式，将会在7.1.5节中做详细阐述。

### 4.1.4 为什么选择ZooKeeper

很遗憾的是，在解决分布式数据一致性上，除了ZooKeeper之外，目前还没有一个成熟稳定且被大规模应用的解决方案。ZooKeeper无论从性能、易用性还是稳定性上来说，都已经达到了一个工业级产品的标准。

其次，ZooKeeper是开放源代码的

另外，ZooKeeper是免费的

最后，ZooKeeper已经得到了广泛的应用。诸如Hadoop、HBase、Storm和Solr等越来越多的大型分布式项目都已经将ZooKeeper作为其核心组件，用于分布式协调。

## 4.2 ZooKeeper的ZAB协议

### 4.2.1 ZAB协议

在深入了解ZooKeeper之前，相信很多读者都会认为ZooKeeper就是Paxos算法的一个实现。但事实上，ZooKeeper并没有完全采用Paxos算法，而是使用了一种称为ZooKeeper Atomic Broadcast（ZAB，ZooKeeper原子消息广播协议）的协议作为其数据一致性的核心算法。

ZAB协议是为分布式协调服务ZooKeeper专门设计的一种支持崩溃恢复的原子广播协议。ZAB协议的开发设计人员在协议设计之初并没有要求其具有很好的扩展性，最初只是为雅虎公司内部那些高吞吐量、低延迟、健壮、简单的分布式系统场景设计的。在ZooKeeper的官方文档中也指出，ZAB协议并不像Paxos算法那样，是一种通用的分布式一致性算法，它是一种特别为ZooKeeper设计的崩溃可恢复的原子消息广播算法。

在ZooKeeper中，主要依赖ZAB协议来实现分布式数据一致性，基于该协议，ZooKeeper实现了一种主备模式的系统架构来保持集群中各副本之间数据的一致性。具体的，ZooKeeper使用一个单一的主进程来接收并处理客户端的所有事务请求，并采用ZAB的原子广播协议，将服务器数据的状态变更以事务Proposal的形式广播到所有的副本进程上去。ZAB协议的这个主备模型架构保证了同一时刻集群中只能够有一个主进程来广播服务器的状态变更，因此能够很好地处理客户端大量的并发请求。另一方面，考虑到在分布式环境中，顺序执行的一些状态变更其前后会存在一定的依赖关系，有些状态变更必须依赖于比它早生成的那些状态变更。这样的依赖关系也对ZAB协议提出了一个要求：ZAB协议必须能够保证一个全局的变更序列被顺序应用，也就是说，ZAB协议需要保证如果一个状态变更已经被处理了，那么所有其依赖的状态变更都应该已经被提前处理掉了。最后，考虑到主进程在任何时候都有可能出现崩溃退出或重启现象，因此，ZAB协议还需要做到在当前主进程出现上述异常情况的时候，依旧能够正常工作。

ZAB协议的核心是定义了对于那些会改变ZooKeeper服务器数据状态的事务请求的处理方式，即：

> 所有事务请求必须由一个全局唯一的服务器来协调处理，这样的服务器被称为Leader服务器，而余下的其他服务器则成为Follower服务器。Leader服务器负责将一个客户端事务请求转换成一个事务Proposal（提议），并将该Proposal分发给集群中所有的Follower服务器。之后Leader服务器需要等待所有Follower服务器的反馈，一旦超过半数的Follower服务器进行了正确的反馈后，那么Leader就会再次向所有的Follower服务器分发Commit消息，要求其将前一个Proposal进行提交。

### 4.2.2 协议介绍

从上面的介绍中，我们已经了解了ZAB协议的核心，现在我们就来详细讲解下ZAB协议的具体内容。ZAB协议包括两种基本的模式，分别是崩溃恢复和消息广播。当整个服务框架在启动过程中，或是当Leader服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB协议就会进入恢复模式并选举产生新的Leader服务器。当选举产生了新的Leader服务器，同时集群中已经有过半的机器与该Leader服务器完成了状态同步之后，ZAB协议就会退出恢复模式。其中，所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和Leader服务器的数据状态保持一致。

当集群中已经有过半的Follower服务器完成了和Leader服务器的状态同步，那么整个服务框架就可以进入消息广播模式了。当一台同样遵守ZAB协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个Leader服务器在负责进行消息广播，那么新加入的服务器就会自觉地进入数据恢复模式：找到Leader所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。正如上文介绍中所说的，ZooKeeper设计成只允许唯一的一个Leader服务器来进行事务请求的处理。Leader服务器在接收到客户端的事务请求后，会生成对应的事务提案并发起一轮广播协议；而如果集群中的其他机器接收到客户端的事务请求，那么这些非Leader服务器会首先将这个事务请求转发给Leader服务器。

当Leader服务器出现崩溃退出或机器重启，亦或是集群中已经不存在过半的服务器与该Leader服务器保持正常通信时，那么在重新开始新一轮的原子广播事务操作之前，所有进程首先会使用崩溃恢复协议来使彼此达到一个一致的状态，于是整个ZAB流程就会从消息广播模式进入到崩溃恢复模式。

一个机器要成为新的Leader，必须获得过半进程的支持，同时由于每个进程都有可能会崩溃，因此，在ZAB协议运行过程中，前后会出现多个Leader，并且每个进程也有可能会多次成为Leader。进入崩溃恢复模式后，只要集群中存在过半的服务器能够彼此进行正常通信，那么就可以产生一个新的Leader并再次进入消息广播模式。举个例子来说，一个由3台机器组成的ZAB服务，通常由1个Leader、2个Follower服务器组成。某一个时刻，假如其中一个Follower服务器挂了，整个ZAB集群是不会中断服务的，这是因为Leader服务器依然能够获得过半机器（包括Leader自己）的支持。

接下来我们就重点讲解一下ZAB协议的消息广播和崩溃恢复过程。

#### 消息广播

ZAB协议的消息广播过程使用的是一个原子广播协议，类似于一个二阶段提交过程。针对客户端的事务请求，Leader服务器会为其生成对应的事务Proposal，并将其发送给集群中其余所有的机器，然后再分别收集各自的选票，最后进行事务提交。

在第2章中，我们已经详细讲解了关于二阶段提交协议的内容，而此处ZAB协议中涉及的二阶段提交过程则与其略有不同。在ZAB协议的二阶段提交过程中，移除了中断逻辑，所有的Follower服务器要么正常反馈Leader提出的事务Proposal，要么就抛弃Leader服务器。同时，ZAB协议将二阶段提交中的中断逻辑移除意味着我们可以在过半的Follower服务器已经反馈Ack之后就开始提交事务Proposal了，而不需要等待集群中所有的Follower服务器都反馈响应。当然，在这种简化了的二阶段提交模型下，是无法处理Leader服务器崩溃退出而带来的数据不一致问题的，因此在ZAB协议中添加了另一个模式，即采用崩溃恢复模式来解决这个问题。另外，整个消息广播协议是基于具有FIFO特性的TCP协议来进行网络通信的，因此能够很容易地保证消息广播过程中消息接收与发送的顺序性。

在整个消息广播过程中，Leader服务器会为每个事务请求生成对应的Proposal来进行广播，并且在广播事务Proposal之前，Leader服务器会首先为这个事务Proposal分配一个全局单调递增的唯一ID，我们称之为事务ID（即ZXID）。由于ZAB协议需要保证每一个消息严格的因果关系，因此必须将每一个事务Proposal按照其ZXID的先后顺序来进行排序与处理。

具体的，在消息广播过程中，Leader服务器会为每一个Follower服务器都各自分配一个单独的队列，然后将需要广播的事务Proposal依次放入这些队列中去，并且根据FIFO策略进行消息发送。每一个Follower服务器在接收到这个事务Proposal之后，都会首先将其以事务日志的形式写入到本地磁盘中去，并且在成功写入后反馈给Leader服务器一个Ack响应。当Leader服务器接收到超过半数Follower的Ack响应后，就会广播一个Commit消息给所有的Follower服务器以通知其进行事务提交，同时Leader自身也会完成对事务的提交，而每一个Follower服务器在接收到Commit消息后，也会完成对事务的提交。

#### 崩溃恢复

上面我们主要讲解了ZAB协议中的消息广播过程。ZAB协议的这个基于原子广播协议的消息广播过程，在正常情况下运行非常良好，但是一旦Leader服务器出现崩溃，或者说由于网络原因导致Leader服务器失去了与过半Follower的联系，那么就会进入崩溃恢复模式。在ZAB协议中，为了保证程序的正确运行，整个恢复过程结束后需要选举出一个新的Leader服务器。因此，ZAB协议需要一个高效且可靠的Leader选举算法，从而确保能够快速地选举出新的Leader。同时，Leader选举算法不仅仅需要让Leader自己知道其自身已经被选举为Leader，同时还需要让集群中所有其他机器也能够快速地感知到选举产生的新的Leader服务器。

**基本特性**

根据上面的内容，我们了解到，ZAB协议规定了如果一个事务Proposal在一台机器上被处理成功，那么应该在所有的机器上被处理成功，哪怕机器出现故障崩溃。接下来我们看看在崩溃恢复过程中，可能会出现的两个数据不一致性的隐患及针对这些情况ZAB协议所需要保证的特性。

*ZAB协议需要确保那些已经在Leader服务器上提交的事务最终被所有服务器都提交*

*ZAB协议需要确保丢弃那些只在Leader服务器上被提出的事务*

结合上面提到的这两个崩溃恢复过程中需要处理的特殊情况，就决定了ZAB协议必须设计这样一个Leader选举算法：能够确保提交已经被Leader提交的事务Proposal，同时丢弃已经被跳过的事务Proposal。针对这个要求，如果让Leader选举算法能够保证新选举出来的Leader服务器拥有集群中所有机器最高编号（即ZXID最大）的事务Proposal，那么就可以保证这个新选举出来的Leader一定具有所有已经提交的提案。更为重要的是，如果让具有最高编号事务Proposal的机器来成为Leader，就可以省去Leader服务器检查Proposal的提交和丢弃工作的这一步操作了。

**数据同步**

完成Leader选举之后，在正式开始工作（即接收客户端的事务请求，然后提出新的提案）之前，Leader服务器会首先确认事务日志中的所有Proposal是否都已被集群中过半的机器提交了，即是否完成数据同步。下面我们就来看看ZAB协议的数据同步过程。

所有正常运行的服务器，要么成为Leader，要么成为Follower并和Leader保持同步。Leader服务器需要确保所有的Follower服务器能够接收到每一条事务Proposal，并且能够正确地将所有已经提交了的事务Proposal应用到内存数据库中去。具体的，Leader服务器会为每一个Follower服务器都准备一个队列，并将那些没有被各Follower服务器同步的事务以Proposal消息的形式逐个发送给Follower服务器，并在每一个Proposal消息后面紧接着再发送一个Commit消息，以表示该事务已经被提交。等到Follower服务器将所有其尚未同步的事务Proposal都从Leader服务器上同步过来并成功应用到本地数据库中后，Leader服务器就会将该Follower服务器加入到真正的可用Follower列表中，并开始之后的其他流程。

上面讲到的是正常情况下的数据同步逻辑，下面来看ZAB协议是如何处理那些需要被丢弃的事务Proposal的。在ZAB协议的事务编号ZXID设计中，ZXID是一个64位的数字，其中低32位可以看作是一个简单的单调递增的计数器，针对客户端的每一个事务请求，Leader服务器在产生一个新的事务Proposal的时候，都会对该计数器进行加1操作；而高32位则代表了Leader周期epoch的编号，每当选举产生一个新的Leader服务器，就会从这个Leader服务器上取出其本地日志中最大事务Proposal的ZXID，并从该ZXID中解析出对应的epoch值，然后再对其进行加1操作，之后就会以此编号作为新的epoch，并将低32位置0来开始生成新的ZXID。ZAB协议中的这一通过epoch编号来区分Leader周期变化的策略，能够有效地避免不同的Leader服务器错误地使用相同的ZXID变化提出不一样的事务Proposal的异常情况，这对于识别在Leader崩溃恢复前后生成的Proposal非常有帮助，大大简化和提升了数据恢复流程。

基于这样的策略，当一个包含了上一个Leader周期中尚未提交过的事务Proposal的服务器启动时，其肯定无法成为Leader，原因很简单，因为当前集群中一定包含一个Quorum集合，该集合中的机器一定包含了更高epoch的事务Proposal，因此这台机器的事务Proposal肯定不是最高，也就无法成为Leader了。当这台机器加入到集群中，以Follower角色连接上Leader服务器之后，Leader服务器会根据自己服务器上最后被提交的Proposal来和Follower服务器的Proposal进行比对，比对的结果当然是Leader会要求Follower进行一个回退操作——回退到一个确实已经被集群中过半机器提交的最新的事务Proposal。

### 4.2.3 深入ZAB协议

在4.2.2节中，我们已经基本介绍了ZAB协议的大体内容以及在实际运行过程中消息广播和崩溃恢复这两个基本的模式，下面将从系统模型、问题描述、算法描述和运行分析四方面来深入了解ZAB协议。

#### 系统模型

通常在一个由一组进程 $\prod = {P_1,P_2,\dots,P_n}$组成的分布式系统中，其每一个进程都具有各自的存储设备，各进程之间通过相互通信来实现消息的传递。一般的，在这样的一个分布式系统中，每一个进程都随时有可能出现一次或多次的崩溃退出，当然，这些进程会在恢复之后再次加入到进程组$\prod$中去。如果一个进程正常工作，那么我们称该进程处于UP状态；如果一个进程崩溃了，那么我们称其处于DOWN状态。事实上，当集群中存在过半的处于UP状态的进程组成一个进程子集之后，就可以进行正常的消息广播了。我们将这样的一个进程子集Quorum（下文中使用“Q”来表示），并假设这样的Q已经存在，其满足：
$$
\Lambda \quad \forall Q, \quad Q\subseteq \prod \\
\Lambda \quad \forall Q_1和Q_2, \quad Q_1\cap Q_2 \ne \varnothing
$$
上述集合关系式表示，存在这样的一个进程子集Q，其必定是进程组$\prod$的子集；同时，存在任意两个进程子集$Q_1$和$Q_2$，其交集必定非空。

我们使用$P_i$和$P_j$来分别表示进程组$\prod$中的两个不同进程，使用$C_{ij}$来表示进程$P_i$和$P_j$之间的网络通信通道，其满足如下两个基本特性。

**完整性（Integrity）**

进程$P_j$如果收到来自进程$P_i$的消息m，那么进程$P_i$一定确实发送了消息m。

**前置性（Prefix）**

如果进程$P_j$收到了消息m，那么存在这样的消息m'：如果消息m'是消息m的前置消息，那么$P_j$务必先接收到消息m'，然后再接收到消息m。我们将存在这种前置性关系的两个消息表示为$m'\prec m$。前置性是整个协议设计中最关键的一点，由于每一个消息都有可能是基于之前的消息来进行的，因此所有的消息都必须按照严格的先后顺序来进行处理。

#### 问题描述

在了解了ZAB协议所针对应用的系统模型后，我们再来看看其所要解决的实际分布式问题。再前文的介绍中，我们已经了解到ZooKeeper是一个高可用的分布式协调服务，在雅虎的很多大型系统上得到应用。这类应用有个共同的特点，即通常都存在大量的客户端进程，并且都依赖ZooKeeper来完成一系列诸如可靠的配置存储和运行时状态记录等分布式协调工作。鉴于这些大型应用对ZooKeeper的依赖，因此ZooKeeper必须具备高吞吐和低延迟的特性，并且能够很好地在高并发情况下完成分布式数据地一致性处理，同时能够优雅地处理运行时故障，并具备快速地从故障中恢复过来的能力。

ZAB协议是整个ZooKeeper框架的核心所在，其规定了任何时候都需要保证只有一个主进程负责进行消息广播，而如果主进程崩溃了，就需要选举出一个新的主进程。主进程的选举机制和消息广播机制是紧密相关的。随着时间的推移，会出现无限多个主线程并构成一个主进程序列：$P_1,P_2,\dots,P_{e-1},P_e$，其中$P_e\in \prod$，e表示主进程序列号，也被称作主进程周期。对于这个主进程序列上的任意两个主进程来说，如果e小于e'，那么我们会发生崩溃然后再次恢复，因此会出现这样的情况：存在这样的$P_e$和$P_{e'}$，它们本质上是同一个进程，只是处于不同的周期中而已。

**主进程周期**

为了保证主进程每次广播出来的事务消息都是一致的，我们必须确保ZAB协议只有在充分完成崩溃恢复阶段之后，新的主进程才可以开始生成新的事物消息广播。为了实现这个目的，我们假设各个进程都实现了类似于ready(e)这样的一个函数调用，在运行过程中，ZAB协议能够非常明确地告知上层系统（指主进程和其他副本进程）是否可以开始进行事务消息的广播，同时，在调用ready(e)函数之后，ZAB还需要为当前主进程设置一个实例值。实例值用于唯一标识当前主进程的周期，在进行消息广播的时候，主进程使用该实例值来设置事务标识中的epoch字段——当然，ZAB需要保证实例值在不同的主进程周期中是全局唯一的。如果一个主进程周期e早于另一个主进程周期e'，那么将其表示为$e\prec e'$。

**事务**

我们假设各个进程都存在一个类似于transaction(v, z)这样的函数调用，来实现主进程对状态变更的广播。主进程每次对transaction(v, z)函数的调用都包含了两个字段：事务内容v和事务标识z，而每一个事务标识z=<e, c>也包含两个组成部分，前者是主进程周期e，后者是当前主进程周期内的事务计数c。我们使用epoch(z)来表示一个事务标识中的主进程周期epoch，使用counter(z)来表示事务标识中的事务计数。

针对每一个新的事务，主进程都会首先将事务计数c递增。在实际运行过程中，如果一个事务标识z优先于另一个事务标识z'，那么就有两种情况：一种情况是主进程周期不同，即epoch(z) < epoch(z')；另一种情况则是主进程周期一致，但是事务计数不同，即epoch(z) = epoch(z') 且 counter(z) < counter(z')，无论哪种情况，均使用 $z \prec z'$来表示。

#### 算法描述

下面我们将从算法描述角度来深入讲解ZAB协议的内部原理。整个ZAB协议主要包括消息广播和崩溃恢复两个过程，进一步可以细分为三个阶段，分别是发现（Discovery）、同步（Synchronization）和广播（Broadcast）阶段。组成ZAB协议的每一个分布式进程，会循环地执行这三个阶段，我们将这样一个循环称为一个主进程周期。

为了更好地对ZAB协议各阶段的算法流程进行描述，我们首先定义一些专有标识和术语，如表4-1所示。

| 术语名           | 说明                                                         |
| ---------------- | ------------------------------------------------------------ |
| $F_{\cdot p}$    | Follower f 处理过的最后一个事务Proposal                      |
| $F_{\cdot zxid}$ | Follower f 处理过的历史事务Proposal中最后一个事务Proposal的事务标识ZXID |
| $h_f$            | 每一个Follower f通常都已经处理（接受）了不少事务Proposal，并且会有一个针对已经处理过的事务的集合，将其表示为$h_f$，表示Follower f 已经处理过的事务序列 |
| $I_e$            | 初始化历史记录，在某一个主进程周期epoch e中，当准Leader完成阶段一之后，此时它的$h_f$就被标记为$I_e$。关于ZAB协议的阶段一过程，将在下文中做详细讲解。 |

下面我们就从发现、同步和广播这三个阶段展开来讲解ZAB协议的内部原理。

**阶段一：发现**

阶段一主要就是Leader选举过程，用于在多个分布式进程中选举出主进程，准Leader L和 Follower F的工作流程分别如下。

*步骤F.1.1*

Follower F 将自己最后接受的事务Proposal的epoch值$CEPOCH(F_{\cdot p})$ 发送给准Leader L。

*步骤L.1.1*

当接受到来自过半Follower的 $CEPOCH(F_{\cdot p})$ 消息后，准Leader L会生成 NEWEPOCH(e') 消息给这些过半的Follower。

关于这个epoch值e'，准Leader L会从所有接收到的 $CEPOCH(F_{\cdot p})$ 消息中选取出最大的 epoch 值，然后对其进行加1操作，即为$e'$。

*步骤F.1.2*

当Follower接收到来自准Leader L的 NEWEPOCH(e') 消息后，如果其检测到当前的 $CEPOCH(F_{\cdot p})$ 值小于e'，那么就会将 $CEPOCH(F_{\cdot p})$ 赋值为 e'，同时向这个准Leader L 反馈 Ack 消息。在这个反馈消息（$ACK-E(F_{\cdot p}, h_f)$）中，包含了当前该Follower的epoch $CEPOCH(F_{\cdot p})$，以及该Follower的历史事务Proposal集合：$h_f$。



当Leader L接收到来自过半Follow的确认消息Ack之后，Leader L就会从这过半服务器中选取出一个Follower F，并使用其作为初始化事务集合 $I_{e'}$。

关于这个Follower F的选取，对于Quorum中其他任意一个Follower F'，F需要满足以下两个条件中的一个：
$$
CEPOCH(F'_{\cdot p}) \lt CEPOCH(F_{\cdot p}) \\
(CEPOCH(F'_{\cdot p}) = CEPOCH(F_{\cdot p})) \& (F'_{\cdot zxid} \prec F_{\cdot zxid} 或 F'_{\cdot zxid} = F_{\cdot zxid})
$$
至此，ZAB协议完成阶段一的工作流程。

**阶段二：同步**

在完成发现流程之后，就进入了同步阶段。在这一阶段中，Leader L 和 Follower F的工作流程分别如下。

*步骤L.2.1*

Leader L 会将 e' 和 le' 以 NEWLEADER(e', $I_{e'}$)消息的形式发送给所有Quorum中的Follower。

*步骤F.2.1*

当Follower接收到来自Leader L 的 NEWLEADER(e', $I_{e'}$) 消息后，如果 Follower 发现 $CEPOCH(F_{\cdot p}) \ne e'$，那么直接进入下一轮循环，因为此时Follower发现自己还在上一轮，或者更上轮，无法参与本轮的同步。

如果 CEPOCH($F_{\cdot p}$) = e'，那么Follower就会执行事务应用操作。具体的，对于每一个事务Proposal：$<v, z>\in I_{e'}$，Follower都会接受 <e', <v, z>>。最后，Follower会反馈给Leader，表面自己已经接受并处理了所有$I_{e'}$中的事务Proposal。

*步骤L.2.2*

当Leader接收到来自过半Follower针对NEWLEADER(e', $I_{e'}$)的反馈消息后，就会向所有的Follower发送Commit消息。至此Leader完成阶段二。

*步骤F.2.2*

当Follower收到来自Leader的Commit消息后，就会依此处理并提交所有在$I_{e'}$ 中未处理的事务。至此Follower完成阶段二。

**阶段三：广播**

完成同步阶段之后，ZAB协议就可以正式开始接受客户端新的事务请求，并进行消息广播流程。

*步骤L.3.1*

Leader L 接收到客户端新的事物请求后，会生成对应的事务Proposal，并根据ZXID的顺序向所有Follower发送提案<e', <v, z>>，其中 epoch(z) = e'。

*步骤F.3.1*

Follower根据消息接收的先后次序来处理这些来自Leader的事务Proposal，并将他们追加到$h_f$中去，之后再反馈给Leader。

*步骤L.3.2*

当Leader接收到来自过半Follower针对事务Proposal <e', <v, z>>的 Ack 消息后，就会发送Commit <e', <v, z>> 消息给所有的Follower，要求它们进行事务的提交。

*步骤F.3.2*

当Follower F接收到来自Leader 的 Commit <e', <v, z>> 消息后，就会开始提交事务 Proposal <e', <v, z>>。 需要注意的是，此时该Follower F 必定已经提交了事务 Proposal <v', z'>，其中$<v', z'>\in h_f, z'\prec z$。

以上就是整个ZAB协议的三个核心工作流程，如图4-5所示是在整个过程中各进程之间的消息收发情况，各消息说明依此如下：

CEPOCH：Follower进程向准Leader发送自己处理过的最后一个事务Proposal的epoch值。

NEWEPOCH：准Leader进程根据接收的各进程的epoch，来生成新一轮周期的epoch值。

ACK-E：Follower进程反馈准Leader进程发来的NEWEPOCH消息。

NEWLEADER：准Leader进程确立自己的领导地位，并发送NEWLEADER消息给各进程。

ACK-LD：Follower进程反馈Leader进程发来的NEWLEADER消息。

COMMIT-LD：要求Follower进程提交相应的历史事务Proposal。

PROPOSE：Leader进程生成一个针对客户端请求的Proposal。

ACK：Follower进程反馈Leader进程发来的PROPOSAL消息。

COMMIT：Leader发送COMMIT消息，要求所有进程提交事务PROPOSE。

在正常运行过程中，ZAB协议会一直运行于阶段三反复地进行消息广播流程。如果出现Leader崩溃或其他原因导致Leader缺失，那么此时ZAB协议会再次进入阶段一，重新选举新的Leader。

~~~mermaid
sequenceDiagram
	participant Follower1
	participant Leader
	participant Follower2
	alt 发现
        Follower1 ->> Leader: CEPOCH
        Follower2 ->> Leader: CEPOCH
        Leader -->> Follower1: NEWEPOCH
        Leader -->> Follower2: NEWEPOCH
		Follower2 ->> Leader: ACK-E
		Follower1 ->> Leader: ACK-E
	end
	
	alt 同步
		Leader -->> Follower2: NEWLEADER
        Leader -->> Follower1: NEWLEADER
        Follower1 ->> Leader: ACK-LD
		Follower2 ->> Leader: ACK-LD
		Leader -->> Follower1: COMMIT-LD
        Leader -->> Follower2: COMMIT-LD
	end
	
	alt 广播
		Leader -->> Follower1: PROPOSE
        Leader -->> Follower2: PROPOSE
        Follower1 ->> Leader: ACK
        Leader -->> Follower1: COMMIT
		Follower2 ->> Leader: ACK
        Leader -->> Follower2: COMMIT
	end
~~~

#### 运行分析

在ZAB协议的设计中，每一个进程都有可能处于以下三种状态之一。

- **LOOKING**：Leader选举阶段
- **FOLLOWING**：Follower服务器和Leader保持同步状态
- **LEADING**：Leader服务器作为主进程领导状态

组成ZAB协议的所有进程启动的时候，其初始化状态都是LOOKING状态，此时进程组中不存在Leader。所有处于这种状态的进程，都会试图去选举出一个新的Leader。随后，如果进程发现已经选举出新的Leader了，那么它就会马上切换到FOLLOWING状态，并开始和Leader保持同步。这里，我们将处于FOLLOWING状态的进程称为Follower，将处于LEADING状态的进程称为Leader。考虑到Leader进程随时会挂掉，当检测出Leader已经崩溃或者是放弃了领导地位时，其余的Follower进程就会转换到LOOKING状态，并开始进行新一轮的Leader选举。因此在ZAB协议运行过程中，每个进程都会在LEADING、FOLLOWING和LOOKING状态之间不断地转换。

Leader的选举过程发送在前面两个阶段。图4-5展示了在一次Leader选举过程中，各进程之间的消息发送与接收情况。需要注意的是，只有在完成了阶段二，即完成各进程之间的数据同步之后，准Leader进程才能真正成为新的主进程周期中的Leader。具体的，我们将一个可用的Leader定义如下：

> 如果一个准Leader $L_e$ 接收到来自过半的Follower进程针对$L_e$的NEWLEADER(e, $I_e$)反馈消息，那么$L_e$就成为了周期 e 的Leader。

完成Leader选举以及数据同步之后，ZAB协议就进入了原子广播阶段。在这一阶段中，Leader会以队列的形式为每一个与自己保持同步的Follower创建一个操作队列。同一时刻，一个Follower只能和一个Leader保持同步，Leader进程与所有的Follower之间都通过心跳检测机制来感知彼此的情况。如果Leader能够在超时时间内正常收到心跳检测，那么Follower就会一直与该Leader保持连接。而如果在指定的超时时间内Leader无法从过半的Follower进程那里接收到心跳检测，或者是TCP连接本身断开了，那么Leader就会终止对当前周期的领导，并转换到LOOKING状态，所有的Follower也会选择放弃这个Leader，同时转换到LOOKING状态。之后，所有进程就会开始新一轮的Leader选举，并在选举产生新的Leader之后开始新一轮的主进程周期。

### 4.2.4 ZAB和Paxos算法的联系与区别

ZAB协议并不是Paxos算法的一个典型实现，在讲解ZAB和Paxos之间的区别之前，我们首先来看下两者的联系。

- 两者都存在一个类似于Leader进程的角色，由其负责协调多个Follower进程的运行。
- Leader进程都会等待超过半数的Follower做出正确的反馈后，才会将一个提案进行提交。
- 在ZAB协议中，每个Proposal中都包含了一个epoch值，用来代表当前的Leader周期，在Paxos算法中，同样存在这样的一个标识，只是名字变成了Ballot。

在Paxos算法中，一个新选举产生的主进程会进行两个阶段的工作。第一阶段被称为读阶段，在这个阶段中，这个新的主进程会通过和所有其他进程进行通信的方式来收集上一个主进程提出的提案，并将它们提交。第二阶段被称为写阶段，在这个阶段，当前主进程开始提出它自己的提案。在Paxos算法设计的基础上，ZAB协议额外添加了一个同步阶段。在同步阶段之前，ZAB协议也存在一个和Paxos算法中的读阶段非常类似的过程，称为发现（Discovery）阶段。在同步阶段中，新的Leader会确保存在过半的Follower已经提交了之前Leader周期中的所有事务Proposal。这一同步阶段的引入，能够有效地保证Leader在新的周期中提出事务Proposal之前，所有的进程都已经完成了对之前所有事务Proposal的提交。一旦完成同步阶段之后，那么ZAB就会执行和Paxos算法类似的写阶段。

总的来讲，ZAB协议和Paxos算法的本质区别在于，两者的设计目标不太一样。ZAB协议主要用于构建一个高可用的分布式数据主备系统，例如ZooKeeper，而Paxos算法则是用于构建一个分布式的一致性状态机系统。

# 第5章 使用ZooKeeper

# 第6章 ZooKeeper的典型应用场景

## 6.1 典型应用场景及实现

本节将重点围绕数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁和分布式队列等方面来讲解ZooKeeper的典型应用场景及实现。

### 6.1.1 数据发布/订阅

数据发布/订阅（Publish/Subscribe）系统，即所谓的配置中心，顾名思义就是发布者将数据发布到ZooKeeper的一个或一系列节点上，供订阅者进行数据订阅，进而达到动态获取数据的目的，实现配置信息的集中式管理和数据的动态更新。

发布/订阅系统一般有两种设计模式，分别是推（Push）模式和拉（Pull）模式。在推模式中，服务端主动将数据更新发送给所有订阅的客户端；而拉模式则是由客户端主动发起请求来获取最新数据，通常客户端都采用定时进行轮询拉取的方式。ZooKeeper采用的是推拉相结合的方式：客户端向服务端注册自己需要关注的节点，一旦该节点的数据发生变更，那么服务端就会向相应客户端发送Watcher事件通知，客户端接收到这个消息通知之后，需要主动到服务端获取最新的数据。

如果将配置信息存放到ZooKeeper上进行集中管理，那么通常情况下，应用在启动的时候会主动到ZooKeeper服务端上进行一次配置信息的获取，同时，在指定节点上注册一个Watcher监听，这一依赖，但凡配置信息发生变更，服务端都会实时通知到所有订阅的客户端，从而达到实时获取最新配置信息的目的。

接下来我们就以一个“数据库切换”的应用场景展开，看看如何使用ZooKeeper来实现配置管理。

**配置存储**

在进行配置管理之前，首先我们需要将初始化配置存储到ZooKeeper上去。一般情况下，我们可以在ZooKeeper上选取一个数据节点用于配置的存储，例如`/app1/database_config`(以下简称“配置节点”)

我们将需要集中管理的配置信息写入到该数据节点中去。

**配置获取**

集群中每台机器在启动初始化阶段，首先会从上面提到的ZooKeeper配置节点上读取数据库信息，同时，客户端还需要在该配置节点上注册一个数据变更的Watcher监听，一旦发生节点数据变更，所有订阅的客户端都能够获取到数据变更通知。

**配置变更**

在系统运行过程中，可能会出现需要进行数据库切换的情况，这个时候就需要进行配置变更。借助ZooKeeper，我们只需要对ZooKeeper上配置节点的内容进行更新，ZooKeeper就能够帮我们将数据变更的通知发送到各个客户端，每个客户端在接收到这个变更通知后，就可以重新进行最新数据的获取。

### 6.1.2 负载均衡

根据维基百科上的定义，负载均衡（Load Balance）是一种相当常见的计算机网络技术，用来对多个计算机（计算机集群）、网络连接、CPU、磁盘驱动器或其他资源进行分配负载，以达到优化资源使用、最大化吞吐率、最小化响应时间和避免过载的目的。通常负载均衡可以分为硬件和软件负载均衡两类，本节主要探讨的是ZooKeeper在“软”负载均衡中的应用场景。

#### 一种动态的DNS服务

DNS是域名系统（Domain Name System）的缩写，是因特网中使用最广泛的核心技术之一。DNS系统可以看作是一个超大规模的分布式映射表，用于将域名和IP地址进行一一映射，进而方便人们通过域名来访问互联网站点。

现在，我们来介绍一种基于ZooKeeper实现的动态DNS方案（以下简称该方案为“DDNS”，Dynamic DNS）。

**域名配置**

和配置管理一样，我们首先需要在ZooKeeper上创建一个节点来进行域名配置，例如`/DDNS/app1/server.app1.company1.com`（以下简称“域名节点”）。

每个应用都可以创建一个属于自己的数据节点作为域名配置的根节点，例如`/DDNS/app1`，在这个节点上，每个应用都可以将自己的域名配置上去。

**域名解析**

在传统的DNS解析在，我们都不需要关心域名的解析过程，所有这些工作都交给了操作系统的域名和IP地址映射机制（本地HOST绑定）或是专门的域名解析服务器（由域名注册服务商提供）。因此，在这点上，DDNS方案和传统的域名解析有很大的区别——在DDNS中，域名的解析过程都是由每一个应用自己负责的。通常应用都会首先从域名节点中获取一份IP地址和端口的配置，进行自行解析。同时，每个应用还会在域名节点上注册一个数据变更Watcher监听，以便及时收到域名变更的通知。

**域名变更**

在运行过程中，难免会碰上域名对应的IP地址或是接口变更，这个时候就需要进行域名变更操作。在DDNS中，我们只需要对指定的域名节点进行更新操作，ZooKeeper就会向订阅的客户端发送这个事件通知、应用在接收到这个事件通知后，就会再次进行域名配置的获取。

上面我们介绍了如何使用ZooKeeper来实现一种动态的DNS系统。通过ZooKeeper来实现动态DNS服务，一方面，可以避免域名数量无限增长带来的集中式维护的成本；另一方面，在域名变更的情况下，也能够避免因逐台机器更新本地HOST而带来的繁琐工作。

#### 自动化的DNS服务

首先来介绍整个动态DNS系统的架构体系中几个比较重要的组件及其职责。

- **Register**集群负责域名的动态注册。
- **Dispatcher**集群负责域名解析。
- **Scanner**集群负责检测以及维护服务状态（探测服务的可用性、屏蔽异常服务节点等）。
- **SDK**提供各种语言的系统接入协议，提供服务注册以及查询接口。
- **Monitor**负责收集服务信息以及对DDNS自身状态的监控。
- **Controller**是一个后台管理的Console，负责授权管理、流量控制、静态配置服务和手动屏蔽服务等功能，另外，系统的运维人员也可以在上面管理Register、Dispatcher和Scanner等集群。

整个系统的核心当然是ZooKeeper集群，负责数据的存储以及一系列分布式协调。下面我们再来详细地看下整个系统是如何运行的。在这个架构模型中，我们将那些目标IP地址和端口抽象为服务的提供者，而那些需要使用域名解析的客户端则被抽象成服务的消费者。

**域名注册**

域名注册主要是针对服务提供者来说的。域名注册过程可以简单地概括为：每个服务提供者在启动的过程中，都会把自己的域名信息注册到Register集群中去。

1. 服务提供者通过SDK提供的API接口，将域名、IP地址和端口发送给Register集群。例如，A机器用于提供 `serviceA.xxx.com`，于是它就向Register发送一个“域名→IP:PORT”的映射：“`serviceA.xxx.com → 192.168.0.1:8080`”。
2. Register获取到域名、IP地址和端口配置后，根据域名将信息写入相对于的ZooKeeper域名节点中。

**域名解析**

域名解析是针对服务消费者来说的，正好和域名注册过程相反：服务消费者在使用域名的时候，会向Dispatcher发出域名解析请求。Dispatcher收到请求后，会从ZooKeeper上的指定域名节点读取相应的IP:PORT列表，通过一定的策略选取其中一个返回给前端应用。

**域名探测**

域名探测是指DDNS系统需要对域名下所有注册的IP地址和端口的可用进行检测，俗称“健康度检测”。健康度检测一般有两种方式，第一种是服务端主动发起健康度心跳检测，这种方式一般需要在服务端和客户端之间建立起一个TCP长链接；第二种则是客户端主动向服务端发起健康度心跳检测。在DDNS架构中的域名探测，使用的是服务提供者主动向Scanner进行状态回报（即第二种健康度检测方式）的模式，即每个服务提供者都会定时向Scanner汇报自己的状态。

Scanner会负责记录每个服务提供者最近一次的状态汇报时间，一旦超过5秒没有收到状态汇报，那么就认为该IP地址和端口已经不可用，于是开始进行域名清理过程。

### 6.1.3 命名服务

命名服务（Name Service）也是分布式系统中比较常见的一类场景，在《Java网络高级编程》一书中提到，命名服务是分布式系统最基本的公共服务之一。在分布式系统中，被命名的实体通常可以是集群中的机器、提供的服务地址或远程对象等——这些我们都可以统称它们为名字（Name），其中较为常见的就是一些分布式服务框架（如RPC、RMI）中的服务地址列表，通过命名服务，客户端应用能够根据指定名字来获取资源的实体、服务地址和提供者的信息等。

Java语言中的JNDI便是一种典型的命名服务。JNDI是Java命名与目录接口（Java Naming and Directory Interface）的缩写，是J2EE体系中重要的规范之一，标准的J2EE容器都提供了对JNDI规范的实现。因此，在实际开发中，开发人员常常使用应用服务器自带的JNDI实现来完成数据源的配置与管理——使用JNDI方式后，开发人员可以完全不需要关心与数据库相关的任何信息，包括数据库类型、JDBC驱动类型以及数据库账户等。

ZooKeeper提供的命名服务功能与JNDI技术有相似的地方，都能够帮助应用系统通过一个资源引用的方式来实现对资源的定位与使用。另外，广义上命名服务的资源定位都不是真正意义的实体资源——在分布式环境中，上层应用仅仅需要一个全局唯一的名字，类似于数据库中的唯一主键。

所谓ID，就是一个能够唯一标识某个对象的标识符。在我们熟悉的关系型数据库中，各个表都需要一个主键来唯一标识每条数据库记录，这个主键就是这样的唯一ID。我们必须寻求一种能够在分布式环境下生成全局唯一ID的方法。

一说起全局唯一ID，相信读者都会联想到UUID。没错，UUID是通用唯一标识码（Universally Unique Identifier）的简称，是一种在分布式系统中广泛使用的用于唯一识别码元素的标准，最典型的实现是GUID（Globally Unique Identifier，全局唯一标识符），主流ORM框架Hibernate有对UUID的直接支持。

确实，UUID是一个非常不错的全局唯一ID生成方式，能够非常简便地保证分布式环境中的唯一性。一个标准的UUID是一个包含32位字符和4个短线的字符串，例如“e70f1357-f260-46ff-a32d-53a086c57ade”。UUID的优势自然不必多说，我们重点来看看它的缺陷。

**长度过长**

UUID最大的问题就在于生成的字符串过长。显然，和数据库中的INT类型相比，存储一个UUID需要花费更多的空间。

**含义不明**

上面我们已经看到一个典型的UUID是类似于“e70f1357-f260-46ff-a32d-53a086c57ade”的一个字符串。根据这个字符串，开发人员从字面上基本看不出任何其表达的含义，这将会大大影响问题排查和开发调试的效率。



接下来，我们结合一个分布式任务调度系统来看看如何使用ZooKeeper来实现这类全局唯一ID的生成。

1. 所有客户端都会根据自己的任务类型，在指定类型的任务下面通过调用create()接口来创建一个顺序节点，例如创建“job-”节点。
2. 节点创建完毕后，create()接口会返回一个完整的节点名，例如“job-0000000003”。
3. 客户端拿到这个返回值后，拼接上type类型，例如“type2-job-0000000003”，这就可以作为一个全局唯一的ID了。

在ZooKeeper中，每一个数据节点都能够维护一份子节点的顺序顺列，当客户端对其创建一个顺序子节点的时候ZooKeeper会自动以后缀的形式在其子节点上添加一个序号，在这个场景中就是利用了ZooKeeper的这个特性。关于ZooKeeper的顺序节点，将在7.1.2节中做详细讲解。

### 6.1.4 分布式协调/通知

分布式协调/通知服务是分布式系统中不可缺少的一个环节，是将不同的分布式组件有机结合起来的关键所在。对于一个在多台机器上部署运行的应用而言，通常需要一个协调者（Coordinator）来控制整个系统的运行流程，例如分布式事务的处理、机器间的互相协调等。同时，引入这样一个协调者，便于将分布式协调的职责从应用中分离出来，从而可以大大减少系统之间的耦合者，而且能够显著提高系统的可扩展性。

ZooKeeper中特有的Watcher注册与异步通知机制，能够很好地实现分布式环境下不同机器，甚至是不同系统之间的协调与通知，从而实现对数据变更的实时处理。基于ZooKeeper实现分布式协调与通知功能，通常的做法是不同的客户端都对ZooKeeper上同一个数据节点进行Watcher注册，监听数据节点的变化（包括数据节点本身及其子节点），如果数据节点发生变化，那么所有订阅的客户端都能够接收到相应的Watcher通知，并做出相应的处理。

#### MySQL数据复制总线：Mysql_Replicator

MySQL数据复制总线（以下简称“复制总线”）是一个实时数据复制框架，用于在不同的MySQL数据库实例之间进行异步数据复制和数据变化通知。整个系统是一个由MySQL数据库集群、消息队列系统、任务管理监控平台以及ZooKeeper集群等组件共同构成的一个包含数据生产者、复制管道和数据消费者等部分的数据总线系统。

在该系统中，ZooKeeper主要负责进行一系列的分布式协调工作，在具体的实现上，根据根据功能将数据复制组件分为三个核心子模块：Core、Server和Monitor，每个模块分别为一个单独的进程，通过ZooKeeper进行数据交换。

- **Core**实现了数据复制的核心逻辑，其将数据复制封装成管道，并抽象出生产者和消费者两个概念，其中生产者通常是MySQL数据库的Binlog日志。
- **Server**负责启动和停止复制任务。
- **Monitor**复制监控任务的运行状态，如果在数据复制期间发生异常或出现故障会进行告警。

每个模块作为独立的进程运行在服务端，运行时的数据和配置信息均保存在ZooKeeper上，Web控制台通过ZooKeeper上的数据获取到后台进程的数据，同时发布控制信息。

**任务注册**

Core进程在启动的时候，首先会向`/mysql_replicator/tasks`节点（以下简称“任务列表节点”）注册任务。例如，对于一个“复制热门商品”的任务，Task所在机器在启动的时候，会首先在任务列表节点上创建一个子节点，例如`/mysql_replicator/tasks/copy_hot_item`（以下简称“任务节点”）。如果在注册过程中发现该子节点已经存在，说明已经有其他Task机器注册了该任务，因此自己不需要再创建该节点了。

**任务热备份**

为了应对复制任务故障或者复制任务所在主机故障，复制组件采用“热备份”的容灾方式，即将同一个复制任务部署在不同的主机上，我们称这样的机器为“任务机器”，主、备任务机器通过ZooKeeper互相检测运行健康状况。

为了实现上述热备方案，无论在第一步中是否创建了任务节点，每台任务机器都需要在`/mysql_replicator/tasks/copy_hot_item/instances`节点上将自己的主机名注册上去。注意，这里注册的节点类型很特殊，是一个临时的顺序节点。在注册完这个子节点后，通常一个完整的节点名如下：`/mysql_replicator/tasks/copy_hot_item/instances/[Hostname]-1`，其中最后的序列号就是临时顺序节点的精华所在。关于ZooKeeper的临时顺序节点生成原理，将在7.1.2节中做详细讲解。

在完成该子节点的创建后，每台任务机器都可以获取到自己创建的节点的完成节点名以及所有子节点的列表，然后通过对比判断自己是否是所有子节点中序号最小的。如果自己是序号最小的子节点，那么就将自己的运行状态设置为RUNNING，其余的任务机器则将自己设置为STANDBY——我们将这样的热备份策略称为“小序号优先”策略。

**热备切换**

完成运行状态的标识后，任务的客户端机器就能够正常工作了，其中标记为RUNNING的客户端机器进行正常的数据复制，而标记为STANDBY的客户端机器则进入待命状态。这里所谓待命状态，就是说一旦标记为RUNNING的机器出现故障停止了任务执行，那么就需要在所有标记为STANDBY的客户端机器中再次按照“小序号优先”策略来选出RUNNING机器来执行，具体的做法就是标记为STANDBY的机器都需要在`/mysql_replicator/tasks/copy_hot_item/instances`节点上注册一个“子节点列表变更”的Watcher监听，用来订阅所有任务执行机器的变化情况——一旦RUNNING机器宕机与ZooKeeper断开连接后，对应的节点就会消失，于是其他机器也就接到了这个变更通知，从而开始新一轮的RUNNING选举。

**记录执行状态**

既然使用了热备份，那么RUNNING任务机器就需要将运行时的上下文状态保留给STANDBY任务机器。在这个场景中，最主要的上下文状态就是数据复制过程中的一些进度信息，例如Binlog日志的消费位点，因此需要将这些信息保存到ZooKeeper上以便共享。在Mysql_Replicator的设计中，选择了`/mysql_replicator/tasks/copy_hot_item/lastCommit`作为Binlog日志消费位点的存储节点，RUNNING任务机器会定时向这个节点写入当前的Binlog日志消费位点。

**控制台协调**

在上文中我们主要讲解了Core组件是如何进行分布式任务协调的，接下来我们再看看Server是如何来管理Core组件的。在Mysql_Replicator中，Server主要的工作就是进行任务的控制，通过ZooKeeper来对不同的任务进行控制与协调。Server会将每个复制任务对应生产者的元数据，即库名、表名、用户名和密码等数据库信息以及消费者的相关信息以配置的形式写入任务节点`/mysql_replicator/tasks/copy_hot_item`中去，以便该任务的所有任务机器都能够共享该复制任务的配置。

**冷备切换**

到目前为止我们已经基本了解了Mysql_Replicator的工作原理，现在再回过头来看上面提到的热备份。在该热备份方案中，针对一个任务，都会至少分配两台任务机器来进行热备份，但是在一定规模的大型互联网公司中，往往有许多MySQL实例需要进行数据复制，每个数据库实例都会对应一个复制任务，如果每个任务都进行双机热备份的话，那么显然需要消耗太多的机器。

因此我们同时设计了一种冷备份的方案，它和热备份方案最大的不同点在于，对所有任务进行分组。

和热备份中比较大的区别在于，Core进程被配置了所属Group（组）。举个例子来说，假如一个Core进程被标记了group1，那么在Core进程启动后，会到对应的ZooKeeper group1节点下面获取所有的Task列表，假如找到了任务“copy_hot_item”之后，就会遍历这个Task列表的instances节点，但凡还没有子节点的，则会创建一个临时的顺序节点：`/mysql_replicator/task-groups/group1/copy_hot_item/instances/[Hostname]-1`——当然，在这个过程中，其他Core进程也会在这个instances节点下创建类似的子节点。和热备份中的“小序号优先”策略一样，顺序小的Core进程将自己标记为RUNNING，不同之处在于，其他Core进程则会自动将自己创建的子节点删除，然后继续遍历下一个Task节点——我们将这样的过程称为“冷备份扫描”。就这样，所有Core进程在一个扫描周期内不断地对相应的Group下面的Task进行冷备份扫描。

**冷热备份对比**

在热备份方案中，针对一个任务使用了两台机器进行热备份，借助ZooKeeper的Watcher通知机制和临时顺序节点的特性，能够非常实时地进行互相协调，但缺陷就是机器资源消耗比较大。而在冷备份方案中，采用了扫描机制，虽然降低了任务协调的实时性，但是节省了机器资源。

#### 一种通用的分布式系统机器间通信方式

在绝大部分的分布式系统中，系统机器间的通信无外乎心跳检测、工作进度汇报和系统调度这三种类型。接下来，我们将围绕这三种类型的机器通信来讲解如何基于ZooKeeper去实现一种分布式系统间的通信方式。

**心跳检测**

机器间的心跳检测机制是指在分布式环境中，不同机器之间需要检测到彼此是否在正常运行，例如A机器需要知道B机器是否正常运行。在传统的开发中，我们通常是通过主机之间是否可以相互PING通来判断，更复杂一点的话，则会通过在机器之间建立长连接，通过TCP连接固有的心跳检测机制来实现上层机器的心跳检测，这些确实都是一些非常常见的心跳检测方法。

下面来看看如何使用ZooKeeper来实现分布式机器间的心跳检测。基于ZooKeeper的临时节点特性，可以让不同的机器都在ZooKeeper的一个指定节点下创建临时子节点，不同的机器之间可以根据这个临时节点来判断对应的客户端机器是否存活。通过这种方式，检测系统和被检测系统之间并不需要直接相关联，而是通过ZooKeeper上的某个节点进行关联，大大减少了系统耦合。

**工作进度汇报**

在一个常见的任务分发系统中，通常任务被分发到不同的机器上执行后，需要实时地将自己的任务执行进度汇报给分发系统。这个时候就可以通过ZooKeeper来实现。在ZooKeeper上选择一个节点，某个任务客户端都在这个节点下面创建临时子节点，这样便可以实现两个功能：

- 通过判断临时节点是否存在来确定任务机器是否存活。
- 各个任务机器会实时地将自己的任务执行进度写到这个临时节点上去，以便中心系统能够实时地获取到任务的执行进度。

**系统调度**

使用ZooKeeper，能够实现另一种系统调度模式：一个分布式系统由控制台和一些客户端系统两部分组成，控制台的职责就是需要将一些指令信息发送给所有的客户端，以控制它们进行相应的业务逻辑。后台管理人员在控制台上做的一些操作，实际上就是修改了ZooKeeper上某些节点的数据，而ZooKeeper进一步把这些数据变更以事件通知的形式发送给了对应的订阅客户端。

总之，使用ZooKeeper来实现分布式系统机器间的通信，不仅能省去大量底层网络通信和协议设计上重复的工作，更为重要的一点是大大降低了系统之间的耦合，能够非常方便地实现异构系统之间的灵活通信。

### 6.1.5 集群管理

随着分布式系统规模的日益扩大，集群中的机器规模也随之变大，因此，如何更好地进行集群管理也显得越来越重要了。

所谓集群管理，包括集群监控与集群控制两大块，前者侧重对集群运行时状态地收集，后者则是对集群进行操作与控制。在日常开发和运维过程中，我们经常会有类似于如下的需求。

- 希望知道当前集群中究竟有多少机器在工作。
- 对集群中每台机器的运行时状态进行数据收集。
- 对集群中机器进行上下线操作。

在传统的基于Agent的分布式集群管理体系中，都是通过在集群中的每台机器上部署一个Agent，由这个Agent复制主动向指定的一个监控中心系统（监控中心系统负责将所有数据进行集中处理，形成一系列报表，并负责实时报警，以下简称“监控中心”）汇报自己所在机器的状态。在集群规模适中的场景下，这确实是一种在生产实践中广泛使用的解决方案，能够快速有效地实现分布式环境集群监控，但是一旦系统的业务场景增多，集群规模变大之后，该解决方案的弊端也就显现出来了。

**大规模升级困难**

**统一的Agent无法满足多样的需求**

**编程语言的多样性**

ZooKeeper具有以下两大特性。

- 客户端如果对ZooKeeper的一个数据节点注册Watcher监听，那么当该数据节点的内容或是其子节点列表发生变更时，ZooKeeper服务器就会向订阅的客户端发送变更通知。
- 对在ZooKeeper上创建的临时节点，一旦客户端与服务端之间的会话失效，那么该临时节点也就被自动清除。

#### 分布式日志收集系统

**注册收集器机器**

**任务分发**

**状态汇报**

**动态分配**

**注意事项**

#### 在线云主机管理

**机器上/下线**

**机器监控**

### 6.1.6 Master选举

Master选举是一个在分布式系统中非常常见的应用场景。分布式最核心的特性就是能够将具有独立计算能力的系统单元部署在不同的机器上，构成一个完整的分布式系统。而与此同时，实际场景中往往也需要在这些分布在不同机器上的独立系统单元中选出一个所谓的“老大”，在计算机科学中，我们称之为Master。

通常情况下，我们可以选择常见的关系型数据库中的主键特性来实现：集群中的所有机器都向数据库中插入一条相同主键ID的记录，数据库会帮助我们自动进行主键冲突检查，也就是说，所有进行插入操作的客户端机器中，只有一台机器能够成功——那么，我们就认为向数据库中成功插入数据的客户端机器成为Master。

乍一看，这个方案确实可行，依靠关系型数据库的主键特性能够很好地保证在集群中选举出唯一的一个Master。但是我们需要考虑的另一个问题是，如果当前选举出的Master挂了，那么该如何处理？谁来告诉我Master挂了呢？

ZooKeeper创建节点的API接口，其中提到的一个重要特性便是：利用ZooKeeper的强一致性，能够很好地保证在分布式高并发情况下节点的创建一定能够保证全局唯一性，即ZooKeeper将会保证客户端无法重复创建一个已经存在的数据节点。也就是说，如果同时有多个客户端请求创建同一个节点，那么最终一定只有一个客户端请求能够创建成功。利用这个特性，就能很容易地在分布式环境中进行Master选举了。

### 6.1.7 分布式锁

分布式锁是控制分布式系统之间同步访问共享资源的一种方式。如果不同的系统或是同一个系统的不同主机之间共享了一个或一组资源，那么访问这些资源的时候，往往需要通过一些互斥手段来防止彼此之间的干扰，以保证一致性，在这种情况下，就需要使用分布式锁。

在平时的实际项目开发中，我们很少会去在意分布式锁，而是依赖于关系型数据库固有的排他性来实现不同进程之间的互斥。这确实是一种非常简便且被广泛使用的分布式锁的实现方式。然而有一个不争的事实是，目前绝大多数大型分布式系统的性能瓶颈都集中在数据库操作上。因此，如果上层业务再给数据库添加一些额外的锁，例如行锁、表锁甚至是繁重的事务处理，那么是不是会让数据库更加不堪重负呢？下面我们来看看使用ZooKeeper如何实现分布式锁，这里注意讲解排他锁和共享锁两类分布式锁。

#### 排他锁

排他锁（Exclusive Locks，简称X锁），又称为写锁或独占锁，是一种基本的锁类型。如果事务$T_1$对数据对象$O_1$加上了排他锁，那么在整个加锁期间，只允许事务$T_1$对$O_1$进行读取和更新操作，其他任何事务都不能再对这个数据对象进行任何类型的操作——直到$T_1$释放了排他锁。

从上面讲解的排他锁的基本概念中，我们可以看到，排他锁的核心是如何保证当前有且仅有一个事务获得锁，并且锁被释放后，所有正在等待获取锁的事务都能够被通知到。下面我们就看看如何借助ZooKeeper实现排他锁。

**定义锁**

在通常的Java开发编程中，有两种常见的方式可以用来定义锁，分别是synchronized机制和JDK5提供的ReentrantLock。然而，在ZooKeeper中，没有类似于这样的API可以直接使用，而是通过ZooKeeper上的数据节点来表示一个锁，例如`/exclusive_lock/lock`节点就可以被定义为一个锁。

**获取锁**

在需要获取排他锁时，所有的客户端都会试图通过调用create()接口，在`/exclusive_lock`节点下创建临时子节点`/exclusive/lock`。在前面几节中我们也介绍了，ZooKeeper会保证在所有的客户端中，最终只有一个客户端能够创建成功，那么就可以认为该客户端获取了锁。同时，所有没有获取到锁的客户端就需要到`/exclusive_lock`节点上注册一个子节点变更的Watcher监听，以便实时监听到lock节点的变更情况。

**释放锁**

在“定义锁”部分，我们已经提到，`/exclusive_lock/lock`是一个临时节点，因此在以下两种情况下，都有可能释放锁。

- 当前获取锁的客户端机器发生宕机，那么ZooKeeper上的这个临时节点就会被移除。
- 正常执行完业务逻辑后，客户端就会主动将自己创建的临时节点删除。

无论在什么情况下移除了lock节点，ZooKeeper都会通知所有在`/exclusive_lock`节点上注册了子节点变更Watcher监听的客户端。这些客户端在接收到通知后，再次重新发起分布式锁获取，即重复“获取锁”过程。

#### 共享锁

共享锁（Shared Locks，简称S锁），又称为读锁，同样是一种基本的锁类型。如果事务$T_1$对数据对象$O_1$加上了共享锁，那么当前事务只能对$O_1$进行读取操作，其他事务也只能对这个数据对象加共享锁——直到该数据对象上的所有共享锁都被释放。

共享锁和排他锁最根本的区别在于，加上排他锁后，数据对象只对一个事务可见，而加上共享锁后，数据对所有事务都可见。下面我们就来看看如何借助ZooKeeper来实现共享锁。

**定义锁**

和排他锁一样，同样是通过ZooKeeper上的数据节点来表示一个锁，是一个类似于“`/shared_lock/[Hostname]-请求类型-序号`”的临时顺序节点，例如`/shared_lock/192.168.0.1-R-0000000001`，那么，这个节点就代表了一个共享锁。

**获取锁**

在需要获取共享锁时，所有客户端都会到`/shared_lock`这个节点下面创建一个临时顺序节点，如果当前是读请求，那么就创建例如`/shared_lock/192.168.0.1-R-0000000001`的节点；如果是写请求，那么就创建例如`/shared_lock/192.168.0.1-W-0000000001`的节点。

**判断读写顺序**

根据共享锁的定义，不同的事务都可以同时对同一个数据对象进行读取操作，而更新操作必须在当前没有任何事务进行读写操作的情况下进行。基于这个原则，我们来看看如何通过ZooKeeper的节点来确定分布式读写顺序，大致可以分为如下4个步骤。

1. 创建完节点后，获取`/shared_lock`节点下的所有子节点，并对该节点注册子节点变更的Watcher监听。

2. 确定自己的节点序号在所有子节点中的顺序。

3. 对于读请求：
   如果没有比自己序号小的子节点，或是所有比自己序号小的子节点都是读请求，那么表面自己已经成功获取到了共享锁，同时开始执行读取逻辑。
   如果比自己序号小的子节点中有写请求，那么就需要进入等待。

   对于写请求：
   如果自己不是序号最小的节点，那么就需要进入等待。

4. 接收到Watcher通知后，重复步骤1。

**释放锁**

释放锁的逻辑和排他锁是一致的，这里不再赘述。

#### 羊群效应

一般场景是指集群规模不是特别大，一般是在10台机器以内。但是如果机器规模扩大之后，会有什么问题呢？我们着重来看上面“判断读写顺序”过程的步骤3。

1. 192.168.0.1 这台机器首先进行读操作，完成读操作后将节点/192.168.0.1-R-0000000001删除。
2. 余下的4台机器均收到了这个节点被移除的通知，然后重新从`/shared_lock`节点上获取一份新的子节点列表。
3. 每个机器判断自己的读写顺序。其中192.168.0.2这台机器检测到自己已经是序号最小的机器了，于是开始进行写操作，而余下的其他机器发现没有轮到自己进行读取或更新操作，于是继续等待。
4. 继续……

上面这个过程就是共享锁在实际运行中最主要的步骤了，我们着重看下上面步骤3中提到的：“而余下的其他机器发现没有轮到自己进行读取或更新操作，于是继续等待。”很明显，我们看到，192.168.0.1这个客户端在移除自己的共享锁后，ZooKeeper发送了子节点变更Watcher通知给所有机器，然而这个通知除了给192.168.0.2这台机器产生实际影响外，对于余下的其他所有机器都没有任何作用。

在这整个分布式锁的竞争过程中，大量的“Watcher通知”和“子节点列表获取”两个操作重复运行，并且绝大多数的运行结果都是判断出自己并非是序号最小的节点，从而继续等待下一次通知。客户端无端地接收到过多和自己并不相关的事件通知，如果在集群规模比较大的情况下，不仅会对ZooKeeper服务器造成巨大的性能影响和网络冲击，更为严重的是，如果同一时间有多个节点对应的客户端完成事务或是事务中断引起节点消失，ZooKeeper服务器就会在短时间内向其余客户端发送大量的事件通知——这就是所谓的羊群效应。

上面这个ZooKeeper分布式共享锁实现中出现羊群效应的根源在于，没有找准客户端真正的关注点。我们再来回顾一下上面的分布式锁竞争过程，它的核心逻辑在于：判断自己是否是所有子节点中序号最小的。于是，很容易可以联想到，每个节点对应的客户端只需要关注比自己序号小的那个相关节点的变更情况就可以了——而不需要关注全局的子列表变更情况。

#### 改进后的分布式锁实现

现在我们来看看如何改进上面的分布式锁实现。首先，我们需要肯定的一点是，上面提到的共享锁实现，从整体思路上来说完全正确。这里主要的改动在于：每个锁竞争者，只需要关注`/shared_lock`节点下序号比自己小的那个节点是否存在即可，具体实现如下。

1. 客户端调用create()方法创建一个类似于“`/shared_lock/[Hostname]-请求类型-序号`”的临时顺序节点。
2. 客户端调用getChildren()接口来获取所有已经创建的子节点列表，注意，这里不注册任何Watcher。
3. 如果无法获取共享锁，那么就调用exist()来对比自己小的那个节点注册Watcher。注意，这里“比自己小的节点”只是一个笼统的说法，具体对于读请求和写请求不一样。
   读请求：向比自己序号小的最后一个写请求节点注册Watcher监听。
   写请求：向比自己序号小的最后一个节点注册Watcher监听。
4. 等待Watcher通知，继续进入步骤2。

**注意**

改进后的分布式锁实现相对来说比较麻烦。在具体的实际开发过程中，我们提倡根据具体的业务场景和集群规模来选择适合自己的分布式锁实现。

### 6.1.8 分布式队列

业界有不少分布式队列产品，不过绝大多数都是类似于ActiveMQ、Metamorphosis、Kafka和HornetQ等的消息中间件（或者称为消息队列）。在本节中，我们主要介绍基于ZooKeeper实现的分布式队列。分布式队列，简单地讲分为两大类，一种是常规的先入先出队列，另一种则是要等到队列元素集聚之后才统一安排执行的Barrier模型。

#### FIFO：先进先出

使用ZooKeeper实现FIFO队列，和6.1.7节中提到的共享锁的实现非常类似。FIFO队列就类似于一个全写的共享锁模型

#### Barrier：分布式屏障

## 6.2 ZooKeeper在大型分布式系统中的应用

在6.1节中，我们已经从理论上详细地讲解了ZooKeeper的典型应用场景及其实现细节。而在实际工业实践中，由于ZooKeeper便捷的使用方式、卓越的运行性能以及良好的稳定性，已经被广泛地应用在越来越多的大型分布式系统中，用来解决诸如配置管理、分布式通知/协调、集群管理和Master选举等一系列分布式问题，其中最著名的就是Hadoop、HBase和Kafka等开源系统。

### 6.2.1 Hadoop

Hadoop是Apache开源的一个大型分布式计算框架，由Lucene创始人Doug Cutting牵头创建，其定义了一种能够开发和运行处理海量数据的软件规范，用来实现一个在大规模集群中对海量数据进行分布式计算的软件平台。Hadoop的核心是HDFS和MapReduce，分别提供了对海量数据的存储和计算能力，自0.23.0版本开始，Hadoop又引入了全新一代MapReduce框架YARN。

在Hadoop中，ZooKeeper主要用于实现HA（High Availability），这部分逻辑主要集中在Hadoop Common的HA模块中，HDFS的NameNode与YARN的ResourceManager都是基于此HA模块来实现自己的HA功能的。同时，在YARN中又特别提供了ZooKeeper来存储应用的运行状态。

#### YARN介绍

YARN 是 Hadoop 为了提高计算节点 Master(JT) 的扩展性，同时为了支持多计算模型和提供资源的细粒度调度而引入的全新一代分布式调度框架。其上可以支持MapReduce计算引擎，也支持其他的一些计算引擎，如Tez、Spark、Storm、Imlala和Open MPI等。

YARN主要由ResourceManager（RM）、NodeManager（NM）、ApplicationMaster（AM）和Container四部分组成。其中最核心的就是ResourceManager，它作为全局的资源管理器，负责整个系统的资源管理和分配。

**ResourceManager单点问题**

ResourceManager是YARN中非常复杂的一个组件。ResourceManager的工作状况直接决定了整个YARN框架是否可以正常运转。

**ResourceManager HA**

为了解决ResourceManager的这个单点问题，YARN设计了一套Active/Standby模式的ResourceManager HA架构。

在运行期间，会有多个ResourceManager并存，并且其中只有一个ResourceManager处于Active状态，另外的一些（允许一个或者多个）则是处于Standby状态，当Active节点无法正常工作（如机器挂掉或重启等）时，其余处于Standby状态的节点则会通过竞争选举产生新的Active节点。

**主备切换**

ActiveStandbyElector组件

HDFS中的NameNode和ResourceManager模块都是使用该组件来实现各自的HA的

**Fencing（隔离）**

在分布式环境中，经常会出现诸如单机“假死”的情况。所谓的“假死”是指机器由于网络闪断或是其自身由于负载过高（常见的有GC占用时间过长或CPU的负载过高等）而导致无法正常地对外进行及时响应。在上述主备切换过程中，我们假设RM集群由ResourceManager1和ResourceManager2两台机器组成，且ResourceManager1为Active状态，ResourceManager2为Standby状态。某一时刻，ResourceManager1发生了“假死”现象，此时ZooKeeper认为ResourceManager1挂了，根据上述主备切换逻辑，ResourceManager2就会称为Active状态。但是在随后，ResourceManager1恢复了正常，其依然认为自己还处于Active状态。这就是我们常说的分布式“脑裂”（Brain-Split）现象，即存在了多个处于Active状态的ResourceManager各司其职。那么该如何解决这样的问题呢？

YARN中引入了Fencing机制，借助ZooKeeper数据节点的ACL权限控制机制来实现不同RM之间的隔离。具体做法其实非常简单，在上文的“主备切换”部分中我们讲到，多个RM之间通过竞争创建锁节点来实现主备状态的确定。这个地方需要改进的一点是，创建的根节点必须携带ZooKeeper的ACL信息，目的是为了独占该根节点，以防止其他RM对该节点进行更新。

**ResourceManager状态存储**

在ResourceManager中，RMStateStore能够存储一些RM的内部状态信息。在存储的设计方案中，提供了三种可能的实现，分别如下。

- 基于内存时序，一般是用于日常开发测试。
- 基于文件系统的实现，如HDFS。
- 基于ZooKeeper的实现。

由于这些状态信息的数据量都不是特别大，因此Hadoop官方建议基于ZooKeeper来实现状态信息的存储。

**小结**

ZooKeeper一开始是Hadoop的子项目，因此很多设计之初的原始需求都是为了解决Hadoop系统中碰到的一系列分布式问题。

### 6.2.2 HBase

HBase，全称Hadoop Database，是Google Bigtable的开源实现，是一个基于Hadoop文件系统设计的面向海量数据的高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可以在廉价的PC服务器上搭建起大规模结构化的存储集群。

与大部分分布式NoSQL数据库不同的是，HBase针对数据写入具有强一致性的特性，甚至包括索引列也都实现了强一致性，因此收到了很多互联网企业的青睐。

HBase在实现上严格遵守了Google BigTable论文的设计思想。BigTable使用Chubby来负责分布式状态的协调。在3.1节中我们已经讲解了Chubby，这是Google实现的一种基于Paxos算法的分布式锁服务，而HBase则采用了开源的ZooKeeper服务来完成对整个系统的分布式协调工作。

在HBase的整个架构体系中，ZooKeeper是串联起HBase集群与Client的关键所在。

#### 系统容错

HMaster则会接收到ZooKeeper的NodeDelete通知，从而感知到某个节点断开，并立即开始容错工作——在HBase的实现中，HMaster会将该RegionServer所处理的数据分片（Region）重新路由到其他节点上，并记录到Meta信息中供客户端查询。

#### RootRegion管理

对于HBase集群来说，数据存储的位置信息是记录在元数据分片，也就是RootRegion上的。每次客户端发起新的请求，需要知道数据的位置，就会去查询RootRegion，而RootRegion自身的位置则是记录在ZooKeeper上的（默认情况下，是记录在ZooKeeper的`/hbase/root-region-server`节点中）。

#### Region状态管理

Region是Hbase中数据的物理切片，每个Region中记录了全局数据的一小部分，并且不同的Region之间的数据是相互不重复的。

对于HBase集群来说，Region的数量可能会多达10万级别，甚至更多，因此这样规模的Region状态管理也只有依靠ZooKeeper这样的系统才能做到。

#### 分布式SplitLog任务管理

当某台RegionServer服务器挂掉时，由于总有一部分新写入的数据还没有持久化HFile中，因此在迁移该RegionServer的服务时，一个重要的工作就是从HLog中恢复这部分还在内存中的数据，而这部分工作最关键的一步就是SplitLog，即HMaster需要遍历该RegionServer服务器的HLog，并按Region切分成小块移动到新的地址下，并进行数据的Replay。

HMaster会在ZooKeeper上创建一个splitlog的节点（默认情况下，是`/hbase/splitlog`节点）。ZooKeeper在这里担负起了分布式集群中相互通知和信息持久化的角色。

#### Replication管理

Replication是实现HBase中主备集群间的实时同步的重要模块。

HBase同样借助ZooKeeper来完成Replication功能。做法是在ZooKeeper上记录一个replication节点（默认情况下，是`/hbase/replication`节点）

#### ZooKeeper部署

HBase的启动脚本（hbase-env.sh）中可以选择是由HBase启动其自带的默认ZooKeeper，还是使用一个已有的外部ZooKeeper集群。一般的建议是使用第二种方式，因为这样就可以使得多个HBase集群复用同一套ZooKeeper集群，从而大大节省机器成本。

**小结**

以上就是一些HBase系统中依赖ZooKeeper完成分布式协调功能的典型场景。但事实上，HBase对于ZooKeeper的依赖还不止这些，比如HMaster依赖ZooKeeper来完成ActiveMaster的选举、BackupMaster的实时接管、Table的enable/disable状态记录，以及HBase中几乎所有的元数据存储都是放在ZooKeeper上的。有趣的是，HBase甚至还通过ZooKeeper来实现DrainingServer这样的增强功能（相当于降级标志）。HBase中所有对ZooKeeper的操作都封装在了org.apache.hadoop.hbase.zookeeper这个包中，感兴趣的读者可以自行研究。

### 6.2.3 Kafka

Kafka是知名社交网络公司LinkedIn于2010年12月份开源的分布式消息系统，主要由Scala语言开发，于2012年成为Apache的顶级项目。

Kafka主要用于实现低延迟的发送和收集大量的事件和日志数据——这些数据通常都是活跃的数据。

Kafka是一个吞吐量极高的分布式消息系统，其整体设计是典型的发布与订阅模式系统。在Kafka集群中，没有“中心主节点”的概念，集群中所有的服务器都是对等的，因此，可以在不做任何配置更改的情况下实现服务器的添加与删除。

#### 术语介绍

尽管Kafka是一个近似符合JMS规范的消息中间件实现，但是为了让读者能够更好地理解本节余下部分的内容，这里首先对Kakfa中的一些术语进行简单的介绍。

- **消息生产者**，即Producer，是消息产生的源头，负责生成消息并发送到Kafka服务器上。
- **消息消费者**，即Consumer，是消息的使用方，负责消费Kafka服务器上的消息。
- **主题**，即Topic，由用户定义并配置在Kafka服务端，用于建立生产者和消费者之间的订阅关系：生产者发送消息到指定Topic下，消费者从这个Topic下消费消息。
- **消息分区**，即Partition，一个Topic下面会分为多个分区，例如“kafka-test”这个Topic可以分为10个分区，分别由两台服务器提供，那么通常可以配置为让每台服务器提供5个分区，假设服务器ID分别为0和1，则所有分区为0-0、0-1、0-2、0-3、0-4和1-0、1-1、1-2、1-3、1-4。消息分区机制和分区的数量与消费者的负载均衡机制有很大关系，后面将会重点展开讲解。
- **Broker**，即Kafka的服务器，用于存储消息，在消息中间件中通常被称为Broker。
- **消费者分组**，即Group，用于归组同类消费者。在Kafka中，多个消费者可以共同消费一个Topic下的消息，每个消费者消费其中的部分消息，这些消费者就组成了一个分组，拥有同一个分组名称，通常也被称为消费者集群。
- **Offset**，消息存储在Kafka的Broker上，消费者拉取消息数据的过程中需要知道消息在文件中的偏移量，这个偏移量就是所谓的Offset。

#### Broker注册

Kafka是一个分布式的消息系统，这也体现在其Broker、Producer和Consumer的分布式部署上。虽然Broker是分布式部署并且相互之间是独立运行的，但还是需要有一个注册系统能够讲整个集群中的Broker服务器都管理起来。在Kafka的设计中，选择了使用ZooKeeper来进行所有Broker的管理。

在ZooKeeper上会有一个专门用来进行Broker服务器列表记录的节点，下文中我们称之为“Broker节点”，其节点路径为`/brokers/ids`

请注意，Broker创建的节点是一个临时节点，也就是说，一旦这个Broker服务器宕机或是下线后，那么对应的Broker节点也就被删除了。因此我们可以通过ZooKeeper上Broker节点的变化情况来动态表征Broker服务器的可用性。

#### Topic注册

在Kafka中，会将同一个Topic的消息分成多个分区并将其分布到多个Broker上，而这些分区信息以及与Broker的对应关系也都是由ZooKeeper维护的，由专门的节点来记录，其节点路径为`/brokers/topics`。下文我们将这个节点称为“Topic节点”。

#### 生产者负载均衡

在上面的内容中，我们讲解了Kafka是分布式部署Broker服务器的，会对同一个Topic的消息进行分区并将其分布到不同的Broker服务器上。因此，生产者需要将消息合理地发送到这些分布式地Broker上——这就面临一个问题：如何进行生产者的负载均衡。对于生产者的负载均衡，Kafka支持传统的四层负载均衡，同时也支持使用ZooKeeper方式来实现负载均衡，这里我们首先来看使用四层负载均衡方案。

**四层负载均衡**

四层负载均衡方案在设计上比较简单，一般就是根据生产者的IP地址和端口来为其确定一个相关联的Broker。通常一个生产者只会对应单个Broker，然后该生产者生成的所有消息都发送给这个Broker。从设计上，我们可以很容易发现这种方式的优缺点：好处是整体逻辑简单，不需要引入其他三方系统，同时每个生产者也不需要同其他系统建立额外的TCP连接，只需要和Broker维护单个TCP连接即可。

但这种方案的弊端也是显而易见的，事实上该方案无法做到真正的负载均衡。因为在系统实际运行过程中，每个生产者生成的消息量，以及每个Broker的消息存储量都是不一样的，如果有些生产者产生的消息远多于其他生产者的话，那么会导致不同的Broker接收到的消息总数非常不均匀。另一方面，生产者也无法实时感知到Broker的新增与删除，因此，这种负载均衡方式无法做到动态的负载均衡。

**使用ZooKeeper进行负载均衡**

在Kafka中，客户端使用了基于ZooKeeper的负载均衡策略来解决生产者的负载均衡问题。在实现上，Kafka的生产者会对ZooKeeper上的“Broker的新增与减少”、“Topic的新增与减少”和“Broker与Topic关联关系的变化”等事件注册Watcher监听，这样就可以实现一种动态的负载均衡机制了。

#### 消费者负载均衡

Kafka有消费者分组的概念，每个消费者分组中都包含了若干个消费者，每一条消息都只会发送给分组中的一个消费者，不同的消费者分组消费自己特定Topic下面的消息，互不干扰，也不需要互相进行协调。因此消费者的负载均衡也可以看作是同一个消费者分组内部的消息消费策略。

**消息分区与消费者关系**

**消息消费进度Offset记录**

**消费者注册**

**负载均衡**

## 6.3 ZooKeeper在阿里巴巴的实践与应用

# 第7章 ZooKeeper技术内幕

## 7.1 系统模型

在本节中，我们首先将从数据模型、节点特性、版本、Watcher和ACL五方面来讲述ZooKeeper的系统模型。

### 7.1.1 数据模型

ZooKeeper的视图结构和标准的Unix文件系统非常类似，但没有引入传统文件系统中目录和文件等相关概念，而是使用了其特有的“数据节点”概念，我们称之为ZNode。ZNode是ZooKeeper中数据的最小单元，每个ZNode上都可以保存数据，同时还可以挂在子节点，因此构成了一个层次化的命名空间，我们称之为树。

#### 树

在ZooKeeper中，每个数据节点都被称为一个ZNode，所有ZNode按层次化结构进行组织，形成一棵树。ZNode的节点路径标识方式和Unix文件系统路径非常相似，都是由一系列使用斜杠（/）进行分割的路径表示，开发人员可以向这个节点中写入数据，也可以在节点下面创建子节点。

#### 事务ID

在《事务处理：概念与技术》一书中提到，事务是对物理和抽象的应用状态上的操作集合。在现在的计算机科学中，狭义上的事务通常指的是数据库事务，一般包含了一系列对数据库有序的读写操作，这些数据库事务具有所谓的ACID特性，即原子性（Atomic）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability）。

在ZooKeeper中，事务是指能够改变ZooKeeper服务器状态的操作，我们也称之为事务操作或更新操作，一般包括数据节点创建与删除、数据节点内容更新和客户端会话创建与失效等操作。对于每一个事务请求，ZooKeeper都会为其分配一个全局唯一的事务ID，用ZXID来表示，通常是一个64位的数字。每一个ZXID对应一次更新操作，从这些ZXID中可以间接地识别出ZooKeeper处理这些更新操作请求的全局顺序。

### 7.1.2 节点特性

#### 节点类型

在ZooKeeper中，每个数据节点都是有生命周期的，其生命周期的长短取决于数据节点的节点类型。在ZooKeeper中，节点类型可以分为持久节点（PERSISTENT）、临时节点（EPHEMERAL）和顺序节点（SEQUENTIAL）三大类，具体在节点创建过程中，通过组合使用，可以生成一下四种组合型节点类型：

**持久节点（PERSISTENT）**

持久节点是ZooKeeper中最常见的一种节点类型。所谓持久节点，是指该数据节点被创建后，就会一直存在于ZooKeeper服务器上，直到有删除操作来主动清除这个节点。

**持久顺序节点（PERSISTENT_SEQUENTIAL）**

持久顺序节点的基本特性和持久节点是一致的，额外的特性表现在顺序性上。在ZooKeeper中，每个父节点都会为它的第一级子节点维护一份顺序，用于记录下每个子节点创建的先后顺序。基于这个顺序特性，在创建子节点的时候，可以设置这个标记，那么在创建节点过程中，ZooKeeper会自动为给定节点名加上数字后缀，作为一个新的、完整的节点名。另外需要注意的是，这个数字后缀的上限是整型的最大值。

**临时节点（EPHEMERAL）**

和持久节点不同的是，临时节点的生命周期和客户端的会话绑定在一起，也就是说，如果客户端会话失效，那么这个节点就会被自动清理掉。注意，这里提到的是客户端会话失效，而非TCP连接断开。关于ZooKeeper客户端会话和连接，将在7.4节中做详细讲解。另外，ZooKeeper规定了不能基于临时节点来创建子节点，即临时节点只能作为叶子节点。

**临时顺序节点（EPHEMERAL_SEQUENTIAL）**

临时顺序节点的基本特性和临时节点也是一致的，同样是在临时节点的基础上，添加了顺序的特性。

#### 状态信息

在7.1.1节中，我们提到可以针对ZooKeeper上的数据节点进行数据的写入和子节点的创建。事实上，每个数据节点除了存储了数据内容之外，还存储了数据节点本身的一些状态信息。在5.2.2节中，我们介绍了如何使用get命令来获取一个数据节点的内容。

我们可以看到，第一行是当前数据节点的数据内容，从第二行开始就是节点的状态信息了，这其实就是数据节点的Stat对象的格式化输出。

Stat类中包含了ZooKeeper上一个数据节点的所有状态信息，包括事务ID、版本信息和子节点个数等，表7-1中对所有这些属性进行了说明。

| 状态属性       | 说明                                                         |
| -------------- | ------------------------------------------------------------ |
| czxid          | 即Created ZXID，表示该数据节点被创建时的事务ID               |
| mzxid          | 即Modified ZXID，表示该节点最后一次被更新时的事务ID          |
| ctime          | 即Created Time，表示节点被创建的时间                         |
| mtime          | 即Modified Time，表示该节点最后一次被更新的时间              |
| version        | 数据节点的版本号。关于ZooKeeper中版本相关的内容，将在7.1.3节中做详细讲解 |
| cversion       | 子节点的版本号                                               |
| aversion       | 节点的ACL版本号                                              |
| ephemeralOwner | 创建该临时节点的会话的sessionID。如果该节点是持久节点，那么这个属性值为0 |
| dataLength     | 数据内容的长度                                               |
| numChildren    | 当前节点的子节点个数                                         |
| pzxid          | 表示该节点的子节点列表最后一次被修改时的事务ID。注意，只有子节点列表变更了才会变更pzxid，子节点内容变更不会影响pzxid |

### 7.1.3 版本——保证分布式数据原子性操作

ZooKeeper中为数据节点引入了版本的概念，每个数据节点都具有三种类型的版本信息，对数据节点的任何更新都会引起版本号的变化，表7-2中对这三类版本信息分别进行了说明。

| 版本类型 | 说明                         |
| -------- | ---------------------------- |
| version  | 当前数据节点数据内容的版本号 |
| cversion | 当前数据节点子节点的版本号   |
| aversion | 当前数据节点ACL变更版本号    |

ZooKeeper中的版本概念和传统意义上的软件版本有很大的区别，它表示的是对数据节点的数据内容、子节点列表，或是节点ACL信息的修改次数，我们以其中的version这种版本类型为例来说明。在一个数据节点`/zk-book`被创建完毕之后，节点的version的值是0，表示的含义是“当前节点自从创建之后，被更新过0次”。如果现在对该节点的数据内容进行更新操作，那么随后，version的值就会变成1。同时需要注意的是，在上文中提到的关于version的说明，其表示的是对数据节点数据内容的变更次数，强调的是变更次数，因此即使前后两次变更并没有使得数据内容的值发生变化，version的值依然会变更。

在上面的介绍中，我们基本了解了ZooKeeper中的版本概念。那么版本究竟用来干嘛呢？在讲解版本的作用之前，我们首先来看下分布式领域中最常见的一个概念——锁。

在并发环境中，我们需要通过一些机制来保证这些数据在某个操作过程中不会被外界修改，我们称这样的机制为“锁”。在数据库技术中，通常提到的“悲观锁”和“乐观锁”就是这种机制的典型实现。

悲观锁，又被称作悲观并发控制（Pessimistic Concurrency Control，PCC），是数据库中一种非常典型且非常严格的并发控制策略。悲观锁具有强烈的独占和排他特性，能够有效地避免不同事务对同一数据并发更新而造成的数据一致性问题。一般我们认为，在实际生产应用中，悲观锁策略适合解决那些对于数据更新竞争十分激烈的场景——在这类场景中，通常采用简单粗暴的悲观锁机制来解决并发控制问题。

乐观锁，又被称作乐观并发控制（Optimistic Concurrency Control，OCC），也是一种常见的并发控制策略。乐观锁通常适合使用在数据并发竞争不大、事务冲突较少的应用场景中。

我们其实可以把一个乐观锁控制的事务分成如下三个阶段：数据读取、写入校验和数据写入，其中写入校验阶段是整个乐观锁控制的关键所在。在写入校验阶段，事务会检查数据在读取阶段后是否有其他事务对数据进行过更新，以确保数据更新的一致性。

好了，现在我们再回过头来看看ZooKeeper中版本的作用。事实上，在ZooKeeper中，version属性正是用来实现乐观锁机制中的“写入校验”的。在5.3.5节中，我们已经详细地讲解了如何正确地使用version属性来实现乐观锁机制。

### 7.1.4 Watcher——数据变更的通知

在6.1.1节中，我们已经提到，ZooKeeper提供了分布式数据的分布/订阅功能。一个典型的发布/订阅模型系统定义了一种一对多的订阅关系，能够让多个订阅者同时监听某一个主题对象，当这个主题对象自身状态变化时，会通知所有订阅者，使它们能够做出相应的处理。在ZooKeeper中，引入了Watcher机制来实现这种分布式的通知功能。ZooKeeper允许客户端向服务器注册一个Watcher监听，当服务端的一些指定事件出发了这个Watcher，那么就会向指定客户端发送一个事件通知来实现分布式的通知功能。

ZooKeeper的Watcher机制主要包括客户端线程、客户端WatchManager和ZooKeeper服务器三部分。在具体工作流程上，简单地讲，客户端在向ZooKeeper服务器注册Watcher的同时，会将Watcher对象存储在客户端的WatchManager中。当ZooKeeper服务端触发Watcher事件后，会向客户端发送通知、客户端线程从WatchManager中取出对应的Watcher对象来执行回调逻辑。

#### Watcher接口

在ZooKeeper中，接口类Watcher用于表示一个标准的事件处理器，其定义了事件通知相关的逻辑，包含KeeperState和EventType两个枚举类，分别代表了通知状态和事件类型，同时定义了事件的回调方法：process（WatchedEvent event）

**Watcher事件**

同一个事件在不同的通知状态中代表的含义有所不同，表7-3列举了常见的通知状态和事件类型。

| KeeperState              | EventType                | 触发条件                                                     | 说明                                                         |
| ------------------------ | ------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| SyncConnected（3）       | None（-1）               | 客户端与服务器成功建立会话                                   | 此时客户端和服务器处于连接状态                               |
|                          | NodeCreated（1）         | Watcher监听的对应数据节点被创建                              |                                                              |
|                          | NodeDeleted（2）         | Watcher监听的对应数据节点被删除                              |                                                              |
|                          | NodeDataChanged（3）     | Watcher监听的对应数据节点的数据内容发生变更                  |                                                              |
|                          | NodeChildrenChanged（4） | Watcher监听的对应数据节点的子节点列表发生变更                |                                                              |
| Disconnected（0）        | None（-1）               | 客户端与ZooKeeper服务器断开连接                              | 此时客户端和服务器出于断开状态                               |
| Expired（-112）          | None（-1）               | 会话超时                                                     | 此时客户端会话失效，通常同时也会收到SessionExpiredException异常 |
| AuthFailed（4）          | None（-1）               | 通常有两种情况：<br />- 使用错误的scheme进行权限检查<br />- SASL权限检查失败 | 通常同时也会收到AuthFailedException异常                      |
| ~~Unknown~~（-1）        |                          |                                                              | 从3.1.0版本开始已废弃                                        |
| ~~NoSyncConnected~~（1） |                          |                                                              |                                                              |

#### 回调方法process()

process方法是Watcher接口中的一个回调方法，当ZooKeeper向客户端发送一个Watcher事件通知时，客户端就会对相应的process方法进行回调，从而实现对事件的处理。process方法的定义如下：

~~~java
abstract public void process(WatchedEvent event)
~~~

这个回调方法的定义非常简单，我们重点看下方法的参数定义：WatchedEvent。WatchedEvent包含了每一个事件的三个基本属性：通知状态（keeperState）、事件类型（eventType）和节点路径（path），其数据结构如图7-5所示。ZooKeeper使用WatchedEvent对象来封装服务端事件并传递给Watcher，从而方便回调方法process对服务端事件进行处理。

~~~mermaid
classDiagram
	class WatchedEvent {
		- keeperState: KeeperState
		- eventType: EventType
		- path: String
		+ getWrapper(): WatcherEvent
	}
~~~

提到WatchedEvent，不得不讲下WatcherEvent实体。笼统地讲，两者表示的是同一个事物，都是对一个服务端事件的封装。不同的是，WatchedEvent是一个逻辑事件，用于服务器和客户端程序执行过程中所需的逻辑对象，而WatcherEvent因为实现了序列化接口，因此可以用于网络传输，其数据结构如图7-6所示。

~~~mermaid
classDiagram
	class WatcherEvent {
		- type: int
		- state: int
		- path: String
		+ serialize(OutputArchive, String): void 
		+ deserialize(InputArchive, String): void 
	}
~~~

服务端在生成WatchedEvent事件之后，会调用getWrapper方法将自己包装成一个可序列化的WatcherEvent事件，以便通过网络传输到客户端。客户端在接收到服务端的这个事件对象后，首先会将WatcherEvent事件还原成一个WatchedEvent事件，并传递给process方法处理，回调方法process根据入参就能够解析出完整的服务器事件了。

需要注意的一点是，无论是WatchedEvent还是WatcherEvent，其对ZooKeeper服务端事件的封装都是极其简单的。

客户端无法直接从该事件中获取到对应数据节点的原始数据内容以及变更后的新数据内容，而是需要客户端再次主动去重新获取数据——这也是ZooKeeper Watcher机制的一个非常重要的特性。

#### 工作机制

ZooKeeper的Watcher机制，总的来说可以概括为以下三个过程：客户端注册Watcher、服务端处理Watcher和客户端回调Watcher。

**客户端注册Watcher**

**服务端处理Watcher**

无论是dataWatches还是childWatches管理器，Watcher的触发逻辑都是一致的，基本步骤如下。

1. 封装WatcherEvent
   首先将通知状态（KeeperState）、事件类型（EventType）以及节点路径（Path）封装成一个WatchedEvent对象。
2. 查询Watcher。
   根据数据节点的节点路径从watchTable中取出对应的Watcher。如果没有找到Watcher，说明没有任何客户端在该数据节点上注册过Watcher，直接退出。而如果找到了这个Watcher，会将其提取出来，同时会直接从watchTable和watch2Paths中将其删除——从这里我们也可以看出，Watcher在服务端是一次性的，即触发一次就失效了。
3. 调用process方法来触发Watcher。
   在process方法中，主要逻辑如下。
   - 在请求头中标记“-l”，表面当前是一个通知。
   - 将WatchedEvent包装成WatcherEvent，以便进行网络传输序列化。
   - 向客户端发送该通知。

**客户端回调Watcher**

#### Watcher特性总结

通过上面内容的讲解，我们不难发现ZooKeeper的Watcher具有以下几个特性。

**一次性**

从上面的介绍中可以看到，无论是服务端还是客户端，一旦一个Watcher被触发，ZooKeeper都会将其从相应的存储中移除。因此，开发人员在Watcher的使用上要记住的一点是需要反复注册。这样的设计有效地减轻了服务端的压力。试想，如果注册一个Watcher之后一直有效，那么针对那些更新非常频繁的节点，服务端会不断地向客户端发送事件通知，这无论对于网络还是服务端性能的影响都非常大。

**客户端串行执行**

客户端Watcher回调的过程是一个串行同步的过程，这为我们保证了顺序，同时，需要开发人员注意的一点是，千万不要因为一个Watcher的处理逻辑影响了整个客户端的Watcher回调。

**轻量**

WatchedEvent是ZooKeeper整个Watcher通知机制的最小通知单元，这个数据结构中只包含三部分内容：通知状态、事件类型和节点路径。也就是说，Watcher通知非常简单，只会告诉客户端发生了事件，而不会说明事件的具体内容。

另外，客户端向服务端注册Watcher的时候，并不会把客户端真实的Watcher对象传递到服务端，仅仅只是在客户端请求中使用boolean类型属性进行了标记，同时服务端也仅仅只是保存了当前连接的ServerCnxn对象。

如此轻量的Watcher机制设计，在网络开销和服务端内存开销上都是非常廉价的。

### 7.1.5 ACL——保障数据的安全

ZooKeeper提供了一套完善的ACL（Access Control List）权限控制机制来保障数据的安全。

提到权限控制，我们首先来看看大家都熟悉的，在Unix/Linux文件系统中使用的，也是目前应用最广泛的权限控制方式——UGO（User、Group和Others）权限控制机制。简单地讲，UGO就是针对一个文件或目录，对创建者（User）、创建者所在的组（Group）和其他用户（Other）分别配置不同的权限。从这里可以看出，UGO其实是一种粗粒度的文件系统权限控制模式，利用UGO只能对三类用户进行权限控制，即文件的创建者、创建者所在的组以及其他所有用户，很显然，UGO无法解决下面这个场景：

>用户U1创建了文件F1，希望U1所在的用户组G1拥有对F1读写和执行的权限，另一个用户组G2拥有读权限，而另一个用户U3则没有任何权限。

接下来我们来看另一种典型的权限控制方式：ACL。ACL，即访问控制列表，是一种相对来说比较新颖且更细粒度的权限管理方式，可以针对任意用户和组进行细粒度的权限控制。目前绝大部分Unix系统都已经支持了ACL方式的权限控制，Linux也从2.6版本的内核开始支持这个特性。

#### ACL介绍

ZooKeeper的ACL权限控制和Unix/Linux操作系统中的ACL有一些区别，读者可以从三个方面来理解ACL机制，分别是：权限模式（Scheme）、授权对象（ID）和权限（Permission），通常使用“`scheme:id:permission`”来标识一个有效的ACL信息。

**权限模式：Scheme**

权限模式用来确定权限验证过程中使用的检验策略。在ZooKeeper中，开发人员使用最多的就是以下四种权限模式。

*IP*

IP模式通过IP地址粒度来进行权限控制。同时，IP模式也支持按照网段的方式进行配置。

*Digest*

Digest是最常用的权限控制模式，也更符合我们对于权限控制的认识，其以类似于“`username:password`”形式的权限标识来进行权限配置，便于区分不同应用来进行权限控制。

当我们通过“`username:password`”形式配置了权限标识后，ZooKeeper会对其先后进行两次编码处理，分别是SHA-1算法加密和BASE64编码

*World*

World是一种最开放的权限控制模式，从其名字中可以看出，事实上这种权限控制方式几乎没有任何作用，数据节点的访问权限对所有用户开放，即所有用户都可以在不进行任何权限校验的情况下操作ZooKeeper上的数据。另外，World模式也可以看作是一种特殊的Digest模式，它只有一个权限标识，即“world:anyone”。

*Super*

Super模式，顾名思义就是超级用户的意思，也是一种特殊的Digest模式。在Super模式下，超级用户可以对任意ZooKeeper上的数据节点进行任何操作。关于Super模式的用法，本节后面会进行详细的讲解。

**授权对象：ID**

授权对象指的是权限赋予的用户或一个指定实体，例如IP地址或是机器等。在不同的权限模式下，授权对象是不同的，表7-4列出了各个权限模式和授权对象的对应关系。

| 权限模式 | 授权对象                                                  |
| -------- | --------------------------------------------------------- |
| IP       | 通常是一个IP地址或是IP段                                  |
| Digest   | 自定义，通常是“username:BASE64(SHA-1(username:password))” |
| World    | 只有一个ID：“anyone”                                      |
| Super    | 与Digest模式一致                                          |

**权限：Permission**

权限就是指那些通过权限检查后可以被允许执行的操作。在ZooKeeper中，所有对数据的操作权限分为以下五大类：

- **CREATE（C）**：数据节点的创建权限，允许授权对象在该数据节点下创建子节点。
- **DELETE（D）**：子节点的删除权限，允许授权对象删除该数据节点的子节点。
- **READ（R）**：数据节点的读取权限，允许授权对象访问该数据节点并读取其数据内容或子节点列表等。
- **WRITE（W）**：数据节点的更新权限，允许授权对象对该数据节点进行更新操作。
- **ADMIN（A）**：数据节点的管理权限，允许授权对象对该数据节点进行ACL相关的设置操作。

#### 权限扩展体系

在上文中，我们已经讲解了ZooKeeper默认提供的IP、Digest、World和Super这四种权限模式，在绝大部分的场景下，这四种权限模式已经能够很好地实现权限控制地目的。同时，ZooKeeper提供了特殊的权限控制插件体系，允许开发人员通过指定方式对ZooKeeper的权限进行扩展。这些扩展的权限控制方式就像插件一样插入到ZooKeeper的权限体系中去，因此在ZooKeeper的官方文档中，也称该机制为“Pluggable ZooKeeper Authentication”。

**实现自定义权限控制器**

**注册自定义权限控制器**

#### ACL管理

**设置ACL**

**Super模式的用法**

## 7.2 序列化与协议

### 7.2.1 Jute介绍

Jute是ZooKeeper中的序列化组件，最初也是Hadoop中的默认序列化组件，其前身是Hadoop Record IO中的序列化组件，后来由于Apache Avro具有出众的跨语言特性、丰富的数据结构和对MapReduce的天生支持，并且能非常方便地用于RPC调用，从而深深吸引了Hadoop。因此Hadoop从0.21.0版本开始，废弃了Record IO，使用了Avro这个序列化框架，同时Jute也从Hadoop工程中被剥离出来，成为了独立的序列化组件。

## 7.3 客户端

客户端是开发人员使用ZooKeeper最主要的途径，因此我们有必要对ZooKeeper客户端的内部原理进行详细讲解。ZooKeeper的客户端主要由以下几个核心组件组成。

- **ZooKeeper实例**：客户端入口。
- **ClientWatchManager**：客户端Watcher管理器。
- **HostProvider**：客户端地址列表管理器。
- **ClientCnxn**：客户端核心线程，其内部又包含两个线程，即SendThread和EventThread。前者是一个I/O线程，主要负责ZooKeeper客户端和服务端之间的网络I/O通信；后者是一个事件线程，主要负责对服务端事件进行处理。

客户端的整个初始化和启动过程大体可以分为以下3个步骤。

1. 设置默认Watcher。
2. 设置ZooKeeper服务器地址列表。
3. 创建ClientCnxn。

如果在ZooKeeper的构造方法中传入一个Watcher对象的话，那么ZooKeeper就会将这个Watcher对象保存在ZKWatchManager的defaultWatcher中，作为整个客户端会话期间的默认Watcher。关于Watcher的更多详细讲解，已经在7.1.4节中做了详细说明。

### 7.3.1 一次会话的创建过程

**初始化阶段**

1. 初始化ZooKeeper对象。
   通过调用ZooKeeper的构造方法来实例化一个ZooKeeper对象，在初始化过程中，会创建一个客户端的Watcher管理器：ClientWatchManager。
2. 设置会话默认Watcher。
   如果在构造方法中传入了一个Watcher对象，那么客户端会将这个对象作为默认Watcher保存在ClientWatchManager中。
3. 构造ZooKeeper服务器地址列表管理器：HostProvider。
   对于构造方法中传入的服务器地址，客户端会将其存放在服务器地址列表管理器HostProvider中。
4. 创建并初始化客户端网络连接器：ClientCnxn。
   ZooKeeper客户端首先会创建一个网络连接器ClientCnxn，用来管理客户端与服务器的网络交互。另外，客户端在创建ClientCnxn的同时，还会初始化客户端两个核心队列outgoingQueue和pendingQueue，分别作为客户端的请求发送队列和服务端响应的等待队列。
   在后面的章节中我们也会讲到，ClientCnxn连接器的底层I/O处理器是ClientCnxnSocket，因此在这一步中，客户端还会同时创建ClientCnxnSocket处理器。
5. 初始化SendThread和EventThread。
   客户端会创建两个核心网络线程SendThread和EventThread，前者用于管理客户端和服务端之间的所有网络I/O，后者则用于进行客户端的事件处理。同时，客户端还会将ClientCnxnSocket分配给SendThread作为底层网络I/O处理器，并初始化EventThread的待处理事件队列waitingEvents，用于存放所有等待被客户端处理的事件。

**会话创建阶段**

6. 启动SendThread 和 EventThread
   SendThread首先会判断当前客户端的状态，进行一系列清理性工作，为客户端发送“会话创建”请求做准备。
7. 获取一个服务器地址。
   在开始创建TCP连接之前，SendThread首先需要获取一个ZooKeeper服务器的目标地址，这通常是从HostProvider中随机获取出一个地址，然后委托给ClientCnxnSocket去创建与ZooKeeper服务器之间的TCP连接。
8. 创建TCP连接。
   获取到一个服务器地址后，ClientCnxnSocket负责和服务器创建一个TCP长连接。
9. 创造ConnectRequest请求。
   在TCP连接创建完毕后，可能有的读者会认为，这样是否就说明已经和ZooKeeper服务器完成连接了呢？其实不然，步骤8只是纯粹地从网络TCP层面完成了客户端与服务端之间的Socket连接，但远未完成ZooKeeper客户端的会话创建。
   SendThread会负责根据当前客户端的实际设置，构造出一个ConnectRequest请求，该请求代表了客户端视图与服务器创建一个会话。同时，ZooKeeper客户端还会进一步将该请求包装成网络I/O层的Packet对象，放入请求发送队列outgoingQueue中去。
10. 发送请求
    当客户端请求准备完毕后，就可以开始向服务端发送请求了。ClientCnxnSocket负责从outgoingQueue中取出一个待发送的Packet对象，讲其序列化成ByteBuffer后，向服务端进行发送。

**响应处理阶段**

11. 接收服务端响应。
    ClientCnxnSocket接收到服务端的响应后，会首先判断当前的客户端状态是否是“已初始化”，如果尚未完成初始化，那么就认为该响应一定是会话创建请求的响应，直接交由readConnectResult方法来处理该响应。
12. 处理Response。
    ClientCnxnSocket会对接收到的服务端响应进行反序列化，得到ConnectResponse对象，并从中获取到ZooKeeper服务端分配的会话sessionId。
13. 连接成功。
    连接成功后，一方面需要通知SendThread线程，进一步对客户端进行会话参数的设置，包括readTimeout和connectTimeout等，并更新客户端状态：另一方面，需要通知地址管理器HostProvider当前成功连接的服务器地址。
14. 生成事件：SyncConnected-None。
    为了能够让上层应用感知到会话的成功创建，SendThread会生成一个事件SyncConnected-None，代表客户端与服务器会话创建成功，并将该事件传递给EventThread线程。
15. 查询Watcher。
    EventThread线程收到事件后，会从ClientWatchManager管理器中查询出对应的Watcher，针对SyncConnected-None事件，那么就直接找出步骤2中存储的默认Watcher，然后将其放到EventThread的waitingEvents队列中去。
16. 处理事件。
    EventThread不断地从waitingEvents队列中取出待处理的Watcher对象，然后直接调用该对象的process接口方法，以达到触发Watcher的目的。

### 7.3.2 服务器地址列表

在使用ZooKeeper构造方法时，用户传入的ZooKeeper服务器地址列表，即connectString参数，通常是这样一个使用英文状态逗号分隔的多个IP地址和断开的字符串：

~~~
192.168.0.1:2181,192.168.0.1:2181,192.168.0.1:2181
~~~

从这个地址串中我们可以看出，ZooKeeper客户端允许我们将服务器的所有地址都配置在一个字符串上，于是一个问题就来了：ZooKeeper客户端在连接服务器的过程中，是如何从这个服务器列表中选择服务器机器的呢？是按序访问，还是随机访问呢？

ZooKeeper客户端内部在接收到这个服务器地址列表后，会将其首先放入一个ConnectStringParser对象中封装起来。ConnectStringParser是一个服务器地址列表的解析器，该类的基本结构如下：

~~~java
public final class ConnectStringParser {
    String chrootPath;
    ArrayList<InetSocketAddress> serverAddress = new ArrayList<InetSocketAddress>();
}
~~~

ConnectStringParser解析器将会对传入的connectString做两个主要处理：解析chrootPath；保存服务器地址列表。

#### Chroot：客户端隔离命名空间

在3.2.0及其之后版本的ZooKeeper中，添加了“Chroot”特性，该特性允许每个客户端为自己设置一个命名空间（Namespace）。如果一个ZooKeeper客户端设置了Chroot，那么该客户端对服务器的任何操作，都将会被限制在其自己的命名空间下。

通过设置Chroot，我们能够将一个客户端应用与ZooKeeper服务端的一棵子树相对应，在那些多个应用共用一个ZooKeeper集群的场景下，这对于实现不同应用之间的相互隔离非常有帮助。

客户端可以通过在connectString中添加后缀的方式来设置Chroot，如下所示：

~~~
192.168.0.1:2181,192.168.0.1:2181,192.168.0.1:2181/apps/X
~~~

将这样一个connectString传入客户端的ConnectStringParser后就能够解析出Chroot并保存在chrootPath属性中。

#### HostProvider：地址列表管理器

在ConnectStringParser解析器中会对服务器地址做一个简单的处理，并将服务器地址和相应的端口封装成一个InetSocketAddress对象，以ArrayList形式保存在ConnectStringParser.serverAddresses属性中。然后，经过处理的地址列表会被进一步封装到StaticHostProvider类中。

#### StaticHostProvider

接下来我们看看ZooKeeper客户端中对HostProvider的默认实现：StaticHostProvider。

**解析服务器地址**

针对ConnectStringParser.serverAddressed集合中那些没有被解析的服务器地址，StaticHostProvider首先会对这些地址逐个进行解析，然后再放入serverAddresses集合中去。同时，使用Collections工具类的shuffle方法来将这个服务器地址列表进行随机的打散。

**获取可用的服务器地址**

通过调用StaticHostProvider的next()方法，能够从StaticHostProvider中获取一个可用的服务器地址。这个next()方法并非简单地从serverAddresses中依次获取一个服务器地址，而是先将随机打散后的服务器地址列表拼装成一个环形循环队列。注意，这个随机过程是一次性的，也就是说，之后的使用过程中一直是按照这样的顺序来获取服务器地址的。

#### 对HostProvider的几个设想

### 7.3.3 ClientCnxn：网络I/O

ClientCnxn是ZooKeeper客户端的核心工作类，负责维护客户端与服务端之间的网络连接并进行一系列网络通信。

#### Packet

Packet是ClientCnxn内部定义的一个对协议层的封装，作为ZooKeeper中请求与响应的载体。

Packet中包含了最基本的请求头（requestHeader）、响应头（replyHeader）、请求体（request）、响应体（response）、节点路径（clientPath/serverPath）和注册的Watcher（watchRegistration）等信息。

针对Packet中这么多的属性，读者可能会疑惑它们是否都会在客户端和服务端之间进行网络传输？答案是否定的。Packet的createBB()方法负责对Packet对象进行序列化，最终生成可用于底层网络传输的ByteBuffer对象。在这个过程中，只会将requestHeader、request和readOnly三个属性进行序列化，其余属性都保存在客户端的上下文中，不会进行与服务端之间的网络传输。

#### outgoingQueue 和 pendingQueue

ClientCnxn中，有两个比较核心的队列outgoingQueue 和 pendingQueue，分别代表客户端的请求发送队列和服务端响应的等待队列。Outgoing队列是一个请求发送队列，专门用于存储那些需要发送到服务端的Packet集合。Pending队列是为了存储那些已经从客户端发送到服务端的，但是需要等待服务端响应的Packet集合。

#### ClientCnxnSocket：底层Socket通信层

ClientCnxnSocket定义了底层Socket通信的接口。在ZooKeeper 3.4.0以前的版本中，客户端的这个底层通信层并没有被独立出来，而是混合在了ClientCnxn代码中。但后来为了使客户端代码结构更为清晰，同时也是为了便于对底层Socket层进行扩展（例如使用Netty来实现），因此从3.4.0版本开始，抽取出了这个接口类。在ZooKeeper中，其默认的实现是ClientCnxnSocketNIO。该实现类使用Java原生的NIO接口，其核心是doIO逻辑，主要负责对请求的发送和响应接受过程。

**请求发送**

**响应接收**

#### SendThread

SendThread是客户端ClientCnxn内部一个核心的I/O调度线程，用于管理客户端和服务端之间的所有网络I/O操作。在ZooKeeper客户端的实际允许过程中，一方面，SendThread维护了客户端与服务端之间的会话生命周期，其通过在一定的周期频率内向服务端发送一个PING包来实现心跳检测。同时，在会话周期内，如果客户端与服务端之间出现TCP连接断开的情况，那么就会自动且透明化地完成重连操作。

另一方面，SendThread管理了客户端所有的请求发送和响应接收操作，其将上层客户端API操作转换成相应的请求协议并发送到服务器，并完成对同步调用的返回和异步调用的回调。同时，SendThread还负责将来自服务端的事件传递给EventThread去处理。

#### EventThread

EventThread是客户端ClientCnxn内部的另一个核心线程，负责客户端的事件处理，并触发客户端注册的Watcher监听。EventThread中有一个waitingEvents队列，用于临时存放那些需要被触发的Object，包括那些客户端注册的Watcher和异步接口中注册的回调器AsyncCallback。同时，EventThread会不断地从waitingEvents这个队列中取出Object，识别出其具体类型（Watcher或者AsyncCallback），并分别调用process和processResult接口方法来实现对事件的触发和回调。

## 7.4 会话

