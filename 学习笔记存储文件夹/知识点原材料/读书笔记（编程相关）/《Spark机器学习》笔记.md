# 第1章 Spark的环境搭建与运行

Apache Spark是一个分布式计算框架，旨在简化运行于计算机集群上的并行程序的编写。该框架对资源调度，任务的提交、执行和跟踪，节点间的通信以及数据并行处理的内在底层操作都进行了抽象。它提供了一个更高级别的API用于处理分布式数据。从这方面说，它与Apache Hadoop等分布式处理框架类似。但在底层架构上，Spark与它们有所不同。

Spark起源于加利福利亚大学伯克利分校的一个研究项目。学校当时关注分布式机器学习算法的应用情况。因此，Spark从一开始便为应对迭代式应用的高性能需求而设计。在这类应用中，相同的数据会被多次访问。该设计主要靠利用数据集内存缓存以及启动任务时的低延迟和低系统开销来实现高性能。再加上其容错性、灵活的分布式数据结构和强大的函数式编程接口，Spark在各类基于机器学习和迭代分析的大规模数据处理任务上有广泛的应用，这也表明了其实用性。

Spark支持四种运行模式。

- 本地单机模式：所有Spark进程都运行在同一个Java虚拟机（Java Vitural Machine，JVM）中。
- 集群单机模式：使用Spark自己内置的任务调度框架。
- 基于Mesos：Mesos是一个流行的开源集群计算框架。
- 基于YARN：即Hadoop 2，它是一个与Hadoop关联的集群计算和资源调度框架。

## 1.1 Spark的本地安装与配置

Spark能通过内置的单机集群调度器来在本地运行。此时，所有的Spark进程运行在同一个Java虚拟机中。这实际上构造了一个独立、多线程版本的Spark环境。本地模式很适合程序的原型设计、开发、调试及测试。同样，它也适应于在单机上进行多核并行计算的实际场景。

Spark的本地模式与集群模式完全兼容，本地编写和测试过的程序仅需增加少许设置便能在集群上运行。

为了访问**HDFS**（Hadoop Distributed File System，Hadoop分布式文件系统）以及标准或定制的Hadoop输入源，Spark的编译需要与Hadoop的版本对应。上述下载页面提供了针对Hadoop 1、CDH4（Cloudera的Hadoop发行版）、MapR的Hadoop发行版和Hadoop 2（YARN）的预编译二进制包。除非你想构建针对特定版本Hadoop的Spark，否则建议你通过如下链接从Apache镜像下载Hadoop 2.4 预编译版本

Spark的运行依赖Scala编程语言（本书写作时为2.10.4版）。好在预编译的二进制包中已包含Scala运行环境，我们不需要另外安装Scala便可运行Spark。但是，JRE（Java运行时环境）或JDK（Java开发套件）是要安装的（相应的安装指南可参见本书代码包中的软硬件列表）。

## 1.2 Spark集群

Spark集群由两类程序构成：一个驱动程序和多个执行程序。本地模式时所有的处理都运行在同一个JVM内，而在集群模式时它们通常运行在不同的节点上。

举例来说，一个采用单机模式的Spark集群（即使用Spark内置的集群管理模块）通常包括：

- 一个运行Spark单机主进程和驱动程序的主节点；
- 各自运行一个执行程序进程的多个工作节点。

在本书中，我们将使用Spark的本地单机模式做概念讲解和举例说明，但所用的代码也可运行在Spark集群上。比如在一个Spark单机集群上运行上述示例，只需传入主节点的URL即可

## 1.3 Spark编程模型

在对Spark的设计进行更全面的介绍前，我们先介绍SparkContext对象以及Spark shell。后面将通过它们来了解Spark编程模型的基础知识。

### 1.3.1 SparkContext类与SparkConf类

任何Spark程序的编写都是从SparkContext（或用Java编写时的JavaSparkContext）开始的。SparkContext的初始化需要一个SparkConf对象，后者包含了Spark集群配置的各种参数（比如主节点的URL）。

初始化后，我们便可用SparkContext对象所包含的各种方法来创建和操作分布式数据集和共享变量。Spark shell（在Scala和Python下可以，但不支持Java）能自动完成上述初始化。若要用Scala代码来实现的话，可参照下面的代码：

~~~scala
val conf = new SparkConf() 
.setAppName("Test Spark App") 
.setMaster("local[4]") 
val sc = new SparkContext(conf)
~~~

这段代码会创建一个4线程的SparkContext对象，并将其相应的任务命名为Test Spark APP。我们也可通过如下方式调用SparkContext的简单构造函数，以默认的参数值来创建相应的对象。其效果和上述的完全相同：

~~~scala
val sc = new SparkContext("local[4]", "Test Spark App")
~~~

### 1.3.2 Spark shell

Spark支持用Scala或Python REPL（Read-Eval-Print-Loop，即交互式shell）来进行交互式的程序编写。由于输入的代码会被立即计算，shell能在输入代码时给出实时反馈。在Scala shell里，命令执行结果的值与类型在代码执行完后也会显示出来。

要想通过Scala来使用Spark shell，只需从Spark的主目录执行`./bin/spark-shell`。它会启动Scala shell并初始化一个SparkContext对象。我们可以通过sc这个Scala值来调用这个对象。

### 1.3.3 弹性分布式数据集

**RDD**（Resilient Distributed Dataset，弹性分布式数据集）是Spark的核心概念之一。一个RDD代表一系列的“记录”（严格来说，某种类型的对象）。这些记录被分配或分区到一个集群的多个节点上（在本地模式下，可以类似地理解为单个进程里的多个线程上）。Spark中的RDD具备容错性，即当某个节点或任务失败时（因非用户代码错误的原因而引起，如硬件故障、网络不通等），RDD会在余下的节点上自动重建，以便任务能最终完成。

#### 1.创建RDD

RDD可从现有的集合创建。比如在Scala shell中：

~~~scala
val collection = List("a", "b", "c", "d", "e") 
val rddFromCollection = sc.parallelize(collection)
~~~

RDD也可以基于Hadoop的输入源创建，比如本地文件系统、HDFS和Amazon S3。基于Hadoop的RDD可以使用任何实现了Hadoop InputFormat接口的输入格式，包括文本文件、其他Hadoop标准格式、HBase、Cassandra等。以下举例说明如何用一个本地文件系统里的文件创建RDD：

~~~scala
val rddFromTextFile = sc.textFile("LICENSE")
~~~

上述代码中的textFile函数（方法）会返回一个RDD对象。该对象的每一条记录都是一个表示文本文件中某一行文字的String（字符串）对象。

#### 2.Spark操作

创建RDD后，我们便有了一个可供操作的分布式记录集。在Spark编程模式下，所有的操作被分为转换（transformation）和执行（action）两种。一般来说，转换操作是对一个数据集里的所有记录执行某种函数，从而使记录发生改变；而执行通常是运行某些计算或聚合操作，并将结果返回运行SparkContext的那个驱动程序。

Spark的操作通常采用函数式风格。对于那些熟悉用Scala或Python进行函数式编程的程序员来说，这不难掌握。但Spark API其实容易上手，所以那些没有函数式编程经验的程序员也不用担心。

Spark程序中最常用的转换操作便是map操作。该操作对一个RDD里的每一条记录都执行某个函数，从而将输入映射成为新的输出。比如，下面这段代码便对一个从本地文本文件创建的RDD进行操作。它对该RDD中的每一条记录都执行size函数。之前我们曾创建过一个这样的由若干String构成的RDD对象。通过map函数，我们将每一个字符串都转换为一个整数，从而返回一个由若干Int构成的RDD对象。

~~~scala
val intsFromStringsRDD = rddFromTextFile.map(line => line.size)
~~~

其输出应与如下类似，其中也提示了RDD的类型：

~~~
intsFromStringsRDD: org.apache.spark.rdd.RDD[Int] = MappedRDD[5] at map at <console>:14
~~~

示例代码中的=>是Scala下表示匿名函数的语法。匿名函数指那些没有指定函数名的函数（比如Scala或Python中用def关键字定义的函数）。

现在我们可以调用一个常见的执行操作count，来返回RDD中的记录数目。

~~~scala
intsFromStringsRDD.count
~~~

执行的结果应该类似如下输出：

~~~
14/01/29 23:28:28 INFO SparkContext: Starting job: count at <console>:17 ... 
14/01/29 23:28:28 INFO SparkContext: Job finished: count at <console>:17, took 0.019227 s res4: Long = 398
~~~

如果要计算这个文本文件里每行字符串的平均长度，可以先使用sum函数来对所有记录的长度求和，然后再除以总的记录数目：

~~~scala
val sumOfRecords = intsFromStringsRDD.sum 
val numRecords = intsFromStringsRDD.count 
val aveLengthOfRecord = sumOfRecords / numRecords
~~~

结果应该如下：

~~~
aveLengthOfRecord: Double = 52.06030150753769
~~~

Spark的大多数操作都会返回一个新RDD，但多数的执行操作则是返回计算的结果（比如上面例子中，count返回一个Long，sum返回一个Double）。这就意味着多个操作可以很自然地前后连接，从而让代码更为简洁明了。举例来说，用下面的一行代码可以得到和上面例子相同的结果：

~~~scala
val aveLengthOfRecordChained = rddFromTextFile.map(line => line.size).sum / rddFromTextFile.count
~~~

值得注意的一点是，Spark中的转换操作是延后的。也就是说，在RDD上调用一个转换操作并不会立即触发相应的计算。相反，这些转换操作会链接起来，并只在有执行操作被调用时才被高效地计算。这样，大部分操作可以在集群上并行执行，只有必要时才计算结果并将其返回给驱动程序，从而提高了Spark的效率。

这就意味着，如果我们的Spark程序从未调用一个执行操作，就不会触发实际的计算，也不会得到任何结果。

#### 3.RDD缓存策略

Spark最为强大的功能之一便是能够把数据缓存在集群的内存里。这通过调用RDD的cache函数来实现：

~~~scala
rddFromTextFile.cache
~~~

调用一个RDD的cache函数将会告诉Spark将这个RDD缓存在内存中。在RDD首次调用一个执行操作时，这个操作对应的计算会立即执行，数据会从数据源里读出并保存到内存。因此，首次调用cache函数所需要的时间会部分取决于Spark从输入源读取数据所需要的时间。但是，当下一次访问该数据集的时候，数据可以直接从内存中读出从而减少低效的I/O操作，加快计算。多数情况下，这会取得数倍的速度提升。

如果现在在已缓存了的RDD上调用count或sum函数，应该可以感觉到RDD的确已经载入到了内存中：

### 1.3.4 广播变量和累加器

Spark的另一个核心功能是能创建两种特殊类型的变量：广播变量和累加器。

广播变量（broadcast variable）为只读变量，它由运行SparkContext的驱动程序创建后发送给会参与计算的节点。对那些需要让各工作节点高效地访问相同数据的应用场景，比如机器学习，这非常有用。Spark下创建广播变量只需在SparkContext上调用一个方法即可：

~~~scala
val broadcastAList = sc.broadcast(List("a", "b", "c", "d", "e"))
~~~

广播变量也可以被非驱动程序所在的节点（即工作节点）访问，访问的方法是调用该变量的value方法：

~~~scala
sc.parallelize(List("1", "2", "3")).map(x => broadcastAList.value ++ x).collect
~~~

这段代码会从{"1", "2", "3"}这个集合（一个Scala List）里，新建一个带有三条记录的RDD。map函数里的代码会返回一个新的List对象。这个对象里的记录由之前创建的那个broadcastAList里的记录与新建的RDD里的三条记录分别拼接而成。

注意，上述代码使用了collect函数。这个函数是一个Spark执行函数，它将整个RDD以Scala（Python或Java）集合的形式返回驱动程序。

通常只在需将结果返回到驱动程序所在节点以供本地处理时，才调用collect函数。

从如下结果可以看出，新生成的RDD里包含3条记录，其每一条记录包含一个由原来被广播的List变量附加一个新的元素所构成的新记录（也就是说，新记录分别以1、2、3结尾）。

~~~
... 
14/01/31 10:15:39 INFO SparkContext: Job finished: collect at <console>:15, took 0.025806 s res6: Array[List[Any]] = Array(List(a, b, c, d, e, 1), List(a, b, c, d, e, 2), List(a, b, c, d, e, 3))
~~~

累加器（accumulator）也是一种被广播到工作节点的变量。累加器与广播变量的关键不同，是后者只能读取而前者却可累加。但支持的累加操作有一定的限制。具体来说，这种累加必须是一种有关联的操作，即它得能保证在全局范围内累加起来的值能被正确地并行计算以及返回驱动程序。每一个工作节点只能访问和操作其自己本地的累加器，全局累加器则只允许驱动程序访问。累加器同样可以在Spark代码中通过value访问。

## 1.4 Spark Scala 编程入门

下面我们用上一节所提到的内容来编写一个简单的Spark数据处理程序。该程序将依次用Scala、Java和Python三种语言来编写。所用数据是客户在我们在线商店的商品购买记录。该数据存在一个CSV文件中，名为UserPurchaseHistory.csv，内容如下所示。文件的每一行对应一条购买记录，从左到右的各列值依次为客户名称、商品名以及商品价格。

~~~
John,iPhone Cover,9.99 
John,Headphones,5.49 
Jack,iPhone Cover,9.99 
Jill,Samsung Galaxy Cover,8.95 
Bob,iPad Cover,5.49
~~~

对于Scala程序而言，需要创建两个文件：Scala代码文件以及项目的构建配置文件。项目将使用**SBT**（Scala Build Tool，Scala构建工具）来构建。

我们的SBT配置文件是build.sbt，其内容如下面所示（注意，各行代码之间的空行是必需的）

~~~sbt
name := "scala-spark-app" 

version := "1.0" 

scalaVersion := "2.10.4" 

libraryDependencies += "org.apache.spark" %% "spark-core" % "1.2.0 "
~~~

最后一行代码是添加Spark到本项目的依赖库。相应的Scala程序在ScalaApp.scala这个文件里。接下来我们会逐一讲解代码的各个部分。首先，导入所需要的Spark类：

~~~scala
import org.apache.spark.SparkContext 
import org.apache.spark.SparkContext._ 
/** 
 * 用Scala编写的一个简单的Spark应用
 */ 
object ScalaApp {
~~~

在主函数里，我们要初始化所需的SparkContext对象，并且用它通过textFile函数来访问CSV数据文件。之后对每一行原始字符串以逗号为分隔符进行分割，提取出相应的用户名、产品和价格信息，从而完成对原始文本的映射：

~~~scala
def main(args: Array[String]) { 
    val sc = new SparkContext("local[2]", "First Spark App") 
    // 将CSV格式的原始数据转化为(user,product,price)格式的记录集
    val data = sc.textFile("data/UserPurchaseHistory.csv") 
    	.map(line => line.split(",")) 
    	.map(purchaseRecord => (purchaseRecord(0), purchaseRecord(1), purchaseRecord(2)))
~~~

现在，我们有了一个RDD，其每条记录都由(user, product, price)三个字段构成。我们可以对商店计算如下指标：

- 购买总次数
- 客户总个数
- 总收入
- 最畅销的产品

计算方法如下：

~~~scala
// 求购买次数
val numPurchases = data.count() 
// 求有多少个不同客户购买过商品
val uniqueUsers = data.map{ case (user, product, price) => user }.distinct().count() 
// 求和得出总收入
val totalRevenue = data.map{ case (user, product, price) => price.toDouble }.sum() 
// 求最畅销的产品是什么
val productsByPopularity = data 
    .map{ case (user, product, price) => (product, 1) } 
    .reduceByKey(_ + _) 
    .collect() 
    .sortBy(-_._2) 
val mostPopular = productsByPopularity(0)
~~~

最后那段计算最畅销产品的代码演示了如何进行Map/Reduce模式的计算，该模式随Hadoop而流行。第一步，我们将(user, product, price)格式的记录映射为(product, 1)格式。然后，我们执行一个reduceByKey操作，它会对各个产品的1值进行求和。

转换后的RDD包含各个商品的购买次数。有了这个RDD后，我们可以调用collect函数，这会将其计算结果以Scala集合的形式返回驱动程序。之后在驱动程序的本地对这些记录按照购买次数进行排序。（注意，在实际处理大量数据时，我们通常通过sortByKey这类操作来对其进行并行排序。）

最后，可在终端上打印出计算结果：

~~~scala
        println("Total purchases: " + numPurchases) 
        println("Unique users: " + uniqueUsers) 
        println("Total revenue: " + totalRevenue) 
        println("Most popular product: %s with %d purchases".format(mostPopular._1, mostPopular._2)) 
	} 
}
~~~

可以在项目的主目录下执行sbt run命令来运行这个程序。如果你使用了IDE的话，也可以从Scala IDE直接运行。

## 1.5 Spark Java 编程入门

Java API与Scala API本质上很相似。Scala代码可以很方便地调用Java代码，但某些Scala代码却无法在Java里调用，特别是那些使用了隐式类型转换、默认参数和采用了某些Scala反射机制的代码。

一般来说，这些特性在Scala程序中会被广泛使用。这就有必要另外为那些常见的类编写相应的Java版本。由此，SparkContext有了对应的Java版本JavaSparkContext，而RDD则对应JavaRDD。

1.8及之前版本的Java并不支持匿名函数，在函数式编程上也没有严格的语法规范。于是，套用到Spark的Java API上的函数必须要实现一个带有call函数的WrappedFunction接口。这会使得代码冗长，所以我们经常会创建临时类来传递给Spark操作。这些类会实现操作所需的接口以及call函数，以取得和用Scala编写时相同的效果。

Spark提供对Java 8匿名函数（lambda）语法的支持。使用该语法能让Java 8书写的代码看上去很像等效的Scala版。

用Scala编写时，键/值对记录的RDD能支持一些特别的操作（比如reduceByKey和saveAsSequenceFile）。这些操作可以通过隐式类型转换而自动被调用。用Java编写时，则需要特别类型的JavaRDD来支持这些操作。它们包括用于键/值对的JavaPairRDD，以及用于数值记录的JavaDoubleRDD。

项目中包含一个名为JavaApp.java的Java源文件：

~~~java
import org.apache.spark.api.java.JavaRDD; 
import org.apache.spark.api.java.JavaSparkContext; 
import org.apache.spark.api.java.function.DoubleFunction; 
import org.apache.spark.api.java.function.Function; 
import org.apache.spark.api.java.function.Function2; 
import org.apache.spark.api.java.function.PairFunction; 
import scala.Tuple2; 
import java.util.Collections; 
import java.util.Comparator; 
import java.util.List; 
/** 
 * 用Java编写的一个简单的Spark应用
 */ 
public class JavaApp { 
    public static void main(String[] args) {
~~~

正如在Scala项目中一样，我们首先需要初始化一个上下文对象。值得注意的是，这里所使用的是JavaSparkContext类而不是之前的SparkContext。类似地，调用JavaSparkContext对象，利用textFile函数来访问数据，然后将各行输入分割成多个字段。请注意下面代码的高亮部分是如何使用匿名类来定义一个分割函数的。该函数确定了如何对各行字符串进行分割。

~~~java
JavaSparkContext sc = new JavaSparkContext("local[2]", "First Spark App"); 
// 将CSV格式的原始数据转化为(user,product,price)格式的记录集
JavaRDD<string[]> data = 
    sc.textFile("data/UserPurchaseHistory.csv") 
    .map(new Function<String, String[]>() {
        @Override
        public String[] call(String s) throws Exception { 
            return s.split(","); 
        } 
    });
~~~

现在可以算一下用Scala时计算过的指标。这里有两点值得注意的地方，一是下面Java API中有些函数（比如distinct和count）实际上和在Scala API中一样，二是我们定义了一个匿名类并将其传给map函数。匿名类的定义方式可参见代码的高亮部分。

~~~java
// 求总购买次数
long numPurchases = data.count(); 
// 求有多少个不同客户购买过商品
long uniqueUsers = data.map(new Function<String[], String>() { 
    @Override
    public String call(String[] strings) throws Exception { 
        return strings[0]; 
    } 
}).distinct().count(); 
// 求和得出总收入
double totalRevenue = data.map(new DoubleFunction<String[]>() {
    @Override
    public Double call(String[] strings) throws Exception { 
        return Double.parseDouble(strings[2]); 
    } 
}).sum();
~~~

下面的代码展现了如何求出最畅销的产品，其步骤与Scala示例的相同。多出的那些代码看似复杂，但它们大多与Java中创建匿名函数有关，实际功能与用Scala时一样：

~~~java
        // 求最畅销的产品是哪个
        // 首先用一个PairFunction和Tuple2类将数据映射成为(product,1)格式的记录
        // 然后，用一个Function2类来调用reduceByKey操作，该操作实际上是一个求和函数
        List<Tuple2<String, Integer>> pairs = 
            data.map(new PairFunction<String[], String, Integer>() {
                @Override 
                public Tuple2<String, Integer> call(String[] strings) throws Exception { 
                    return new Tuple2(strings[1], 1); 
                } 
            }).reduceByKey(new Function2<Integer, Integer, Integer>() {
                @Override
                public Integer call(Integer integer, Integer integer2) throws Exception { 
                    return integer + integer2; 
                } 
            }).collect(); 
        // 最后对结果进行排序。注意，这里会需要创建一个Comparator函数来进行降序排列
        Collections.sort(pairs, new Comparator<Tuple2<String, Integer>>() { 
            @Override 
            public int compare(Tuple2<String, Integer> o1, 
                               Tuple2<String, Integer> o2) { 
                return -(o1._2() - o2._2()); 
            } 
        }); 
        String mostPopular = pairs.get(0)._1(); 
        int purchases = pairs.get(0)._2(); 
        System.out.println("Total purchases: " + numPurchases); 
        System.out.println("Unique users: " + uniqueUsers); 
        System.out.println("Total revenue: " + totalRevenue); 
        System.out.println(String.format("Most popular product: %s with %d purchases", mostPopular, purchases)); 
	} 
}
~~~

从前面代码可以看出，Java代码和Scala代码相比虽然多了通过内部类来声明变量和函数的引用代码，但两者的基本结构类似。读者不妨分别练习这两种版本的代码，并比较一下计算同一个指标时两种语言在表达上的异同。

## 1.6 Spark Python 编程入门

对比Scala版和Python版代码，不难发现语法大致相同。主要不同在于匿名函数的表达方式上，匿名函数在Python语言中亦称lambda函数，lambda也是语法表达上的关键字。用Scala编写时，一个将输入x映射为输出y的匿名函数表示为x => y，而在Python中则是lambda x : y。

## 1.7 在 Amazon EC2 上运行 Spark 

# 第2章 设计机器学习系统

本章，我们将为一个智能分布式机器学习系统设计高层架构，该系统以Spark作为其核心计算引擎。这里我们将会关注如何对现有的基于网页的业务进行重新设计，以令其能利用自动化机器学习系统来增强业务中的关键部分。本章的主要内容有：

- 介绍假想的业务场景
- 概述现有架构
- 探寻用机器学习系统来增强或是替代某些业务功能的可能途径
- 根据上述内容，提出新的架构

现代的大数据场景包含如下需求。

- 必须能与系统的其他组件整合，尤其是数据的收集和存储系统、分析和报告以及前端应用。
- 易于扩展且与其他组件相对独立。理想情况下，同时具备良好的水平和垂直可扩展性。
- 支持高效完成所需类型的计算，即机器学习和迭代式分析应用。
- 最好能同时支持批处理和实时处理。

Spark作为一个框架本身能满足上述需求。然而我们还需确保基于它设计的机器学习系统也能满足这些需求。若算法的实现存在能引发系统故障的瓶颈，比如不再能满足上述某些需求，那该实现就没多大意义。

## 2.1 MovieStream 介绍

为便于说明我们的架构设计，这里假设存在一个贴近现实的情景。假设我们受命领导MovieStream数据科学团队。MovieStream是一家假想的互联网公司，为用户提供在线电影和电视节目的内容服务。

向用户推荐哪些电影和节目以及在站点的何处显示，都由MovieStream内容编辑团队负责。该团队还负责MovieStream的群发营销，包括电子邮件和其他直销渠道。现阶段，MovieStream以汇总的方式来收集用户的电影浏览记录，并能访问一些用户注册时所填写的资料。此外，他们还能访问其所收录的电影的一些基本元数据。

随着业务快速发展，新发布的电影和用户的活动不断增加，MovieStream团队愈发难以跟上这样的趋势。MovieStream的CEO之前对大数据、机器学习和人工智能有过较多了解。他希望我们能为MovieStream创建一个机器学习系统，以处理现在由内容团队人工处理的许多内容。

## 2.2 机器学习系统商业用例

我们该问的第一个问题或许是：为什么要使用机器学习？为何不直接仍以人工方式来支持MovieStream？使用机器学习的理由有很多（不使用的理由同样也有很多），其中最为重要的几点有：

- 涉及的数据规模意味着完全依靠人工处理会很快跟不上MovieStream的发展；
- 机器学习和统计模型等基于模型的方式能发现人类（因数据集量级和复杂度过高）难以发现的模式；
- 基于模型的方式能避免个人或是情感上的偏见（只要应用时足够细心且正确）

然而，没有任何理由说基于模型和基于人工的处理和决策不能并存。比如，许多机器学习系统依赖已标记的数据来训练模型。通常来说，标记数据代价高昂、耗时且需人工参与。文本数据分类和文本的情感标识便是很好的例子。许多现实中的系统会采取某种人力机制来为数据生成标识，并用于训练模型。之后，这些模型则部署到在线系统中用于大规模环境下的预测。

在MovieStream的案例中，我们并不需要担心机器学习的引入会使得内容团队多余。事实上，我们的目标是让机器学习来负担那些耗时且机器擅长的任务，并向内容团队提供工具以帮助他们更好地理解用户和内容。比如，帮助他们确定向电影库中新增哪些电影（新增电影代价高昂，因而对业务至关重要）。

### 2.2.1 个性化

对MovieStream的业务来说，个性化或许是机器学习最为重要的潜在应用。一般来说，个性化是根据各种因素来改变用户体验和呈现给用户内容。这些因素可能包括用户的行为数据和外部因素。

推荐（recommendation）从根本上说是个性化的一种，常指向用户呈现一个他们可能感兴趣的物品列表。推荐可用于网页（比如推荐相关产品）、电子邮件、其他直销渠道或移动应用等。

个性化和推荐十分相似，但推荐通常专指向用户显式地呈现某些产品或是内容，而个性化有时也偏向隐式。比如说，对MovieStream的搜索功能个性化，以根据该用户的数据来改变搜索结果。这些数据可能包括基于推荐的数据（在搜索产品或内容时），或基于地理位置和搜索历史等各种数据。用户可能不会明显感觉到搜索结果的变化，这就是个性化更偏向隐性的原因。

### 2.2.2 目标营销和客户细分

目标营销用与推荐类似的方法从用户群中找出要营销的对象。一般来说，推荐和个性化的应用场景都是一对一，而客户细分则试图将用户分成不同的组。其分组根据用户的特征进行，并可能参考行为数据。这种方法可能比较简单，也可能使用了某种机器学习模型，比如聚类。但无论如何，其结果都是对市场的若干细分。这些细分或许有助于理解各组用户的共性、同组用户之间的相似性，以及不同组之间的差异。

这些将能帮助MovieStream理解用户行为背后的动机。相比个性化时的一对一营销，它们甚至还能有助于制定针对用户群的更为广泛的营销策略。

当没有已标记数据时，这些方法能帮助制定营销策略，而非采取一刀切的方法。

### 2.2.3 预测建模与分析

第三种机器学习的应用领域是预测性分析。这个词的范围很宽泛，甚至从某种意义上说还覆盖推荐、个性化和目标营销。再考虑到推荐和市场细分有所区别，这里用预测建模（predictive modeling）来表示其他做预测的模型。借助活动记录、收入数据以及内容属性，MovieStream可以创建一个回归模型（regression model）来预测新电影的市场表现。

另外，我们也可使用分类模型（classificaiton model）来对只有部分数据的新电影自动分配标签、关键字或分类。

## 2.3 机器学习模型的种类

以上MovieSteam的例子列出了机器学习的一些应用场景，但这些并非全部。后面几章在介绍不同机器学习任务时还会提到一些相关例子。

以上应用案例和方法大致可分为如下两种。

- 监督学习（supervised learning）：这种方法使用已标记数据来学习。推荐引擎、回归和分类便是例子。它们所使用的标记数据可以是用户对电影的评级（对推荐来说）、电影标签（对上述分类例子来说）或是收入数字（对回归预测来说）。我们将在第4章、第5章和第6章讨论监督学习。
- 无监督学习（unsupervised learning）：一些模型的学习过程不需要标记数据，我们称其为无监督学习。这类模型试图学习或是提取数据背后的结构或从中抽取最为重要的特征。聚类、降维和文本处理的某些特征提取都是无监督学习。我们将在第7章、第8章和第9章分别介绍它们。

## 2.4 数据驱动的机器学习系统的组成

### 2.4.1 数据获取与存储

获取数据后通常需将其存储起来。要存储的数据包括：原始数据、即时处理后的数据，以及可用于生产系统的最终建模结果。

### 2.4.2 数据清理与转换

大部分机器学习模型所处理的都是特征（feature）。特征通常是输入变量所对应的可用于模型的数值表示。

虽然我们希望能将大部分时间用于机器学习模型探索，但通常经上述途径获取到的数据都是原始形式，需要进一步处理。比如我们记录的一些用户事件的细节，比如用户查看某部电影页面的时间、观看某部电影的时间或给出某些反馈的时间。我们还可能收集了一些外部信息，比如用户的位置（通过他们的IP查到）。这些时间日志通常由一些文字或数值信息组合而成。

绝大部分情况下，这些原始数据都需要经过预处理才能为模型所使用。预处理的情况可能包括以下几种。

- 数据过滤：比如我们想从原始数据的部分数据中创建一个模型，而所需数据只是最近几月的活动数据或是满足特定条件的事件数据。
- 处理数据缺失、不完整或有缺陷：许多现实中的数据集都存在某种程度上的不完整。这可能包括数据缺失（比如用户没有输入），数据存在错误或是缺陷（比如数据收集或存储时的错误，又或是技术问题或漏洞，以及软硬件故障）。可能要过滤掉非规整数据，或通过某种方式来填充缺失的数据点（比如选取数据集的平均值来作为缺失点的值）。
- 处理可能的异常、错误和异常值：错误或异常的数据可能不利于模型的训练，所以需要过滤掉，或是通过某些方法来处理。
- 合并多个数据源：比如可能要将各个用户的事件数据与不同的内部数据或是外部数据合并。内部数据如用户属性；外部数据如地理位置、天气和经济数据。
- 数据汇总：某些模型需要输入的数据进行过某种汇总，比如统计各用户经历过的事件类型的总数目。

对数据进行初步预处理后，需要将其转换为一种适合机器学习模型的表示形式。对许多模型类型来说，这种表示就是包含数值数据的向量或矩阵。数据转换和特征提取时常见的挑战包括以下这些情况。

- 将类别数据（比如地理位置所在的国家或是电影的类别）编码为对应的数值表示。
- 从文本数据提取有用信息。
- 处理图像或是音频数据。
- 数值数据常被转换为类别数据以减少某个变量的可能值的数目。例如将年龄分为几个段（比如25~35、45~55等）。
- 对数值特征进行转换。比如对数值变量应用对数转换，这会有助于处理值域很大的变量。
- 对特征进行正则化、标准化，以保证同一模型的不同输入变量的值域相同。
- 特征工程是对现有变量进行组合或转换以生成新特征的过程。例如从其他数据求平均数，像求某个用户看电影的平均时间。

这些方法都会在本书的例子中讲到。

这些数据清理、探索、聚合和转换步骤，都能通过Spark核心API、SparkSQL引擎和其他外部Scala、Java或Python包做到。借助Spark的Hadoop功能还能实现上述多种存储系统上的读写。

### 2.4.3 模型训练与测试回路

当数据已转换为可用于模型的形式，便可开始模型的训练和测试。在这个部分，我们主要关注模型选择（model selection）问题。这可以归结为对特定任务最优建模方法的选择，或是对特定模型最佳参数的选择问题。在许多情况下，我们会想尝试多种模型并选出表现最好的那个（各模型都采用了最佳的参数时）。因而，这个词在现实中经常同时指代这两个过程。在这个阶段，探索多个模型组合（也称集成学习法，ensemble method）的效果也很常见。

在训练数据集上运行模型并在测试数据集（即为评估模型而预留的数据，在训练阶段模型没接触过该数据）上测试其效果，这个过程一般相对直接，被称作交叉验证（cross-validation）。

然而我们所处理的通常是大型数据集。这样，先在具有代表性的小样本数据集上进行初步的训练-测试回路，或是尽可能并行地选择模型，都会有所帮助。

Spark内置的机器学习库MLlib完全能胜任这个阶段的需求。本书将主要关注如何借助MLlib和Spark核心功能来实现对各种机器学习方法的模型训练、评估以及交叉验证。

### 2.4.4 模型部署与整合

通过训练测试循环找出最佳模型后，要让它能得出可付诸实践的预测，还需将其部署到生产系统中。

这个过程一般要将已训练的模型导入特定的数据存储中。该位置也是生产系统获取新版本的地方。通过这种方式，实时服务系统能在训练新模型时进行周期性的更新。

### 2.4.5 模型监控与反馈

监控机器学习系统在生产环境下的表现十分重要。在部署了最优训练的模型后，我们会想知道其在实际中的表现如何：它在新的未知数据上的表现是否符合预期？其准确度怎么样？毕竟不管之前的模型选择优化做得如何，检验其实际表现的唯一方法是观察其在生产环境下的表现。

同样值得注意的是，模型准确度和预测效果只是现实中系统表现的一部分。通常还应该关注其他业务效果（比如收入和利润率）或用户体验（比如站点使用时间和用户总体活跃度）的相关指标。多数情况下很难将它们与模型预测能力直接关联。推荐系统或目标营销系统的准确度可能很重要，但它只与我们真正关心的那些指标（如用户体验度、活跃度以及最终收入）间接相关。

所以，现实中应该同时监控模型准确度相关指标和业务指标。我们可以尽可能在生产系统中部署不同的模型，通过调整它们而优化业务指标。实践中，这通常通过在线分割测试（live split test）进行。然而，做好这类测试并不容易。在线测试和实验可能引发错误，也可能效果不好，或者会使用基准模型，这些都会给用户体验和收入带来负面影响，故其代价高昂。

本阶段另一个重要的方面是模型反馈（model feedback），指通过用户的行为来对模型的预测进行反馈的过程。在现实系统中，模型的应用将影响用户的决策和潜在行为，从而反过来将从根本上改变模型自己将来的训练数据。

举例来说，假设我们部署了一个推荐系统。由于推荐实际上限制了用户的可选项，从而影响了用户的选择。我们希望用户的选择不会受模型的影响，然而这种反馈回路会反过来影响模型的训练数据，并最终对模型准确度和重要的业务指标产生不利影响。

好在我们可以借助一些机制来降低反馈回路的这种负面影响，比如提供一些无偏见的训练数据。这类数据来自那些没有被推荐的用户，又或者在一开始就考虑到这种平衡需求而划分出来的客户。这些机制有助于对数据的理解、探索以及利用已有的经验来提升系统的表现。

### 2.4.6 批处理或实时方案的选择

前几节简要概括了常见的批处理方法。在这类方法下，模型用所有数据或一部分数据进行周期性的重新训练。由于上述流程会花费一定的时间，这就使得批处理方法难以在新数据到达时立即完成模型的更新。

虽然本书将主要讨论批处理机器学习方法，但的确存在一类名为在线学习（online learning）的机器学习方法。它们在新数据到达时便能立即更新模型，从而使实时系统成为可能。常见的例子有对线性模型的在线优化算法，如随机梯度下降法。我们可以通过例子来学习该算法。这类方法的优势在于其系统将能对新的信息和底层行为（即输入数据的特征或是分布会随时间变化，现实中的绝大部分情况都会如此）作出快速的反应和调整。

但在实际生产环境中，在线学习模型也会面对特有的挑战。比如，对数据的获取和转换难以做到实时。在一个纯在线环境下选择适当的模型也不简单。在线训练和模型选择以及部署阶段的延时可能难以达到实时性的需求（比如在线广告对延时的需求是以毫秒计）。最后，批处理框架不适合对本质为流的数据进行实时处理。

幸运的是，Spark提供了实时流处理组件Spark Streaming，对实时机器学习任务来说是个不错的选择。第10章将探讨Spark Streaming和在线学习问题。

现实中的实时机器学习系统具有天生的复杂性，故实践中大部分的系统都以近实时性为设计目标。这是一种混合方法，它并不要求模型一定在数据到达时立即更新。相反，新的数据会被收集为小批量的训练数据，再输入给在线学习算法。大部分情况下，该方法会周期性地进行某种批处理。处理的内容可能包括在整个数据集上重新计算模型，或是更为复杂的某些数据处理以及模型的选择。这些能保证实时模型的表现不会随时间推移而变差。

另一种类似的方法是，在周期性批处理中进行重新计算时，若有新的数据到来则只对更复杂的模型进行近似更新。这样模型可从新的数据学习，但有短暂延迟。因为是近似更新，所以模型的准确度会随着时间推移而下降。但周期性地在所有数据上重新计算模型能弥补这一点。

## 2.5 机器学习系统架构

# 第3章 Spark上数据的获取、处理与准备

## 3.1 获取公开数据集

商业敏感数据虽然难以获取，但好在仍有相当多有用数据可公开访问。它们中的不少常用来作为特定机器学习问题的基准测试数据。常见的有以下几个。

- UCL机器学习知识库：包括近300个不同大小和类型的数据集，可用于分类、回归、聚类和推荐系统任务。数据集列表位于：http://archive.ics.uci.edu/ml/。 
- Amazon AWS公开数据集：包含的通常是大型数据集，可通过Amazon S3访问。这些数据集包括人类基因组项目、Common Crawl网页语料库、维基百科数据和Google Books Ngrams。相关信息可参见：http://aws.amazon.com/publicdatasets/。 
- Kaggle：这里集合了Kaggle举行的各种机器学习竞赛所用的数据集。它们覆盖分类、回归、排名、推荐系统以及图像分析领域，可从Competitions区域下载：http://www.kaggle.com/competitions。 
- KDnuggets：这里包含一个详细的公开数据集列表，其中一些上面提到过的。该列表位于：http://www.kdnuggets.com/datasets/index.html。

为说明Spark下的数据处理、转换和特征提取相关的概念，需要下载一个电影推荐方面的常用数据集MovieLens。它能应用于推荐系统和其他可能的机器学习任务，适合作为示例数据集。

## 3.2 探索与可视化数据

### 3.2.1 探索用户数据

### 3.2.2 探索电影数据

### 3.2.3 探索评级数据

## 3.3 处理与转换数据

现在我们已对数据集进行过探索性的分析，并了解了用户和电影的一些特征。那接下来做什么呢？

为让原始数据可用于机器学习算法，需要先对其进行清理，并可能需要将其进行各种转换，之后才能从转换后的数据里提取有用的特征。数据的转换和特征提取联系紧密。某些情况下，一些转换本身便是特征提取的过程。

在之前处理电影数据集时我们已经看到数据清理的必要性。一般来说，现实中的数据会存在信息不规整、数据点缺失和异常值问题。理想情况下，我们会修复非规整数据。但很多数据集都源于一些难以重现的收集过程（比如网络活动数据和传感器数据），故实际上会难以修复。值缺失和异常也很常见，且处理方式可与处理非规整信息类似。总的来说，大致的处理方法如下。

- 过滤掉或删除非规整或有值缺失的数据：这通常是必须的，但的确会损失这些数据里那些好的信息。
- 填充非规整或缺失的数据：可以根据其他的数据来填充非规整或缺失的数据。方法包括用零值、全局期望或中值来填充，或是根据相邻或类似的数据点来做插值（通常针对时序数据）等。选择正确的方式并不容易，它会因数据、应用场景和个人经验而不同。
- 对异常值做鲁棒处理：异常值的主要问题在于即使它们是极值也不一定就是错的。到底是对是错通常很难分辨。异常值可被移除或是填充，但的确存在某些统计技术（如鲁棒回归）可用于处理异常值或是极值。
- 对可能的异常值进行转换：另一种处理异常值或极值的方法是进行转换。对那些可能存在异常值或值域覆盖过大的特征，利用如对数或高斯核对其转换。这类转换有助于降低变量存在的值跳跃的影响，并将非线性关系变为线性的。

## 3.4 从数据中提取有用特征

在完成对数据的初步探索、处理和清理后，便可从中提取可供机器学习模型训练用的特征。

特征（feature）指那些用于模型训练的变量。每一行数据包含可供提取到训练样本中的各种信息。从根本上说，几乎所有机器学习模型都是与用向量表示的数值特征打交道；因此，我们需要将原始数据转换为数值。

特征可以概括地分为如下几种。

- 数值特征（numerical feature）：这些特征通常为实数或整数，比如之前例子中提到的年龄。
- 类别特征（categorical feature）：它们的取值只能是可能状态集合中的某一种。我们数据集中的用户性别、职业或电影类别便是这类。
- 文本特征（text feature）：它们派生自数据中的文本内容，比如电影名、描述或是评论。
- 其他特征：大部分其他特征都最终表示为数值。比如图像、视频和音频可被表示为数值数据的集合。地理位置则可由经纬度或地理散列（geohash）表示。

这里我们将谈到数值、类别以及文本类的特征。

### 3.4.1 数值特征

原始的数值和一个数值特征之间的区别是什么？实际上，任何数值数据都能作为输入变量。但是，机器学习模型中所学习的是各个特征所对应的向量的权值。这些权值在特征值到输出或是目标变量（指在监督学习模型中）的映射过程中扮演重要角色。

由此我们会想使用那些合理的特征，让模型能从这些特征学到特征值和目标变量之间的关系。比如年龄就是一个合理的特征。年龄的增加和某项支出之间可能就存在直接关系。类似地，高度也是一个可直接使用的数值特征。

当数值特征仍处于原始形式时，其可用性相对较低，但可以转化为更有用的表示形式。位置信息便是如此。若使用原始位置信息（比如用经纬度表示的），我们的模型可能学习不到该信息和某个输出之间的有用关系，这就使得该信息的可用性不高，除非数据点的确很密集。然而若对位置进行聚合或挑选后（比如聚焦为一个城市或国家），便容易和特定输出之间存在某种关联了。

### 3.4.2 类别特征

当类别特征仍为原始形式时，其取值来自所有可能取值所构成的集合而不是一个数字，故不能作为输入。如之前的例子中的用户职业便是一个类别特征变量，其可能取值有学生、程序员等。

这样的类别特征也称作名义（nominal）变量，即其各个可能取值之间没有顺序关系。相反，那些存在顺序关系的（比如之前提到的评级，从定义上说评级5会高于或是好于评级1）则被称为有序（ordinal）变量。

将类别特征表示为数字形式，常可借助*k*之1（1-of-*k*）方法进行。将名义变量表示为可用于机器学习任务的形式，会需要借助如*k*之1编码这样的方法。有序变量的原始值可能就能直接使用，但也常会经过和名义变量一样的编码处理。

假设变量可取的值有*k*个。如果对这些值用1到*k*编序，则可以用长度为*k*的二元向量来表示一个变量的取值。在这个向量里，该取值对应的序号所在的元素为1，其他元素都为0。

### 3.4.3 派生特征

上面曾提到，从现有的一个或多个变量派生出新的特征常常是有帮助的。理想情况下，派生出的特征能比原始属性带来更多信息。

比如，可以分别计算各用户已有的电影评级的平均数。这将能给模型加入针对不同用户的个性化特征（事实上，这常用于推荐系统）。在前文中我们也从原始的评级数据里创建了新的特征以学习出更好的模型。

从原始数据派生特征的例子包括计算平均值、中位值、方差、和、差、最大值或最小值以及计数。在先前内容中，我们也看到是如何从电影的发行年份和当前年份派生了新的movie age特征的。这类转换背后的想法常常是对数值数据进行某种概括，并期望它能让模型学习更容易。

数值特征到类别特征的转换也很常见，比如划分为区间特征。进行这类转换的变量常见的有年龄、地理位置和时间。

### 3.4.4 文本特征

从某种意义上说，文本特征也是一种类别特征或派生特征。下面以电影的描述（我们的数据集中不含该数据）来举例。即便作为类别数据，其原始的文本也不能直接使用。因为假设每个单词都是一种可能的取值，那单词之间可能出现的组合有几乎无限种。这时模型几乎看不到有相同的特征出现两次，学习的效果也就不理想。从中可以看出，我们会希望将原始的文本转换为一种更便于机器学习的形式。

文本的处理方式有很多种。自然语言处理便是专注于文本内容的处理、表示和建模的一个领域。关于文本处理的完整内容并不在本书的讨论范围内，但我们会介绍一种简单且标准化的文本特征提取方法。该方法被称为词袋（bag-of-word）表示法。

词袋法将一段文本视为由其中的文本或数字组成的集合，其处理过程如下。

- 分词（tokenization）：首先会应用某些分词方法来将文本分隔为一个由词（一般如单词、数字等）组成的集合。可用的方法如空白分隔法。这种方法在空白处对文本分隔并可能还删除其他如标点符号和其他非字母或数字字符。
- 删除停用词（stop words removal）：之后，它通常会删除常见的单词，比如the、and和but（这些词被称作停用词）。
- 提取词干（stemming）：下一步则是词干的提取。这是指将各个词简化为其基本的形式或者干词。常见的例子如复数变为单数（比如dogs变为dog等）。提取的方法有很多种，文本处理算法库中常常会包括多种词干提取方法。
- 向量化（vectorization）：最后一步就是用向量来表示处理好的词。二元向量可能是最为简单的表示方式。它用1和0来分别表示是否存在某个词。从根本上说，这与之前提到的*k* 之1编码相同。与*k*之1相同，它需要一个词的字典来实现词到索引序号的映射。随着遇到的词增多，各种词可能达数百万。由此，使用稀疏矩阵来表示就很关键。这种表示只记录某个词是否出现过，从而节省内存和磁盘空间，以及计算时间。

### 3.4.5 正则化特征

在将特征提取为向量形式后，一种常见的预处理方式是将数值数据正则化（normalization）。其背后的思想是将各个数值特征进行转换，以将它们的值域规范到一个标准区间内。正则化的方法有如下几种。

- 正则化特征：这实际上是对数据集中的单个特征进行转换。比如减去平均值（特征对齐）或是进行标准的正则转换（以使得该特征的平均值和标准差分别为0和1）。
- 正则化特征向量：这通常是对数据中的某一行的所有特征进行转换，以让转换后的特征向量的长度标准化。也就是缩放向量中的各个特征以使得向量的范数为1（常指一阶或二阶范数）。

Spark在其MLlib机器学习库中内置了一些函数用于特征的缩放和标准化。它们包括供标准正态变换的StandardScaler，以及提供与上述相同的特征向量正则化的Normalizer。

### 3.4.6 用软件包提取特征

虽然上面已经提到了不少特征提取的方法，但每次都要为这些常见任务编写代码并不轻松。当然，我们可以为之创建可重用的代码库。但更好的是可以依赖现有的工具和软件包。

Spark支持Scala、Java和Python的绑定。我们可以通过这些语言所开发的软件包，借助其中完善的工具箱来实现特征的处理和提取，以及向量表示。特征提取可借助的软件包有scikit-learn、gensim、scikit-image、matplotlib、Python的NLTK、Java编写的OpenNLP以及用Scala编写的Breeze和Chalk。实际上，Breeze自Spark 1.0开始就成为Spark的一部分了。后几章也会介绍如何使用Breeze的线性代数功能。

# 第4章 构建基于Spark的推荐引擎

前几章介绍了数据处理和特征提取的一些基本概念。从本章开始，我们将从推荐引擎开始，对各机器学习模型进行详细探讨。

推荐引擎或许是最为大众所知的一种机器学习模型。人们或许并不知道它确切是什么，但在使用Amazon、Netflix、YouTube、Twitter、LinkedIn和Facebook这些流行站点的时候，可能已经接触过了。推荐是这些网站背后的核心组件之一，有时还是一个重要的收入来源。

推荐引擎背后的想法是预测人们可能喜好的物品并通过探寻物品之间的联系来辅助这个过程。从这点上来说，它和同样也做预测的搜索引擎互补。但与搜索引擎不同，推荐引擎试图向人们呈现的相关内容并不一定就是人们所搜索的，其返回的某些结果甚至人们都没听说过。

一般来讲，推荐引擎试图对用户与某类物品之间的联系建模。比如，第2章MovieStream的案例中，我们使用推荐引擎来告诉用户有哪些电影他们可能会喜欢。如果这点做得很好，就能吸引用户持续使用我们的服务。这对双方都有好处。同样，如果能准确告诉用户有哪些电影与某一电影相似，就能方便用户在站点上找到更多感兴趣的信息。这也能提升用户的体验、参与度以及站点内容对用户的吸引力。

实际上，推荐引擎的应用并不限于电影、书籍或是产品。本章内容同样适用于用户与物品关系或社交网络中用户与用户之间的关系。比方说向用户推荐他们可能认识或关注的用户。

推荐引擎很适合如下两类常见场景（两者可兼有）。

- 可选项众多：可选的物品越多，用户就越难找到想要的物品。如果用户知道他们想要什么，那搜索能有所帮助。然而最适合的物品往往并不为用户所事先知道。这时，通过向用户推荐相关物品，其中某些可能用户事先不知道，将能帮助他们发现新物品。
- 偏个人喜好：当人们主要根据个人喜好来选择物品时，推荐引擎能利用集体智慧，根据其他有类似喜好用户的信息来帮助他们发现所需物品。

## 4.1 推荐模型的分类

