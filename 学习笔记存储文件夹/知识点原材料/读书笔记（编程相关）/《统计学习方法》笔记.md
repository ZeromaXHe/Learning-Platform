# 第1章 统计学习方法概论

本章简要叙述统计学习方法的一些基本概念。这是对全书内容的概括，也是全书内容的基础，首先叙述统计学习的定义、研究对象与方法；然后叙述监督学习，这是本书的主要内容；接着提出统计学习方法的三要素：模型、策略和算法；介绍模型选择，包括正则化、交叉验证与学习的泛化能力；介绍生成模型与判别模型；最后介绍监督学习方法的应用：分类问题、标注问题与回归问题。

## 1.1 统计学习

### 1. 统计学习的特点

统计学习（statistical learning）是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。统计学习也称为统计机器学习（statistical machine learning）。

统计学习的主要特点是：

1. 统计学习以计算机及网络为平台，是建立在计算机及网络之上的；
2. 统计学习以数据为研究对象，是数据驱动的学科；
3. 统计学习的目的是对数据进行预测与分析
4. 统计学习以方法为中心，统计学习方法构建模型并应用模型进行预测与分析；
5. 统计学习是概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科，并且在发展中逐步形成独自的理论体系与方法论。

赫尔伯特·西蒙（Herbert A. Simon）曾对“学习”给出以下定义：“如果一个系统能够通过执行某个过程改进它的性能，这就是学习。”按照这个观点，统计学习就是计算机系统通过运用数据及统计方法提高系统性能的机器学习，现在，当人们提及机器学习时，往往是指统计机器学习。

### 2. 统计学习的对象

统计学习的对象是数据（data）。它从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，又回到对数据的分析与预测中去。作为统计学习的对象，数据是多样的，包括存在于计算机及网络上的各种数字、文字、图像、视频、音频数据以及它们的组合。

统计学习关于数据的基本假设是同类数据具有一定的统计规律性，这是统计学习的前提。这里的同类数据是指具有某种共同性质的数据，例如英文文章、互联网网页、数据库中的数据等。由于它们具有统计规律性，所以可以用概率统计方法来加以处理。比如，可以用随机变量描述数据中的特征，用概率分布描述数据的统计规律。

在统计学习过程中，以变量或变量组表示数据。数据分为由连续变量和离散变量表示的类型。本书以讨论离散变量的方法为主。另外，本书只涉及利用数据构建模型及利用模型对数据进行分析与预测，对数据的观测和收集等问题不作讨论。

### 3. 统计学习的目的

统计学习用于对数据进行预测与分析，特别是对未知新数据进行预测与分析，对数据的预测可以使计算机更加智能化，或者说使计算机的某些性能得到提高；对数据的分析可以让人们获取新的知识，给人们带来新的发现。

对数据的预测与分析是通过构建概率统计模型实现的，统计学习总的目标就是考虑学习什么样的模型和如何学习模型，以使模型能对数据进行准确的预测与分析，同时也要考虑尽可能地提高学习效率。

### 4. 统计学习的方法

统计学习的方法是基于数据构建模型从而对数据进行预测与分析。统计学习由监督学习（supervised learning）、非监督学习（unsupervised learning）、半监督学习（semi-supervised learning）和强化学习（reinforcement learning）等组成。

本书主要讨论监督学习，这种情况下统计学习的方法可以概况如下：从给定的、有限的、用于学习的训练数据（training data）集合出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，称为假设空间（hypothesis space）；应用某个评价准则（evaluation criterion），从假设空间中选取一个最优的模型，使它对已知训练数据及未知测试数据（test data）在给定的评价准则下有最优的预测；最优模型的选取由算法实现。这样，统计学习方法包括模型的假设空间、模型选择的准则以及模型学习的算法，称其为统计学习方法的三要素，简称为模型（model）、策略（strategy）和算法（algorithm）。

实现统计学习方法的步骤如下：

1. 得到一个有限的训练数据集合；
2. 确定包含所有可能的模型的假设空间，即学习模型的集合；
3. 确定模型选择的准则，即学习的策略；
4. 实现求解最优模型的算法，即学习的算法；
5. 通过学习方法选择最优模型；
6. 利用学习的最优模型对新数据进行预测或分析。

本书以介绍统计学习方法为主，特别是监督学习方法，主要包括用于分类、标注与回归问题的方法。这些方法在自然语言处理、信息检索、文本数据挖掘等领域中有着极其广泛的应用。

### 5. 统计学习的研究

统计学习研究一般包括统计学习方法（statistical learning method）、统计学习理论（statistical learning theory）及统计学习应用（application of statistical learning）三个方面。统计学习方法的研究旨在开发新的学习方法；统计学习理论的研究在于探求统计学习方法的有效性与效率，以及统计学习的基本理论问题；统计学习应用的研究主要考虑将统计学习方法应用到实际问题中去，解决实际问题。

### 6. 统计学习的重要性

## 1.2 监督学习

统计学习包括监督学习、非监督学习、半监督学习及强化学习。本书主要讨论监督学习问题。

监督学习（supervised learning）的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出一个好的预测（注意，这里的输入、输出是指某个系统的输入与输出，与学习的输入与输出不同），计算机的基本操作就是给定一个输入产生一个输出，所以监督学习是及其重要的统计学习分支，也是统计学习中内容最丰富、应用最广泛的部分。

### 1.2.1 基本概念

#### 1. 输入空间、特征空间与输出空间

在监督学习中，将输入与输出所有可能取值的集合分别称为输入空间（input space）与输出空间（output space）。输入与输出空间可以是有限元素的集合，也可以是整个欧氏空间。输入空间与输出空间可以是同一个空间，也可以是不同的空间；但通常输出空间远远小于输入空间。

每个具体的输入是一个实例（instance），通常由特征向量（feature vector）表示。这时，所有特征向量存在的空间称为特征空间（feature space）。特征空间的每一维对应于一个特征。有时假设输入空间与特征空间为相同的空间，对它们不予区分；有时假设输入空间与特征空间为不同的空间，将实例从输入空间映射到特征空间。模型实际上都是定义在特征空间上的。

在监督学习过程中，将输入与输出看作是定义在输入（特征）空间与输出空间上的随机变量的取值。输入、输出变量用大写字母表示，习惯上输入变量写作X，输出变量写作Y。输入、输出变量所取的值用小写字母表示，输入变量的取值写作x，输出变量的取值写作y。变量可以是标量或向量，都用相同类型字母表示，除特别声明外，本书中向量均为列向量，输入实例x的特征向量记作
$$
x=(x^{(1)},x^{(2)},\cdots,x^{(i)},\cdots,x^{(n)})^T\\
x^{(i)}表示x的第i个特征。注意，x^{(i)}与x_i不同，本书通常用x_i表示多个输入变量中的第i个，即\\
x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T
$$
监督学习从训练数据（training data）集合中学习模型，对测试数据（test data）进行预测。训练数据由输入（或特征向量）与输出对组成，训练集通常表示为
$$
T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}
$$
测试数据也由相应的输入与输出对组成。输入与输出对又称为样本（sample）或样本点。

输入变量X和输出变量Y有不同的类型，可以是连续的，也可以是离散的。人们根据输入、输出变量的不同类型，对预测任务给予不同的名称：输入变量与输出变量均为连续变量的预测问题称为回归问题；输出变量为有限个离散变量的预测问题称为分类问题；输入变量与输出变量均为变量序列的预测问题称为标注问题。

#### 2. 联合概率分布

监督学习假设输入与输出的随机变量X和Y遵循联合概率分布P(X,Y)。P(X,Y)表示分布函数，或分布密度函数。注意，在学习过程中，假定这一联合概率分布存在，但对学习过程来说，联合概率分布的具体定义是未知的。训练数据与测试数据被看作是依联合概率分布P(X,Y)独立同分布产生的。统计学习假设数据存在一定的统计规律，X和Y具有联合概率分布的假设就是监督学习关于数据的基本假设。

#### 3. 假设空间

监督学习的目的在于学习一个由输入到输出的映射，这一映射由模型来表示。换句话说，学习的目的就在于找到最好的这样的模型。模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间（hypothesis space）。假设空间的确定意味着学习范围的确定。

监督学习的模型可以是概率模型或非概率模型，由条件概率分布P(Y|X)或决策函数（decision function）Y=f(X)表示，随具体学习方法而定，对具体的输入进行相应的输出预测时，写作P(y|x)或y=f(x)。

### 1.2.2 问题的形式化

## 1.3 统计学习三要素

统计学习方法都是由模型、策略和算法构成的，即统计学习方法由三要素构成，可以简单地表示为：方法=模型+策略+算法

下面论述监督学习中的统计学习三要素。非监督学习、强化学习也同样拥有这三要素。可以说构建一种统计学习方法就是确定具体的统计学习三要素。

### 1.3.1 模型

统计学习首要考虑的问题是学习什么样的模型。在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。

### 1.3.2 策略

有了模型的假设空间，统计学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。统计学习的目标在于从假设空间中选取最优模型。

首先引入损失函数与风险函数的概念。损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。

#### 1. 损失函数和风险函数

监督学习问题是在假设空间 F 中选取模型 f 作为决策函数，对于给定的输入 X，由 f(X) 给出相应的输出 Y，这个输出的预测值 f(X) 与真实值 Y 可能一致也可能不一致，用一个损失函数（loss function）或代价函数（cost function）来度量预测错误的程度。损失函数是 f(X) 和 Y 的非负实值函数，记作 L(Y, f(X))。

统计学习常用的损失函数有以下几种：

（1）0-1损失函数（0-1 loss function）
$$
L(Y, f(X))=
	\begin{cases}
		1, & Y\neq f(X)\\
		0, & Y=f(X)
	\end{cases}
$$
（2）平方损失函数（quadratic loss function）
$$
L(Y,f(X))=(Y-f(X))^2
$$
（3）绝对损失函数（absolute loss function）
$$
L(Y,f(X))=|Y-f(X)|
$$
（4）对数损失函数（logarithmic loss function）或对数似然损失函数（log-likelihood loss function）
$$
L(Y,P(Y|X))=-logP(Y|X)
$$
损失函数越小，模型就越好，由于模型的输入、输出（X，Y）是随机变量，遵循联合分布P(X, Y)，所以损失函数的期望是
$$
R_{exp}(f)=E_p [L(Y,f(X))]=\int_{x\times y}L(y,f(x))P(x,y)dxdy
$$
这是理论上模型 f(X) 关于联合分布 P(X, Y) 的平均意义下的损失，称为风险函数（risk function）或期望损失（expected loss）。

学习的目标就是选择期望风险最小的模型。由于联合分布 P(X, Y)是未知的，$R_{exp}(f)$ 不能直接计算。实际上，如果知道联合分布 P(X, Y)，可以从联合分布直接求出条件概率分布 P(Y|X)，也就不需要学习了。正因为不知道联合概率分布，所以才需要进行学习。这样一来，一方面根据期望风险最小学习模型要用到联合分布，另一方面联合分布又是未知的，所以监督学习就成为了一个病态问题（ill-formed problem）。

给定一个训练数据集
$$
T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N\}
$$
模型 f(X) 关于训练数据集的平均损失称为经验风险（empirical risk）或经验损失（empirical loss），记作$R_{emp}$:（1.10）
$$
R_{emp}(f)=\frac{1}{N}\sum^N_{i=1}L(y_i,f(x_i))
$$
期望风险$R_{exp}(f)$是模型关于联合分布的期望损失，经验风险$R_{emp}(f)$是模型关于训练样本集的平均损失。根据大数定律，当样本容量N趋于无穷时，经验风险$R_{emp}(f)$趋于期望风险$R_{exp}(f)$。所以一个很自然的想法是用经验风险估计期望风险。但是，由于现实中训练样本数目有限，甚至很小，所以用经验风险估计期望风险常常并不理想，要对经验风险进行一定的矫正。这就关系到监督学习的两个基本策略：经验风险最小化和结构风险最小化。

#### 2. 经验风险最小化与结构风险最小化

在假设空间、损失函数以及训练数据集确定的情况下，经验风险函数式（1.10）就可以确定。经验风险最小化（empirical risk minimization，ERM）的策略认为，经验风险最小的模型是最优的模型。根据这一策略，按照经验风险最小化求最优模型就是求解最优化问题：
$$
\min_{f\in F}\frac{1}{N}\sum^N_{i=1}L(y_i,f(x_i))\\
其中，F是假设空间
$$
当样本容量足够大时，经验风险最小化能保证有很好的学习效果，在现实中被广泛采用。比如，极大似然估计（maximum likelihood estimation）就是经验风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。

但是，当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生后面将要叙述的“过拟合(over-fitting)”现象。

结构风险最小化（structural risk minimization，SRM）是为了防止过拟合而提出来的策略。结构风险最小化等价于正则化（regularization）。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term）。在假设空间、损失函数以及训练数据集确定的情况下，结构风险的定义是
$$
R_{srm}(f)=\frac{1}{N}\sum^N_{i=1}L(y_i,f(x_i))+\lambda J(f)\\
其中J(f)为模型的复杂度，是定义在假设空间F上的泛函。
$$
模型 f 越复杂，复杂度 J(f) 就越大；反之，模型 f 越简单，复杂度 J(f) 就越小。也就是说，复杂度表示了对复杂模型的惩罚。$\lambda \geq 0$是 

# 第6章 逻辑斯谛回归与最大熵模型

逻辑斯谛回归（logistic regression）是统计学习中的经典分类方法。最大熵是概率模型学习的一个准则，将其推广到分类问题得到最大熵模型（maximum entropy model）。逻辑斯谛回归模型与最大熵模型都属于对数线性模型。本章首先介绍逻辑斯谛回归模型，然后介绍最大熵模型，最后讲述逻辑斯谛回归与最大熵模型的学习算法，包括改进的迭代尺度算法和拟牛顿法。

## 6.1 逻辑斯谛回归模型

### 6.1.1 逻辑斯谛分布

首先介绍逻辑斯谛分布（logistic distribution）。

**定义6.1（逻辑斯谛分布）** 设X是连续随机变量，X服从逻辑斯谛分布是指X具有下列分布函数和密度函数：
$$
\begin{array}{fx}
F(x)=P(X \leq x)=\frac{1}{1+e^{-(x-\mu)/\gamma}} &(6.1) \\
f(x)=F'(x)=\frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})^2} & (6.2) \\
式中，\mu为位置参数，\gamma > 0为形状参数。
\end{array}
$$
![图6.1](统计学习方法_配图\图6_1逻辑斯谛分布得密度函数与分布函数.png)

逻辑斯谛分布的密度函数f(x)和分布函数F(x)的图形如图6.1所示，分布函数属于逻辑斯谛函数，其图形是一条S型曲线（sigmoid curve）。该曲线以点$(\mu,\frac{1}{2})$为中心对称，即满足
$$
F(-x+\mu)-\frac{1}{2}=-F(x-\mu)+\frac{1}{2}
$$
曲线在中心附近增长速度较快，在两端增长速度较慢。形状参数$\gamma$的值越小，曲线在中心附近增长得越快。

### 6.1.2 二项逻辑斯谛回归模型

二项逻辑斯谛回归模型（binomial logistic regression model）是一种分类模型，由条件概率分布P(Y|X)表示，形式为参数化的逻辑斯谛分布。这里，随机变量X取值为实数，随机变量Y取值为1或0。我们通过监督学习的方法来估计模型参数。

**定义6.2（逻辑斯谛回归模型）** 二项逻辑斯谛回归模型是如下的条件概率分布：
$$
\begin{array}{Pyx}
P(Y=1|x)=\frac{e^{w\cdot x+b}}{1+e^{w\cdot x+b}} & (6.3)\\
P(Y=0|x)=\frac{1}{1+e^{w\cdot x+b}} & (6.4)\\
这里，x\in R^n 是输入，Y\in \text{{0,1}} 是输出\\
w\in R^n和b\in R是参数，w称为权值向量，b称为偏置，w\cdot x为w和x的内积
\end{array}
$$
对于给定的输入实例x，按照式（6.3）和式（6.4）可以求得P(Y=1|x)和P(Y=0|x)。逻辑斯谛回归比较两个条件概率值的大小，将实例x分到概率值较大的那一类。

有时为了方便，将权值向量和输入向量加以扩充，仍记作w,x，即$w=(w^{(1)},w^{(2)},\cdots,w^{(n)},b)^T, x=(x^{(1)},x^{(2)},\cdots,x^{(n)},1)^T$。这时，逻辑斯谛回归模型如下：
$$
\begin{array}{Pyx}
P(Y=1|x)=\frac{e^{w\cdot x}}{1+e^{w\cdot x}} & (6.5)\\
P(Y=0|x)=\frac{1}{1+e^{w\cdot x}} & (6.6)
\end{array}
$$
现在考查逻辑斯谛回归模型的特点。一个事件的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是p，那么该事件的几率是$\frac{p}{1-p}$，该事件的对数几率（log odds）或logit函数是
$$
logit(p)=\log\frac{p}{1-p}
$$
对逻辑斯谛回归而言，由式(6.5)与式(6.6)得
$$
\begin{alignat}{2} 
\log\frac{P(Y=1|x)}{1-P(Y=1|x)} & =\log\frac{1}{\frac{1}{P(Y=1|x)}-1}\\
& =\log\frac{1}{\frac{1+e^{w\cdot x}}{e^{w\cdot x}}-1}\\
& =w\cdot x
\end{alignat}
$$
这就是说，在逻辑斯谛回归模型中，输出Y=1的对数几率是输入x的线性函数。或者说，输出Y=1的对数几率是由输入x的线性函数表示的模型，即逻辑斯谛回归模型。

换一个角度看，考虑对输入x进行分类的线性函数$w\cdot x$，其值域为实数域。注意，这里$x\in R^{n+1},w\in R^{n+1}$。通过逻辑斯谛回归模型定义式(6.5)可以将线性函数$w\cdot x$转换为概率：
$$
P(Y=1|x)=\frac{e^{w\cdot x}}{1+e^{w\cdot x}}
$$
这时，线性函数的值越接近正无穷，概率值就越接近1；线性函数的值越接近负无穷，概率值就越接近0（如图6.1所示）。这样的模型就是逻辑斯谛回归模型。

### 6.1.3 模型参数估计

逻辑斯谛回归模型学习时，对于给定的训练数据集$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$，其中，$x_i\in R^n, y_i\in \{0,1\}$，可以应用极大似然估计法估计模型参数，从而得到逻辑斯谛回归模型。

设：
$$
P(Y=1|x)=\pi(x), \\
P(Y=0|x)=1-\pi(x)
$$
似然函数为：
$$
\prod^N_{i=1}[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}
$$
对数似然函数为：
$$
\begin{alignat}{2} 
L(w) & = \sum^N_{i=1}[y_i \log\pi(x_i)+(1-y_i)\log(1-\pi(x_i))] \\
& =  \sum^N_{i=1}[y_i\log\frac{\pi(x_i)}{1-\pi(x_i)}+\log(1-\pi(x_i))] \\
& =  \sum^N_{i=1}[y_i(w\cdot x_i) - log(1+e^{w\cdot x_i})]
\end{alignat}
$$
这样，问题就变成了以对数似然函数为目标函数的最优化问题。逻辑斯谛回归学习中通常采用的方法是梯度下降法及拟牛顿法。

假设w的极大似然估计值是$\hat w$，那么学到的逻辑斯谛回归模型为
$$
P(Y=1|x)=\frac{e^{\hat w\cdot x}}{1+e^{\hat w\cdot x}} \\
P(Y=0|x)=\frac{1}{1+e^{\hat w\cdot x}}
$$

### 6.1.4 多项逻辑斯谛回归

上面介绍的逻辑斯谛回归模型是二项分类模型，用于二类分类，可以将其推广为多项逻辑斯谛回归模型（multi-nominal logistic regression model），用于多类分类。假设离散型随机变量Y的取值集合是$\{1,2,\cdots,K\}$，那么多项逻辑斯谛回归模型是
$$
\begin{array}{multilog}
P(Y=k|x)=\frac{e^{w_k\cdot x}}{1+\sum^{K-1}_{k=1}e^{w_k\cdot x}} & , k=1,2,\cdots,K-1 & (6.7)\\
P(Y=K|x)=\frac{1}{1+\sum^{K-1}_{k=1}e^{w_k\cdot x}} & & (6.8) \\
这里，x\in R^{n+1},w_k\in R^{n+1}
\end{array}
$$
二项逻辑斯谛回归的参数估计法也可以推广到多项逻辑斯谛回归。

