# 推荐系统

在互联网永不停歇的增长需求的驱动下，推荐系统的发展可谓一日千里，从2010年之前千篇一律的**协同过滤**（Collaborative Filtering，CF）、**逻辑回归**（Logistic Regression，LR），进化到**因子分解机**（Factorization Machine, FM）、**梯度提升树**（Gradient Boosting Decision Tree, GBDT），再到2015年之后**深度学习推荐模型**的百花齐放，各种模型架构层出不穷。

推荐模型的主流模型经历了**从单一模型到组合模型，从经典框架到深度学习**的发展过程。

# 一、传统推荐系统模型算法

**重要原因**：

诚然，深度学习推荐模型已经成了推荐、广告、搜索领域的主流，但前深度学习时代的推荐模型仍是十分重要的，原因如下：

1. 即使在深度学习空前流行的今天，协同过滤、逻辑回归、因子分解机等传统推荐模型仍然凭借其可解释性强、硬件环境要求低、易于快速训练和部署等不可替代的优势，拥有大量适用的应用场景。
2. 传统推荐模型是深度学习推荐模型的基础。构成深度神经网络（Deep Neural Network，DNN）的基本单元是神经元，而应用广泛的传统逻辑回归模型正式神经元的另一种表现形式；深度学习推荐模型中影响力很大的**基于因子分解机支持的神经网络**（Factorization machine supported Neural Network，FNN）、**深度因子分解机**（Deep Factorization Machine， DeepFM）、**神经网络因子分解机**（Neural Factorization Machine, NFM）等深度学习模型更是与传统的FM模型有着千丝万缕的联系。此外，在传统推荐模型训练中广泛采用的**梯度下降**等训练方式，更是沿用至深度学习时代。所以说，传统推荐模型是深度学习推荐模型的基础，也是学习的入口。

**发展脉络：**

![推荐系统_传统推荐模型的演化关系图](推荐系统 配图\推荐系统_传统推荐模型的演化关系图.png)

简要来说，传统推荐模型的发展脉络主要由以下几个部分组成：

1. **协同过滤算法族**（UserCF、ItemCF、MF）。
   经典的协同过滤算法曾是推荐系统的首选模型，从物品相似度和用户相似度角度出发，协同过滤衍生出物品协同过滤（ItemCF）和用户协同过滤（UserCF）两种算法。为了使协同过滤能够更好地处理稀疏共现矩阵问题、增强模型的泛化能力，从协同过滤衍生出矩阵分解模型（Matrix Factorization，MF），并发展出矩阵分解的各分支模型。
2. **逻辑回归模型族**。
   与协同过滤仅仅利用用户和物品之间的显式或隐式反馈信息相比，逻辑回归能够利用和融合更多用户、物品以及上下文特征。从LR模型衍生出的模型同样“枝繁叶茂”，包括增强了非线性能力的大规模分片线性模型（Large Scale Piece-wise Linear Model，LS-PLM），由逻辑回归发展出来的FM模型，以及与多种不同模型配合使用后的组合模型，等等。
3. **因子分解机模型族**。
   因子分解机在传统逻辑回归的基础上，加入了二阶部分，使模型具备了进行特征组合的能力。更进一步，在因子分解机基础上发展出来的域感知因子分解机（Field-aware Factorization Machine，FFM）则通过加入特征域的概念，进一步加强了因子分解机特征交叉的能力。
4. **组合模型**。
   为了融合多个模型的优点，将不同模型组合使用是构建推荐模型的常用方法。Facebook提出GBDT+LR（梯度提升决策树（Gradient Boosting Decision Tree）+逻辑回归）组合模型是在业界影响力较大的组合方式。

## 1.1 用户协同过滤（UserCF）

协同过滤，就是协同大家的反馈、评价和意见对海量的信息进行过滤，从中筛选出目标用户可能感兴趣的信息的推荐过程。

### 1.1.1 大致步骤

1. 利用用户对商品的历史评价数据。用户、商品、评价记录构成了带有标识的有向图
2. 为了便于计算，将有向图转换为矩阵的形式（被称为“共现矩阵”），用户作为矩阵行坐标，商品作为列坐标，将例如“点赞”和“踩”的用户行为数据转换为矩阵中的相应元素值。
3. 先是找到与用户X兴趣最相似的n（Top n用户）个用户，然后综合相似用户对商品Y的评价，得出用户X对商品Y评价的预测。

### 1.1.2 用户相似度的计算

1. **余弦相似度**
   余弦相似度（Cosine Similarity）衡量了用户向量 $\vec i$ 和用户向量$\vec j$ 之间的夹角大小。显然，夹角越小，证明余弦相似度越大，两个用户越相似。
   $$
   sim(i,j)=cos(i,j)=\frac{i \cdot j}{||i||\cdot||j||}
   $$

2. **皮尔逊相关系数**
   相比余弦相似度，皮尔逊相关系数通过使用用户平均分对各独立评分进行修正，减少了用户评分偏置的影响。
   $$
   sim(i,j)=\frac{\sum_{p\in P}(R_{i,p}-\overline{R_i})(R_{j,p}-\overline{R_j})}{\sqrt{\sum_{p\in P}(R_{i,p}-\overline{R_i})^2} \sqrt{\sum_{p\in P}(R_{j,p}-\overline{R_j})^2}}
   \\其中，R_{i,p}代表用户 i 对用户p的评分。
   \\\overline{R_i}代表用户 i 对所有物品的平均评分，
   \\P代表所有物品的集合。
   $$

3. 基于皮尔逊系数的思路，还可以通过引入物品平均分的方式，减少物品评分偏置对结果的影响
   $$
   sim(i,j)=\frac{\sum_{p\in P}(R_{i,p}-\overline{R_p})(R_{j,p}-\overline{R_p})}{\sqrt{\sum_{p\in P}(R_{i,p}-\overline{R_p})^2} \sqrt{\sum_{p\in P}(R_{j,p}-\overline{R_p})^2}}
   \\其中，\overline{R_p} 代表物品p得到的所有评分的平均分。
   $$

在传统协同过滤改进过程中，人们也是通过对相似度定义的改进来解决传统协同过滤算法存在的一些缺陷的。

### 1.1.3 最终结果的排序

获得Top n相似用户之后，利用Top n用户生成最终推荐结果的过程如下。假设“目标用户与其相似用户的喜好是相似的”，可根据相似用户的已有评价对目标用户的偏好进行预测。

这里最常用的方式是利用用户相似度和相似用户的评价的加权平均获得目标用户的评测预测。
$$
R_{u,p}=\frac{\sum_{s\in S}(w_{u,s}\cdot R_{s,p})}{\sum_{s\in S}w_{u,s}}
\\其中，权重w_{u,s}是用户u和用户s的相似度，
\\R_{s,p}是用户s对物品p的评分。
$$
在获得用户u对不同物品的评价预测后，最终的推荐列表根据预测得分进行排序即可得到。至此，完成协同过滤的全部推荐过程。以上介绍的协同过滤算法基于用户相似度进行推荐，因此也被称为基于用户的协同过滤（UserCF）。

### 1.1.4 缺点

1. 互联网应用的场景下，用户数往往远大于物品数，而UserCF需要维护用户相似度矩阵以便快速找出Top n相似用户。该用户相似度矩阵的存储开销非常大，且随着用户数的增长，存储空间以$n^2$的速度快速增长
2. 用户的历史数据向量往往非常稀疏，对于只有几次购买或者点击行为的用户来说，找到相似用户的准确度是非常低的。

## 1.2 物品协同过滤（ItemCF）

具体的讲，ItemCF是基于物品相似度进行推荐的协同过滤算法。通过计算共现矩阵中物品列向量的相似度得到物品之间的相似矩阵，再找到用户历史正反馈物品的相似物品进行进一步排序和推荐。

### 1.1.1 具体步骤

1. 基于历史数据，构建以用户（假设用户总数为m）为行坐标，物品（物品总数为n）为列坐标的 $m\times n$ 维共现矩阵。
2. 计算共现矩阵两两列向量间的相似性（相似度计算方式与用户相似度的计算方式相同），构建 $n\times n$ 维的物品相似度矩阵。
3. 获得用户历史行为数据中的正反馈物品列表。
4. 利用物品相似度矩阵，针对目标用户历史行为中的正反馈物品，找出相似的Top k个物品，组成相似物品集合。
5. 对相似物品集合中的物品，利用相似度分值进行排序，生成最终的推荐列表。

第五步中，如果一个物品与多个用户行为历史中的正反馈物品相似，那么该物品最终的相似度应该是多个相似度的累加
$$
R_{u,p}=\sum_{h \in H}(w_{p,h}\cdot R_{u,h})
\\其中，H是目标用户的正反馈物品集合，
\\w_{p,h}是物品p与物品h的物品相似度，
\\R_{u,h}是用户u对物品h的已有评分。
$$

### 1.2.2 协同过滤的缺点

协同过滤并不具备较强的泛化能力，换句话说，协同过滤无法将两个物品相似这个信息推广到其他物品的相似性计算上。这就导致了一个比较严重的问题——热门的物品具有很强的头部效应，容易跟大量物品产生相似性；而尾部的物品由于特征向量稀疏，很少与其他物品产生相似性，导致很少被推荐。

> **哈利波特问题**
>
> 亚马逊网的研究人员在涉及ItemCF算法之初发现ItemCF算法计算出的图书相关表存在一个问题，就是很多书都和《哈利波特》相关，主要是因为《哈利波特》太热门了。

这一现象揭示了协同过滤的天然缺陷——**推荐结果的头部效应比较明显，处理稀疏向量的能力弱**。

**解决问题**

为解决上述问题，同时增加模型的泛化能力，**矩阵分解技术**被提出。该方法在协同过滤的共现矩阵的基础上，<u>使用更稠密的隐向量标识用户和物品</u>，挖掘用户和物品的隐含兴趣和隐含特征。

另外，<u>协同过滤仅仅利用用户和物品的交互信息</u>，无法有效地引入用户年龄、性别、商品描述、商品分类、当前时间等一系列用户特征、物品特征和上下文特征，<u>这无疑造成了有效信息地遗漏</u>。为了在推荐模型中引入这些特征，推荐系统逐渐发展到以**逻辑回归模型**为核心的、能够综合不同类型特征的**机器学习模型**的道路上。

## 1.3 矩阵分解算法

2006年，Netflix举办的著名推荐算法竞赛Netflix Prize Challenge中，以矩阵分解为主的推荐算法大放异彩，拉开了矩阵分解在业界流行的序幕。本节借用Netflix的场景例子说明矩阵分解算法的原理。

### 1.3.1 原理

矩阵分解算法期望为每一个用户和视频生成一个隐向量，将用户和视频定位到隐向量的表示空间上，距离相近的用户和视频表明兴趣特点相近，在推荐过程中，就应该把距离相近的视频推荐给目标用户。

在“矩阵分解”的算法框架下，**用户和物品的隐向量是通过分解协同过滤生成的共现矩阵得到的**。这也是“矩阵分解”名字的由来。

矩阵分解算法将 $m \times n$ 维的共现矩阵R分解为 $m\times k$ 维的用户矩阵U和 $k\times n$ 维的物品矩阵V相乘的形式。其中m是用户数量，n是物品数量，k是隐向量的维度。k的大小决定了隐向量表达能力的强弱。k的取值越小，隐向量包含的信息越少，模型的泛化程度越高；反之，k的取值越大，隐向量的表达能力越强，但泛化程度相应降低。此外，k的取值还与矩阵分解的求解复杂度直接相关。在具体应用中，k的取值要经过多次试验找到一个推荐效果和工程开销的平衡点。

基于用户矩阵U和物品矩阵V，用户u对物品i的预估评分:
$$
\hat r_{ui}=q^T_i p_u
\\其中p_u是用户u在用户矩阵U中的对应行向量，
\\q_i是物品i在物品矩阵V中的对应列向量
$$

### 1.3.2 矩阵分解的求解过程

对矩阵进行矩阵分解的主要方法有三种：**特征值分解**（Eiden Decomposition）、**奇异值分解**（Singular Value Decomposition，SVD）和**梯度下降**（Gradient Descent）。其中，特征值分解只能作用于方阵，显然不适用于分解用户-物品矩阵。



**奇异值分解**的具体描述如下：

假设矩阵$\boldsymbol M$是一个$m\times n$的矩阵，则一定存在一个分解$\boldsymbol M = \boldsymbol{U \Sigma V^T}$，其中$\boldsymbol U$是$m\times m$的正交矩阵，$\boldsymbol V$是$n \times n$的正交矩阵，$\boldsymbol \Sigma$是$m\times n$的对角阵。

取对角阵$\boldsymbol \Sigma$中较大的k个元素作为隐含特征，删除$\boldsymbol \Sigma$的其他维度及$\boldsymbol U$和$\boldsymbol V$中的对应维度，矩阵$\boldsymbol M$被分解为$\boldsymbol M \approx \boldsymbol U_{m\times k}\boldsymbol \Sigma_{k\times k} V_{k\times n}^T$，至此完成了隐向量维度为k的矩阵分解。

可以说，奇异值分解似乎完美地解决了矩阵分解的问题，但其存在两点缺陷，使其不宜作为互联网场景下矩阵分解的主要方法。

1. 奇异值分解要求原始的共现矩阵是稠密的。互联网场景下大部分用户的行为历史非常少，用户-物品的共现矩阵非常稀疏，这与奇异值分解的应用条件相悖。如果应用奇异值分解，就必须对缺失的元素值进行填充。
2. 传统奇异值分解的计算复杂度达到了$O(mn^2)$的级别，这对于商品数量动辄上百万、用户数量往往上千万的互联网场景来说几乎是不可接受的。



由于上述两个原因，传统奇异值分解也不适用于解决大规模稀疏矩阵的矩阵分解问题。因此，**梯度下降法**成了进行矩阵分解的主要方法，这里对其进行具体介绍。



### 1.3.3 消除用户和物品打分的偏差

### 1.3.4 矩阵分解的优点和局限性

**优点**

相比协同过滤，矩阵分解有如下非常明显的优点。

1. **泛化能力强**。在一定程度上解决了数据稀疏问题。
2. **空间复杂度低**。不需再存储协同过滤服务阶段所需的“庞大”的用户相似性或物品相似性矩阵，只需存储用户和物品隐向量。空间复杂度由$n^2$级别降低到$(n+m)\cdot k$级别。
3. **更好的扩展性和灵活性**。矩阵分解的最终产出是用户和物品隐向量，这其实与深度学习中Embedding思想不谋而合，因此矩阵分解的结果也非常便于与其他特征进行组合和拼接，并便于与深度学习网络进行无缝结合。



**局限性**

与此同时，也要意识到矩阵分解的局限性。与协同过滤一样，矩阵分解同样不方便加入用户、物品和上下文相关的特征，这使得矩阵分解丧失了利用很多有效信息的机会，同时在缺乏用户历史行为时，无法进行有效的推荐。为了解决这个问题，逻辑回归模型及其后续发展出的因子分解机等模型，凭借其天然的融合不同特征的能力，逐渐在推荐系统领域得到更广泛的应用。

## 1.4 逻辑回归

相比协同过滤模型仅利用用户和物品的相互行为信息进行推荐，逻辑回归模型能够综合利用用户、物品、上下文等多种不同特征，生成较为“全面”的推荐结果。另外，逻辑回归的另一种表现形式“感知机”作为神经网络中最基础的单一神经元，是深度学习的基础性结构。因此，能够进行多特征融合的逻辑回归模型成了独立于协同过滤的推荐模型发展的另一个主要方向。

相比于协同过滤和矩阵分解利用用户和物品的“相似度”进行推荐，逻辑回归将推荐问题看成一个分类问题，通过预测正样本的概率对物品进行排序。这里的正样本可以是用户“点击”了某商品，也可以是用户“观看”了某视频，均是推荐系统希望用户产生的“正反馈”行为。因此，逻辑回归模型将推荐问题转换成了一个点击率（Click Through Rate，CTR）预估问题。

### 1.4.1 基于逻辑回归模型的推荐流程

基于逻辑回归的推荐过程如下：

1. 将用户年龄、性别、物品属性、物品描述、当前时间、当前地点等特征转换成数值型特征向量。
2. 确定逻辑回归模型的优化目标（以优化“点击率”为例），利用已有样本数据对逻辑回归模型进行训练，确定逻辑回归模型的内部参数。
3. 在模型服务阶段，将特征向量输入逻辑回归模型，经过逻辑回归模型的推断，得到用户“点击”（这里用点击作为推荐系统正反馈行为的例子）物品的概率。
4. 利用“点击”概率对所有候选物品进行排序，得到推荐列表。

基于逻辑回归的推荐过程的重点在于，利用样本的特征向量进行模型训练和在线推断。

### 1.4.2 逻辑回归模型的数学形式

![推荐模型_逻辑回归模型的数学形式的推断过程](推荐系统 配图\推荐模型_逻辑回归模型的数学形式的推断过程.png)

逻辑回归模型的推断过程可以分为如下几步:

1. 将特征向量 $\boldsymbol x=(x_1,x_2,\cdots,x_n)^T$ 作为模型的输入。
2. 通过为各特征赋予相应的权重 $(w_1,w_2,\cdots,w_{n+1})$ 来表示各特征的重要性差异，将各特征进行加权求和，得到 $\boldsymbol x^T \boldsymbol w$。
3. 将 $\boldsymbol x^T \boldsymbol w$ 输入sigmoid函数，使之映射到0~1的区间，得到最终的“点击率”。

> 其中sigmoid函数的具体形式：
> $$
> f(z)=\frac{1}{1+e^{-z}}
> $$
> ![sigmoid函数图像](推荐系统 配图\sigmoid函数图像.png)
>
> 可以直观的看到sigmoid的值域在0~1之间，符合“点击率”的物理意义。

综上，<span id = "LRmath">逻辑回归模型整个推断过程的数学形式 </span>为：
$$
f(\boldsymbol x)=\frac{1}{1+e^{-(\boldsymbol w \cdot \boldsymbol x+b)}}
$$

### 1.4.3 逻辑回归模型的训练方法

对于标准的逻辑回归模型来说，要确定的参数就是特征向量相应的权重向量 $\boldsymbol w$，下面介绍逻辑回归模型的权重向量 $\boldsymbol w$ 的训练方法。

逻辑回归模型常用的训练方法是梯度下降法、牛顿法、拟牛顿法等，其中梯度下降法是应用最广泛的训练方法，也是学习深度学习各种训练方法的基础。

>**什么是梯度下降法**？
>
>梯度下降法是一个一阶最优化算法。应用梯度下降法的目的是找到一个函数局部极小值。为此，必须沿函数上当前点对应梯度（或者近似梯度）的反方向进行规定步长距离的迭代搜索。如果向梯度正方向迭代进行搜索，则会接近函数的局部极大值点，这个过程被称为梯度上升法。
>
>这利用了“梯度”的性质：如果实值函数F(x)在点$x_0$处可微且有定义，那么函数F(x)在点$x_0$处沿着梯度相反的方向$-\nabla F(x)$ 下降最快。
>
>因此，在优化某模型的目标函数时，只需对目标函数进行求导，得到梯度的方向，沿梯度的反方向下降，并迭代此过程直至寻找到局部最小点。

使用梯度下降法求解逻辑回归模型的第一步时确定逻辑回归的目标函数。已知[逻辑回归的数学形式](#LRmath)，这里表示成$f_{\boldsymbol w}(\boldsymbol x)$。对于一个输入样本 $\boldsymbol x$ ，预测结果为正样本（类别1）和负样本（类别0）的概率如下：
$$
\left\{
	\begin{array}{l}
		P(y=1|\boldsymbol x;\boldsymbol w)=f_{\boldsymbol w}(\boldsymbol x)\\
		P(y=0|\boldsymbol x;\boldsymbol w)=1-f_{\boldsymbol w}(\boldsymbol x)
	\end{array}
\right.
$$
综合起来，可以写成：
$$
P(y|\boldsymbol x;\boldsymbol w)=(f_{\boldsymbol w}(\boldsymbol x))^y(1-f_{\boldsymbol w}(\boldsymbol x))^{1-y}
$$
由极大似然估计的原理可以写出逻辑回归的目标函数：
$$
L(\boldsymbol w)=\prod^m_{i=1}P(y|\boldsymbol x;\boldsymbol w)
$$
由于目标函数连乘的形式不便于求导，故在上式两侧取log，并乘以系数-(1/m)，将求最大值的问题转换成求极小值的问题，最终的目标函数形式如下：
$$
J(\boldsymbol w)=-\frac{1}{m}l(\boldsymbol w)=-\frac{1}{m}logL(\boldsymbol w)\\
=-\frac{1}{m}(\sum^m_{i=1}(y^i logf_{\boldsymbol w}(\boldsymbol x^i)+(1-y^i)log(1-f_{\boldsymbol w}(\boldsymbol x^i)))
$$
在得到逻辑回归的目标函数后，需对每个参数求偏导，得到梯度方向，对$J(\boldsymbol w)$中参数$w_j$求偏导的结果如下：
$$
\frac{\partial}{\partial w_j}J(\boldsymbol w)=\frac{1}{m}\sum^m_{i=1}(f_{\boldsymbol w}(\boldsymbol x^i)-y^i)\boldsymbol x^i_j
$$
在得到梯度之后，即可得到模型参数的更新方式，如下：
$$
w_j \leftarrow w_j -\gamma\frac{1}{m}\sum^m_{i=1}(f_w(x^i)-y^i)x^i_j
$$
至此，完成了逻辑回归模型的更新推导。

可以看出，无论矩阵分解还是逻辑回归，在用梯度下降求解时都遵循其基本步骤。问题的关键在于利用模型的数学形式找出其目标函数，并通过求导得到梯度下降的公式。

### 1.4.4 逻辑回归模型的优势

在深度学习模型流行之前，逻辑回归模型曾在相当长的一段时间里是推荐系统、计算广告业界的主要选择之一。除了在形式上适于融合不同特征，形成较“全面”的推荐结果，其流行还有三个方面的原因：一是数学含义上的支撑；二是可解释性强；三是工程化的需要。

1. **数学含义上的支撑**
   逻辑回归作为广义线性模型的一种，它的假设是因变量y服从伯努利分布。那么在CTR预估这个问题上，“点击”事件是否发生就是模型的因变量y，而用户是否点击广告是一个经典的掷偏心硬币问题。因此，CTR模型的因变量显然应该服从伯努利分布。所以，采用逻辑回归作为CTR模型是符合“点击”这一事件的物理意义的。
   与之相比，线性回归作为广义线性模型的另一个特例，其假设是因变量y服从高斯分布，这显然不是点击这类二分类问题的数学假设。

2. **可解释性强**
   直观地讲，逻辑回归模型地数学形式是各特征的加权和，再施以sigmoid函数。在逻辑回归数学基础的支撑下，逻辑回归的简单数学形式页非常符合人类对预估过程的直觉认知。
   使用各特征的加权和是为了综合不同特征对CTR的影响，而不同特征的重要程度不一样，所以，为不同特征指定不同的权重，代表不同的特征的重要程度。最后，通过sigmoid函数，使其值能够映射到0~1区间，正好符合CTR的物理意义。
   逻辑回归如此符合人类的直觉认识显然有其他的好处——使模型具有极强的可解释性。算法工程师可以轻易地根据权重的不同解释哪些特征比较重要，在CTR模型的预测有偏差时定位是哪些因素影响了最后的结果。在与负责运营、产品的同事合作时，也便于给出可解释的原因，有效降低沟通成本。

3. **工程化的需要**

   在互联网公司每天动辄TB级别的数据面前，模型的训练开销和在线推断效率显得异常重要。在GPU尚未流行的2012年之前，逻辑回归模型凭借其易于并行化、模型简单、训练开销小等特点，占据着工程领域的主流。囿于工程团队的限制，即使其他复杂模型的效果有所提升，在没有明显击败逻辑回归模型之前，公司也不会贸然加大计算资源的投入，升级推荐模型或CTR模型，这是逻辑回归持续流行的另一重要原因。

### 1.4.5 逻辑回归模型的局限性

逻辑回归作为一个基础模型，显然有其简单、直观、易用的特点。但其局限性也是非常明显的：表达能力不强，无法进行特征交叉、特征筛选等一系列较为“高级”的操作，因此不可避免地造成信息地损失。为解决这一问题，推荐模型朝着复杂化的方向继续发展，衍生出因子分解机等高维的复杂模型。在进入深度学习时代之后，多层神经网络强大的表达能力可以完全替代逻辑回归模型，让它逐渐从各公司退役。

逻辑回归模型表达能力不强的问题，会不可避免地造成有效信息地损失。在仅使用单一特征而非交叉特征进行判断地情况下，有时不仅是信息损失地问题，甚至会得出错误地结论。著名地“辛普森悖论”用一个简单地例子，说明了进行多维度特征交叉地重要性。

> **什么是辛普森悖论**？
>
> 在对样本集合进行分组研究时，在分组比较中都占优势的一方，在总评中有时反而是失势的一方，这种有悖常理的现象，被称为“辛普森悖论”。
>
> 假如下面两表为视频应用中男性用户和女性用户点击视频的数据。
>
> 男性：
>
> | 视频  | 点击（次） | 曝光（次） | 点击率 |
> | ----- | ---------- | ---------- | ------ |
> | 视频A | 8          | 530        | 1.51%  |
> | 视频B | 51         | 1520       | 3.36%  |
>
> 女性：
>
> | 视频  | 点击（次） | 曝光（次） | 点击率 |
> | ----- | ---------- | ---------- | ------ |
> | 视频A | 201        | 2510       | 8.01%  |
> | 视频B | 92         | 1010       | 9.11%  |
>
> 从以上数据中可以看出，无论男性用户还是女性用户，对视频B的点击率都高于A，显然推荐系统应该优先考虑向用户推荐B。
>
> 那么，如果忽略性别这个维度，将数据汇总会得出什么结论呢？
>
> 汇总：
>
> | 视频  | 点击（次） | 曝光（次） | 点击率 |
> | ----- | ---------- | ---------- | ------ |
> | 视频A | 209        | 3040       | 6.88%  |
> | 视频B | 143        | 2530       | 5.65%  |
>
> 在汇总结果中，视频A的点击率居然比视频B高。如果据此进行推荐，将得出与之前的结果完全相反的结论，这就是所谓的“辛普森悖论”。

逻辑回归只对单一特征做简单加权，不具备进行特征交叉生成高维组合特征的能力，因此表达能力很弱，甚至可能得出像“辛普森悖论”那样的错误结论。因此，通过改造逻辑回归模型，使其具备特征交叉能力是必要和迫切的。

## 1.5 POLY2模型

针对特征交叉的问题，算法工程师经常采用先手动组合特征，再通过各种分析手段筛选特征的方法，但该方法无疑是低效的。更遗憾的是，人类的经验往往有局限性，程序员的时间和精力也无法支撑其找到最优的特征组合。因此，采用POLY2模型进行特征的“暴力”组合成了可行的选择。

### 1.5.1 POLY2模型的数学形式

$$
\empty POLY2(\boldsymbol w,\boldsymbol x)=\sum^{n-1}_{j_1=1}\sum^n_{j_2=j_1+1}w_h(j_1,j_2)x_{j_1}x_{j_2}
$$

可以看到，该模型对所有特征进行两两交叉（特征$x_{j_1}$和$x_{j_2}$），并对所有的特征组合赋予权重$w_h(j_1,j_2)$。POLY2通过暴力组合特征的方式，在一定程度上解决了特征组合的问题。POLY2模型本质上仍是线性模型，其训练方法与逻辑回归并无区别，因此便于工程上的兼容。

### 1.5.2 POLY2模型的缺陷

但POLY2模型存在两个较大的缺陷。

1. 在处理互联网数据时，经常采用one-hot编码的方法处理类别型数据，致使特征向量极度稀疏，POLY2进行无选择的特征交叉——原本就非常稀疏的特征向量更加稀疏，导致大部分交叉特征的权重缺乏有效的数据进行训练，无法收敛。
2. 权重参数的数量从n直接上升到$n^2$，极大的增加了训练复杂度。

> **什么是one-hot编码**？
>
> one-hot编码是将类别型特征向量的一种编码方式。由于类别型特征不具备数值化意义，如果不进行one-hot编码，无法将其直接作为特征向量的一个维度使用。
>
> 举例来说，某样本有三个特征，分别是星期、性别和城市，用`[Weekday=Tuesday,Gender=Male,City=London]`表示。由于模型的输入特征向量仅可以是数值型特征向量，最常用的方法就是将特征做one-hot编码。编码结果如下：
> $$
> \begin{matrix} \underbrace{ [0,1,0,0,0,0,0] }\\Weekday=Tuesday \end{matrix}
> \begin{matrix} \underbrace{ [0,1] }\\Gender=Male \end{matrix}
> \begin{matrix} \underbrace{ [0,0,1,0,\cdots,0,0] }\\City=London \end{matrix}
> $$
> 可以看到，Weekday这个特征域有7个维度，Tuesday对应第2个维度，所以把对应维度置为1。Gender分为Male和Female，one-hot编码就有两个维度，City特征域同理。
>
> 虽然one-hot编码方式可以将类别型特征转变成数值型特征向量，但是会不可避免地造成特征向量中存在大量数值为0的特征维度。这在互联网这种海量用户场景下尤为明显。假设某应用有一亿用户，那么将用户id进行one-hot编码后，将造成1亿维特征向量中仅有1维是非零的。这是造成互联网模型的输入特征向量稀疏的主要原因。

## 1.6 FM模型

为了解决POLY2模型的缺陷，2010年，Rendle提出了FM模型。

## 1.7 FFM模型



## 1.8 GBDT+LR

## 1.9 LS-PLM

## 1.10 总结

| 模型名称 | 基本原理                                                     | 特点                                                         | 局限性                                                       |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 协同过滤 | 根据用户的行为历史生成用户-物品共现矩阵，利用用户相似性和物品相似性进行推荐 | 原理简单、直接，应用广泛                                     | 泛化能力差，处理稀疏矩阵的能力差，推荐结果头部效应比较明显   |
| 矩阵分解 | 将协同过滤算法中的共现矩阵分解为用户矩阵和物品矩阵，利用用户隐向量和物品隐向量的内积进行排序并推荐 | 相较协同过滤，泛化能力有所增强，对稀疏矩阵的处理能力有所增强 | 除了用户历史行为数据，难以利用其他用户、物品特征及上下文特征 |
| 逻辑回归 | 将推荐问题转化成类似CTR预估的二分类问题，将用户、物品、上下文等不同特征转换成特征向量，输入逻辑回归模型得到CTR，再按照预估CTR进行排序并推荐 | 能够融合多种类型的不同特征                                   | 模型不具备特征组合能力，表达能力较差                         |
| FM       | 在逻辑回归的基础上，在模型中加入二阶特征交叉的部分，为每一维特征训练得到相应特征隐向量，通过隐向量间的内积运算得到交叉特征权重 | 相比逻辑回归，具备了二阶特征交叉能力，模型的表达能力增强     | 由于组合爆炸的问题的限制，模型不易扩展到三阶特征交叉阶段     |
| FFM      | 在FM模型的基础上，加入“特征域”的概念，使每个特征在与不同域的特征交叉时采用不同的隐向量 | 相比FM，进一步加强了特征交叉的能力                           | 模型的训练开销达到了$O(n^2)$的量级，训练开销较大             |
| GBDT+LR  | 利用GBDT进行“自动化”的特征组合，将原始特征向量转换成离散型特征向量，并输入逻辑回归模型，进行最终的CTR预估 | 特征工程模型化，使模型具备了更高阶特征组合的能力             | GBDT无法进行完全并行的训练，更新所需的训练时长较长           |
| LS-PLM   | 首先对样本进行“分片”，在每个“分片”内部构建逻辑回归模型，将每个样本的各个“分片”概率与逻辑回归的得分进行加权平均，得到最终的预估值 | 模型结构类似三层神经网络，具备了较强的表达能力               | 模型结构相比深度学习模型仍比较简单，有进一步提高的空间       |



# 参考文献

1. 《深度学习推荐系统》王喆/编著 电子工业出版社 2020.3
2. 《推荐系统实践》项亮/编著 人民邮电出版社 2012.6